---
title: 特別講義DS Ch11 線形回帰分析
description: 資料
tags:
    - datascience
    - statistics
    - python
featured: true
date: 2024-03-29
tableOfContents: true
previousChapter: slds10.html
nextChapter: slds12.html
---

# 線形回帰分析

相関分析では,ある変数間に関係があることを示すことができました. しかし,相関分析で示せるのは,変数Aによって変数Bが増加するか,減少するかということのみです. 具体的に,どの程度変数Aが動くことで,変数Bがどの程度変動するかを式によって**説明する**手法に**回帰分析(Regression Analysis)**があります.

また,回帰分析は検定の手法によって求められた式がどの程度信頼できるのかを検定によって確かめることも可能です.

::: note
回帰分析では,データ

$$
y = \beta_1 + \beta_2 x
$$

のような式で変数yとxの関係を説明し,この式を**回帰式,回帰方程式**と呼びます. このとき,

- 説明される変数yを **非説明変数**,**目的変数**などと呼びます.

- 説明する変数xを**説明変数**,**従属変数**などと呼びます.

- 回帰式における **$\beta_1$**のような変数に乗じられていない値を**切片**といいます.

    切片は, $x = 0$ のときのyの値を意味しています.

- 変数に乗じられている**$\beta_2$**のような値を**傾き**といい,$\beta_1,\beta_2$ などを併せて**回帰係数**といいます.

    傾きは,xが1変化した際のyの変化量を表しています.

- 説明変数が一つの回帰式を求める分析を**単回帰分析**, 2つ以上の説明変数を用いる場合を**重回帰分析**といいます.

- yがxの線形関数である場合を **線形回帰(Linear regression)**,それ以外のものを**非線形回帰(non-linear regression)**といいます.
:::

## 発展: 回帰分析は何を行っているのか

回帰分析が何を行っているのかについて,単回帰で行っている最小二乗法を事例に確認していきましょう.
重回帰に関しては, 線形計画問題など異なる学習が必要になるので,今回は扱いません. あくまで,回帰というものがどのような意味であるかに関して簡単に説明します.

こちらの詳細は統計学入門で扱っていますので, この講義ではあまり深く扱いません.興味のある方は読んでみてください.

### 母回帰方程式

体重$y$を慎重$x$によって説明する回帰方程式として, $y = \beta_1 + \beta_2 x$ を考えてみます.
しかし, 実際の体重は身長以外の要素によってばらつきます. そのような**ばらつき**を考慮して,データの$i$人目の体重,身長をそれぞれ,$Y_i,X_i$として,身長以外の要素によるばらつきを$\epsilon_i$とすると,母集団において,以下のような式が立てられます.

$$
Y_i = \beta_1 + \beta_2 X_i + \epsilon_i ~(i=1,2,...,n)
$$

これを**母回帰方程式(Population Regression Equation)**と呼びます.
また, $\beta_1, \beta_2$を**母(偏)回帰係数**といい,これを推定,検定することを回帰分析といいます.

### 誤差項,撹乱項

母回帰方程式

$$
Y_i = \beta_1 + \beta_2 X_i + \epsilon_i ~(i=1,2,...,n)
$$

における $\epsilon_i$は$X_i$で説明できない誤差を表す確率変数であり,誤差項,撹乱項といいます.

回帰分析において,誤差項は以下の仮定をおいています.

::: note
- 期待値0: $E(\epsilon_i) = 0 ~ (i=1,2,...,n)$
- 分散一定: $V(\epsilon_i) = \sigma^2 ~ (i=1,2,...,n)$
- 無相関: $i \neq j \Rightarrow Cov(\epsilon_i, \epsilon_j) = 0$
- 正規分布: $\epsilon_i \sim N(o,\sigma^2)$
:::

これによって,

$$
E(Y_i) = \beta_1 + \beta_2 X_i
$$

が得られます.

### 最小二乗法

母回帰方程式

$$
Y_i = \beta_1 + \beta_2 X_i + \epsilon_i ~(i=1,2,...,n)
$$
における母回帰係数$\beta_1, \beta_2$は観測できないので,**誤差項を最小化する**母回帰係数を統計的推測することで求めます.

誤差項を最小化する母回帰係数の推定方法を**最小二乗法(least squares method)**といいます.

母回帰方程式を変形して,

$$
\epsilon_i = Y_i - (\beta_1 + \beta_2 X_i)
$$

が少ないほど, $X_i$による$Y_i$の説明力が上がります(モデルによってよく関係が説明できている.)

なので,モデル全体で,$\epsilon_i$を最小化することを考えてみます.

![least squares method](/images/least-squares-method1.png)

誤差の正負を打ち消すために,モデル全体の誤差項の二乗の和


$$
S = \sum_{i=1} \epsilon_{1}^{2} = \sum \{Y_i - (\beta_1 + \beta_2 X_i)\}^2
$$

Sを最小化する**(最小二乗)推定量** $\hat{\beta_1},\hat{\beta_2}$を求める問題として整理できます.

$S$の偏微分を0とおいて,

$$
\frac{\partial S}{ \partial \beta_1} = -2 \sum (Y_i = \beta_1 - \beta_2 X_i) = 0 \\
\frac{\partial S}{ \partial \beta_2} = -2 \sum (Y_i = \beta_1 - \beta_2 X_i)X_i = 0 \\
$$

これを解いて,

$$
\hat{\beta_1} = \bar{Y} - \hat{\beta_2}\bar{X} \\
\hat{\beta_2} = \frac{\sum(X_i - \bar{X})(Y_i - \bar(Y))}{\sum(X_i - \bar{X})^2}
$$
が得られます.

### 回帰係数の検定

最小二乗推定量によって得られた方程式

$$
Y = \hat{\beta_1} + \hat{\beta_2}X
$$

を**標本回帰方程式**といいます.

求めた標本回帰方程式が,XとYの関係を説明できているのかを考えます. XがYを全く説明できていない場合, $\epsilon_i$だけで説明ができるため, $\beta_2 \neq 0$と言えれば,統計的にXがYを説明できていると言えます.

そこで, 帰無仮説 $H_0:\beta_2 = 0$として,偏回帰係数に関する統計的仮説検定を実施します.

母数$\beta_2$に関する仮説検定を行うために, $\beta_2$の確率分布を考えます.

誤差項 $\epsilon_i \sim N(o,\sigma^2)$として,

$$
\hat{\beta_1} = \bar{Y} - \hat{\beta_2}\bar{X} \\
\hat{\beta_2} = \frac{\sum(X_i - \bar{X})(Y_i - \bar(Y))}{\sum(X_i - \bar{X})^2}
$$
であるから,

$$
V(\hat{\beta_1}) = \frac{\sigma^2 \sum X_i^2}{n\sum(X_i - \bar{X})^2} \\
E(\hat{\beta_1}) = \beta_1 \\
V(\hat{\beta_2}) = \frac{\sigma^2 }{\sum(X_i - \bar{X})^2} \\
E(\hat{\beta_2}) = \beta_2
$$
なので,

$$
\hat{\beta_2} \sim N(\beta_2,\frac{\sigma^2}{\sum(X_i - \bar{X})^2})
$$

となる.

誤差項の母標準偏差 $\sigma$が含まれるので,推定する.

標本回帰方程式 $Y=\hat{\beta_1}+\hat{\beta_2}X$によって求められる各$i$の値(回帰値)

$$
\hat{Y_i} = \hat{\beta_1} + \hat{\beta_2}X_i
$$

と実際に観測された実測値 $Y_i$との差を

$$
\hat{e_i} = Y_i - \hat{Y_i} = Y_i - \hat{\beta_1} - \hat{\beta_2}X_i
$$

を**回帰残渣(residual)**といい,Xで説明されなかった残渣を表す.

回帰残渣を母回帰方程式 $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$における誤差項$\epsilon_i$の推定値として利用する.

求める必要があるのは,誤差項の分散の推定値としての分散

$$
V(\hat{e_i}) = E(\hat{e_i^2}) - (E(\hat{e_i}))^2
$$

であるが,

$$
\frac{\partial S}{\partial \beta_2} = -2 \sum (Y_i - \beta_1 - \beta_2 X_i)X_i = 0
$$
なので,

$$
\sum (Y_i - \beta_1 - \beta_2 X_i) = \sum \hat{e_i} = 0
$$

となり,

$$
\bar{e_i} = \frac{1}{n} \sum \hat{e_i} = 0
$$

なので,

$$
\begin{align*}
V(\hat{e_i}) &= E(\hat{eI^2}) \\
&= \frac{1}{n-2}\sum (\hat{e_i}^2 - \bar{e_i}^2) \\
&= \frac{\sum \hat{e_i}^2}{n-2}
\end{align*}
$$

となります.

これを誤差項の分散$\sigma^2$の推定値

$$
S^2 = \frac{\sum \hat{e_i}^2}{n-2}
$$
として利用します.

なお,この累乗根は回帰式がどの程度実測値に当てはまっているかを表す,**推定値の標準誤差(standard error of estimates)**と呼ばれます.

$$
s.e. = \sqrt{S^2} = \sqrt{\frac{\sum \hat{e_i}^2}{n-2}}
$$

これを誤差項の母標準偏差$\sigma$の推定値として利用して,$\hat{\beta_2}$の標準誤差の推定値は,

$$
V(\hat{\beta_2}) = \frac{\sigma^2 }{\sum(X_i - \bar{X})^2}
$$

から,

$$
s.e.(\hat{\beta_2}) = \frac{s.e.}{\sqrt{\sum(X_i - \bar{X})^2}}
$$

となり,$s.e.(\hat{\beta_2})$を用いて標準化した値は, $t(n-2)$に従うので,

$$
t_2 = \frac{\hat{\beta_2} - \beta_2}{s.e.(\hat{\beta_2})} \sim t(n-2)
$$



## 重回帰分析

説明変数も被説明変数も量的変数のときに,

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_n x_{ni}
$$

のような回帰式を求めます.

それでは,Pythonで重回帰分析を行ってみましょう.

題材として以下の｢日本教育新聞｣の記事([https://www.kyoiku-press.com/post-223665/
](https://www.kyoiku-press.com/post-223665/))について考えてみます.

::: note

> **Wi-Fi電磁波で学力低下を懸念,市議ら意見交換会**
>
> 2020年12月7日
>電磁波が人体に影響を与え,学力の低下を招くことなどを懸念する市議会議員らは11月8日,無線LANにより生じる「電磁波過敏症」への対策などについて,意見交換会をオンラインで開催した.
>　GIGAスクール構想でICT環境を整備するに当たって,電磁波による問題点とそれへの対策を話し合った.
>　東京都新宿区議会のよだかれん議員は,学力と健康の2つの観点から,「大人でもICT機器を使用すると前頭前野の機能が低下するという様々な研究報告がある.小学1年生からの使用で脳の発達への影響は懸念されないのか」と指摘した.
>　よだ議員は,9月議会の質疑の一部で,令和元年の全国学力テストの結果に基づき,電子黒板やプロジェクターなどの大型電子機器の整備率が1位の佐賀県は正答率が全国で43位だった一方,整備率最下位の秋田県は正答率が1位だったことを紹介した.
>　意見交換会を主催した「いのち環境ネットワーク」の加藤やすこ代表によると,電磁波過敏症は短い時間でも発症の可能性があり,一度の発症が長期に及んで続くという.
>　埼玉県日高市議会の松尾まよか議員は,GIGAスクール構想を進める上で,Wi-Fiのアクセスポイントの位置を児童・生徒から遠ざけた場所に設置する,使用していない時は電源を落とすことを重要な点に位置付けた.
>　松尾議員は,「発症者が出てからでは遅い.発症後の対策に予算をかけるよりも,事前に対策しておく方がよい」と強調した.
>　今回の意見交換会に参加した市議らは,9月議会の発言内容なども報告した.

こちらの記事では,

> 令和元年の全国学力テストの結果に基づき,電子黒板やプロジェクターなどの大型電子機器の整備率が1位の佐賀県は正答率が全国で43位だった一方,整備率最下位の秋田県は正答率が1位だった

ことから,

｢学校教育におけるICT機器の導入が学力低下を招いている｣ということを主張しています.

:::

最下位と1位の2つの観測対象のデータだけでこのような主張が可能なのでしょうか. データを使ってこの主張を検証してみましょう. 必要なデータは[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/math_correct.csv)からダウンロード可能ですが,データの取得手順に興味がある方は,以下から確認して自分でデータを作成してみましょう.

::: note
- 教育データの作成

<details open>
    <summary> 開く/ 閉じる </summary>
まずは全国の県別の学力データを探してみます.

 国立教育政策研究所の行っている全国学力・学習状況調査([https://www.nier.go.jp/18chousakekkahoukoku/index.html](https://www.nier.go.jp/19chousakekkahoukoku/index.html))から,令和元年の県別の小学生の学力データを取得します.

![全国学力･学習状況調査](/images/regression1.png)

ただし,こちらのデータはPDFでのみ公開されているためExcelやAIなどを利用して,CSVに変換する必要があります. 今回私はPDFからテキストエディタにコピー&ペーストして, 不要な記号を置換しましたが,好きな方法でやりましょう.

![全国学力･学習状況調査](/images/regression2.png)

続いて, 記事にある,電子黒板やプロジェクターなどの大型電子機器の整備率に関するデータを[e-stat](https://www.e-stat.go.jp )から取得します.

今回は県別データなので,｢地域｣から,データを取得します.

![e-stat 地域](/images/regression3.png)

｢都道府県データ｣にチェックを入れて｢データ表示｣に進みます.

![都道府県データ](/images/regression4.png)

｢地域選択｣において｢全て選択｣をクリックしたあと選択中地域から全国をクリックし,｢地域を削除｣
を押し,47都道府県のみを選び,確定します.

![地域選択](/images/regression5.png)

｢分野｣から｢教育｣を選び関連のありそうな
    - ｢教育用コンピュータ一台あたりの児童数(小学校)｣
    - ｢普通教室の電子黒板整備率(小学校)｣
    - ｢デジタル教科書の整備率(小学校)｣
などを選択し,｢項目を選択｣を押し,確定します.

![教育関連データの選択](/images/regression6.png)

｢調査年｣からデータが揃っている2017年度を選択し,｢再表示｣を押します.

![調査年の選択](/images/regression7.png)

｢ダウンロード｣から,右の図のように選択して,ダウンロードします.

![ダウンロード](/images/regression8.png)

学力のデータにダウンロードした黒板などのデータをコピーして,適切にヘッダーをつけ,Utf-8で作業フォルダの中のDataフォルダに保存しましょう.例ではmath_correct.csvと名前をつけています.
    - 数学正答率       → math
    - 一台あたりのPC   → pc
    - 黒板             → board
    - 電子教科書       → text

Excelで貼り付けた際に文字列情報になっている場合があり,!マークが表示されていたら数値に変換しておきましょう.

![データの編集](/images/regression9.png)

データが読み込めるか確認してみましょう.

~~~ py

df = pd.read_csv('Data/math_correct.csv')
print(df)

"""
   pref  math   pc  board  text
0   北海道    64  5.8   20.8  38.5
1   青森県    67  5.3   22.4  30.9
2   岩手県    66  5.4   19.6  35.9
3   宮城県    65  6.8   14.0  66.6
4   秋田県    70  5.6   22.4  36.8
5   山形県    65  5.7   16.6  44.0
6   福島県    65  5.3   24.6  47.2
7   茨城県    66  6.5   22.0  49.5
8   栃木県    65  5.9   42.6  70.8
9   群馬県    65  6.1   15.4  39.3
10  埼玉県    66  9.6   23.9  58.8
...
"""
~~~

</details>
:::

それでは,このデータを利用して重回帰分析を行っていきます.

::: note

Pythonで重回帰を行えるライブラリは多数ありますが, 今回は統計モデリングのためのライブラリ`statsmodels`を利用します. pip install し, プログラムの最初に, `import statsmodels.api as sm`と記述し`import`しておきましょう.
:::

ただし, 重回帰を実施する前に,いくつか必要な前処理があります. 順番に見ていきましょう.

### 正規化・標準化

重回帰分析では,複数の説明変数の目的変数に対する影響を比較します. 各説明変数の目的変数への影響力は,回帰係数として現れますが説明変数1の単位がmm,説明変数2の単位がm, 説明変数3の単位がKgなどとなると,それぞれの回帰係数は,それぞれ1mmの変化,1mの変化,1Kgの変化に対する目的変数の変化量を表しているために,同じ基準で比較できません.そこで,**正規化/標準化**というデータの単位などを揃える操作を行う必要があります.


::: note

- 正規化(Normalization)

最大値を1,最小値を0に揃えること.

データを $X = {x_1,x_2,...,x_n}$ とすると, 正規化後の値 $x_i'$は

$$
x_i' = \frac{x_i - min(X)}{max(X) - min(X)}
$$

`pandas`では`X`が対象のデータ列名のリストだとすると,以下のように正規化列`X_n`が求められる.

~~~ py
X_n = (df[X] - df[X].min()) \
    / (df[X].max() - df[X].min())
~~~


:::

::: note
- 標準化(Standarization)

標準得点を求めて,平均0,分散1に揃える.

$$
z_i = \frac{x_i - \bar{x}}{\sigma}
$$

`pandas`では`X`が対象のデータ列名のリストだとすると,以下のように標準化列`X_z`が求められる.

~~~ py
X_z = (df[X] - df[X].mean()) \
    / df[X].std()
~~~

それでは, 先ほど得られた教育データを標準化してみましょう.

~~~ py
Y_label = 'math'
#標準化
Y = (df[Y_label] - df[Y_label].mean()) \
    / df[Y_label].std()

plt.hist(Y)
plt.title(Y_label)
plt.show()

# 説明変数のヘッダーを指定
X_labels = ['pc','board','text']
#標準化
X  = (df[X_labels] - df[X_labels].mean()) \
    / df[X_labels].std()

fig, axes = plt.subplots(nrows= 3 #行数の指定
                        ,ncols= 1 #列数の指定
                        ,sharex=True)

#連番に変換
axes = axes.flatten()
for i in range(3):
    col = X_labels[i] #countをiで共通化
    axes[i].hist(X[col])
    axes[i].set_title(col)
plt.show()
~~~

![Yのヒストグラム](/images/regression10.png)
![Xのヒストグラム](/images/regression11.png)

::: warn
本来ならば,ここで`math`が左右対称ではないことから正規分布を仮定した回帰を行うべきではなく,ベータ分布などを仮定した一般化線形モデルにすることを検討します.

また, `text`に外れ値(長崎県)があることなども考慮するべきですが,今回は線形回帰の事例ですのでそのまま進めてみます.
:::


:::

### 多重共線性(multi-colinearlity)

::: note
説明変数 $x_1, x_2, ..., x_n$ において, 特定の変数$x_i$が他の変数によって

$$
x_i = \sum_{i \neq j} \alpha_j x_j
$$

として,少なくとも1つが0でない$\alpha_j$によって表すことができる場合に,すなわち各変数が一次独立でなくなる場合に,説明変数に**完全な多重共線性**が成り立っているといいます.

このとき, 最小二乗法では, $y = \beta_0 + \beta_1 x_1 + beta_2 x_2 + ... + \beta_n x_n$を解くことができなくなるため,解が得られなくなります.

:::

例えば, $x_1 = \alpha_2 x_2$であったとすると,

$$
y = \beta_0 + (\beta_1 + \alpha_2 \beta_2) x_1 + \beta_3 x_3 + ... + \beta_n x_n
$$

となり,推定値が $\hat{\beta_{12}} = \hat{\beta_1} + \alpha_2 \hat{\beta_2}$であったとすると, $\hat{\beta_{12}}$となる$\hat{\beta_1}$と$\hat{\beta_2}$の組み合わせは無数にあるため,$\hat{\beta_1}$と$\hat{\beta_2}$を特定することができなくなります.

このような**完全な多重共線性**は,主に特定の説明変数を,他の説明変数の変形によって作成している場合に生じるため,変形した変数を利用するならば,変形前の変数はモデルに利用しないようにしましょう.

例えば, これから使い方を学習する`statsmodels`を利用して,あえて**完全な多重共線性**が存在するような重回帰分析を行ってみます.
(`statsmodels`や重回帰の実施については,後述するのでこの時点では変数の生成以外は意味を理解する必要はありません.)

~~~ py
import pandas as pd
import statsmodels.api as sm
import numpy as np

#乱数で目的変数と説明変数を生成
y = np.random.rand(100)
x_1 = np.random.rand(100)
df = pd.DataFrame({'y':y,'x_1':x_1})

#説明変数x_2をx_1から生成
df['x_2'] = df['x_1'] * 2

X = ['x_1','x_2']

#予測モデルを作成(重回帰)
X = sm.add_constant(df[X])
model = sm.OLS(df['y'],X)
result = model.fit()
print(result.summary())

"""
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.000
Model:                            OLS   Adj. R-squared:                 -0.010
Method:                 Least Squares   F-statistic:                   0.01870
Date:                Mon, 08 Jul 2024   Prob (F-statistic):              0.892
Time:                        18:24:01   Log-Likelihood:                -11.013
No. Observations:                 100   AIC:                             26.03
Df Residuals:                      98   BIC:                             31.24
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.4896      0.060      8.121      0.000       0.370       0.609
x_1            0.0028      0.021      0.137      0.892      -0.038       0.044
x_2            0.0056      0.041      0.137      0.892      -0.076       0.087
==============================================================================
Omnibus:                       19.023   Durbin-Watson:                   2.250
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.214
Skew:                           0.170   Prob(JB):                       0.0738
Kurtosis:                       1.934   Cond. No.                     8.65e+16
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 3.44e-32. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
~~~

`statsmodels`では,一応重回帰自体は実施できるものの,

> The smallest eigenvalue is 3.44e-32. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.

のように,多重共線性の存在を教えてくれます. (乱数を利用していることもあり)当然モデルの精度も非常に悪くなっているため, このモデルは利用できません.


一方で,

::: note
$$
x_i \approx \sum_{i \neq j} \alpha_j x_j
$$

で成り立つ(完全ではない/弱い)**多重共線性**という概念もあります. これは簡単に言えば, 説明変数間に相関関係が成り立つような場合を指しています.

変数間に相関がある場合,

- サンプルサイズによって,推定値が大きく変わる

- データによって推定値が大きく変わる

など, 利用するデータに対して推定値が不安定になると言われています. これはサンプルサイズを増やすことで対処できますが,一般には変数間の相関が強い場合には, 片方の変数は説明変数から除外することが望ましいとされています.
:::

それでは,先程の教育データの多重共線性をチェックしてみましょう.

~~~ py
# 多重共線性のチェック
# 散布図行列を作成してみる
pd.plotting.scatter_matrix(df, range_padding=0.2)
plt.show()

#ヒートマップで確認
sns.heatmap(df.corr()
            ,vmax=1
            ,vmin=-1
            ,annot=True)
plt.show()
~~~

![クロスプロット](/images/regression12.png)
![相関係数のヒートマップ](/images/regression13.png)

相関係数を確認すると,`board`と`text`の間に`0.56`の相関があることが分かります.
`0.56`程度であればそれほど影響はないのでそのままにしても構いませんが,練習として片方を除外してみます. `text`のほうが`math`との相関が強いので,`board`を除外したいところですが,市議の主張では電子黒板の普及率が問題と成っていたので`text`を除外します.

### 回帰分析の精度と判断

それでは, データの標準化,多重共線性を考慮して,実際に回帰分析を行ってみましょう.
`statsmodels`では, `sm.add_constant(説明変数)`の形で,定数を追加し,切片をモデルに追加することができます.

`sm.OLS(Y,X)`で線形回帰モデルインスタンスを宣言し, `.fit()`で推定を行います.
推定結果は`.summary()`で確認できます.


~~~ py
#相関が見られるため,textを除外
X.drop('text',axis='columns',inplace=True)

#予測モデルを作成(重回帰)
model = sm.OLS(Y,X)
result = model.fit()

#結果の表示
print(result.summary())

"""
==============================================================================
Dep. Variable:                   math   R-squared:                       0.004
Model:                            OLS   Adj. R-squared:                 -0.042
Method:                 Least Squares   F-statistic:                   0.07867
Date:                Mon, 08 Jul 2024   Prob (F-statistic):              0.924
Time:                        23:55:07   Log-Likelihood:                -66.101
No. Observations:                  47   AIC:                             138.2
Df Residuals:                      44   BIC:                             143.8
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       3.995e-15      0.149   2.68e-14      1.000      -0.300       0.300
pc             0.0493      0.162      0.305      0.762      -0.276       0.375
board          0.0560      0.162      0.347      0.730      -0.270       0.382
==============================================================================
Omnibus:                       13.834   Durbin-Watson:                   1.557
Prob(Omnibus):                  0.001   Jarque-Bera (JB):               14.866
Skew:                           1.170   Prob(JB):                     0.000591
Kurtosis:                       4.454   Cond. No.                         1.46
==============================================================================
"""
~~~

さて,推定結果には様々な情報が記述されていますが,どこをどのように見ればいいのでしょうか.

`statsmodels`の`.summary()`では中央部分に各説明変数の評価が記載されています.

- 各説明変数の評価

~~~ sh
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       3.995e-15      0.149   2.68e-14      1.000      -0.300       0.300
pc             0.0493      0.162      0.305      0.762      -0.276       0.375
board          0.0560      0.162      0.347      0.730      -0.270       0.382
==============================================================================
~~~

それぞれいくつか重要なポイントを順に確認していきましょう.

::: note
- **回帰係数(coef)**
---

回帰係数は,説明変数毎に目的変数にどの程度影響力があるかを示したものになります.
:::

今回の事例では, 切片(`const`)が`3.995e-15`であり,`pc`,`text`が0のとき`math`が`3.995e-15`となります. `pc`の回帰係数が`0.0493`,`text`の回帰係数が`0.0560`であり,それぞれの説明変数が1変化する毎に,回帰係数の分だけ目的変数が変化します.

したがって,回帰式は,

$$
math = 3.995*10^{-15} + 0.0493 pc + 0.0560 text
$$

となり,いずれの変数も正の影響を持っており,市議の主張である電子黒板が普及するほどに成績が下がるという主張と逆の結果が出ています.値が標準化されているため,それぞれの回帰係数は相対的な影響力を表しており,実際の変化ではありません. `pc`の一人あたりの台数よりも,`text`の影響が強いことが分かります.

しかし,回帰係数だけを見て,回帰の結果を判断することはできません. 出てきた値が,信頼に値するか,利用可能であるかを他の値を用いて判断する必要があります.

::: note

- **P値(`P>|t|`)と95%信頼区間(`[0.025 0.975]`)**
---

それぞれの説明変数のP値(`P>|t|`)は,その説明変数に対する回帰係数が0である(影響がない)という仮説に対する仮説検定のP値を表しています. また, `[0.025      0.975]`はそれぞれ上側と下側の95%信頼区間を表しています.

有意水準5%の場合, `P値 < 0.05`で有意となり, 信頼区間が0をまたぎません.
(ここで両側検定ですが,`P値 < 0.025`とならないのは,両側の外側累積確率を合算した値が算出されるためです.)
したがって,これらの値はいずれも同じ判断の基準となりますが,最近は論文などには両方載せることが主流です.
(仮説検定や区間推定,P値の意味などに関しては,統計学入門で詳細を扱っています.分からない人はそちらを履修しましょう.)

:::

今回の値を見てみると,いずれの説明変数も有意ではなく,区間推定の結果も0をまたいでいます. したがって,これらの回帰係数の推定値の解釈は, **｢今回のデータと説明変数の組み合わせでは, 電子黒板や電子教科書が学業成績に影響を与えるかどうかは判断できない.｣**ということになります.

::: warn
有意でない説明変数は, 基本的にモデルから除外することが望ましいので,仮により良いモデルを構築する場合には,別の変数やモデルの組み合わせを探すことになります.

それらの手法は,モデル選択などと呼ばれますが,モデル同士の比較の方法に関してはここでは扱わず,後の一般化線形モデルの章で扱うことにします.

:::

これまで各説明変数の影響を見てみましたが, 変数毎ではなく,モデル全体の評価はどのように行うのでしょうか.
`.summary()`の前半部分にはモデル全体での評価が記載されています.

- モデル全体の評価

~~~ sh
                            OLS Regression Results
==============================================================================
Dep. Variable:                   math   R-squared:                       0.004
Model:                            OLS   Adj. R-squared:                 -0.042
Method:                 Least Squares   F-statistic:                   0.07867
Date:                Mon, 08 Jul 2024   Prob (F-statistic):              0.924
Time:                        23:55:07   Log-Likelihood:                -66.101
No. Observations:                  47   AIC:                             138.2
Df Residuals:                      44   BIC:                             143.8
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
~~~


::: note

- **調整済決定係数(Adj. R-squared)**
---

自由度調整済み決定係数$Adj R^2$は,**モデルがどの程度当てはまっているかの基準**です.
基本的に$0 \leq Adj R^2 \leq 1$の値をとり,目安として$0.5 \leq Adj 𝑅^2$であればある程度予測できていると考えられます.

どんな散布図になっても回帰式自体は作成可能ですが,以下の左右どちらの予測値の方が信頼できそうでしょうか.

![R2](/images/regression14.png)

直感的には左の方が**回帰直線が実際の値にフィットしており**信頼できそうな気がしますね. $R^2$はその感覚を数値化したものになります.

![R2](/images/regression15.png)

$R^2$は,式から作られた直線と,実際のデータの点の距離の和であり,$Adj R^2$は,0から1に収まるように変換してものになります.
$AdjR^2$が1に近いほど, 式が点によく当てはまっていることを表します.
:::

今回のモデルを見てみると,`Adj. R-squared:-0.042`であり,全く予測精度が高くないことが分かります.

::: note

- **有意F (Prob (F-statistic))**
---

検定にはそれぞれの係数ごとにt検定,全体にF検定を用います.
有意F(Prob (F-statistic))は,モデル全体にF検定を実施した際のp値を表し,値が小さいほど回帰式が有意であることを表します(0.025 > F で有意).
(検定に関しては,検定の章を参照してください.)

以下の回帰式とデータを見るとどちらも,当てはまり方は同じだとしても, 左の方が信頼性が高いように感じます.

![Prob F](/images/regression16.png)

点が少ないと,他のデータを持ってきたら全然違うところに点が行く可能性が高まります.
この感覚=式が偶然の産物ではないかを検定したP値が有意Fです.
0.025 以下で, 信頼できるといえます.

:::

今回のモデルを見てみると`Prob (F-statistic):0.924`であり,回帰式全体は有意ではなく,このモデルは信頼できないことが分かります.


### 結果の図示

これまでは数値によって,モデルの結果を確認してきましたが,結果を図示することでより直感的に説明が可能になります. 回帰分析の結果を図示する方法は様々ありますが,ここでは結果の散布図と,密度プロットによって確認してみましょう.

各説明変数が,どの程度目的変数を説明しているかを確認する方法として,一つの説明変数を選んで散布図を作成し,予測値を重ねるという手法がよく用いられます.

`statsmodels`では,そのための`plot_partregress_grid`というメソッドが準備されています.

~~~ py
#回帰グラフの作成
from statsmodels.graphics.regressionplots import plot_partregress_grid
fig = plt.figure(figsize=(16,8))
plot_partregress_grid(result, fig=fig)
plt.show()
~~~

![3変数 Partial Plot](/images/regression17.png)

どの変数にもあまりフィットしていない様子が視覚的に把握できます.

また,モデルによって予測される値と実際の値のヒストグラムや密度プロットを比較することも良く行われます.
ここでは`seaborn`の`kdeplot`を利用してカーネル密度プロットを行ってみます.

~~~py
#予測結果の作成
pred = result.predict(X)
plt.figure(figsize=(16,8))
sns.kdeplot(pred, label = 'Predicted')
sns.kdeplot(Y, label = 'Actual')
plt.title('Actual/Predicted')
plt.xlabel('math')
plt.ylabel('Density')
plt.legend()
plt.show()
~~~

![3変数 Density Plot](/images/regression18.png)

実測値と予測値が全く違う分布をしており,予測がうまく行っていないことが視覚的に把握できます.

結論として, 今回のモデルでは市議の主張に対しては,否定も肯定もできないが,市議の根拠とするデータでは,市議の主張が導かれないということになりました.

実際に,どのような影響があるかを調べるためには,データかモデルのいずれかを変えて,説明可能なモデルを構築する必要があります.


::: note

- 演習問題

数学の成績と関連のありそうなデータをe-statから複数探し,重回帰分析を行い,その結果を解釈してください.

:::




