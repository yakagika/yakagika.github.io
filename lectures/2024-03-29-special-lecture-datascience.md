---
title: 特別講義(データサイエンス)
description: 資料
tags:
    - datascience
    - statistics
    - python
featured: true
tableOfContents: true
---


# 注意,記法などについて
<details>
    <summary> 開く </summary>
特別講義(データサイエンス)の授業資料などを書いてく予定です.
(現在執筆中)

本資料は,文系学部生向けにデータサイエンスを体験することを目的にしたものです.
これまでにプログラミングや理数系科目を学習していないことを前提にしているので,
初歩の初歩から一つずつ扱う科目です.

こちらの資料では,授業に必要な技術的な内容に限定して掲載します.
授業概要,授業の注意点,成績等については講義中に別資料で説明します.



## デザインについて

文章中で色の変わっているブロックはオレンジ色が注意(warn),青色が演習や強調(note)など独立した部分を表しています.
講義ではwarnに関しては,飛ばす場合があるので,興味のある人は自分で読み進めてください.

::: warn

これは注意や発展的内容を示しています.


:::

::: note

これは演習や強調したい箇所に利用されています.

:::

リンクは[Google](https://www.google.co.jp)のように下線で表示されます.クリックすることでリンクに飛ぶことが可能です. 右クリックして,新しいタブで開くことを推奨しています.

## 演習回答及びデータ

演習回答は[こちら](https://yakagika.github.io/lectures/2024-03-29-special-lecture-datascience-answer.html)からダウンロードできます. ただし,こちらの演習の回答は,講義中に出た学生の回答をSAがまとめたものです.
間違い等が含まれている場合は教員まで伝えてください.


利用するデータは[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/)からダウンロード可能です. 本資料で扱われるデータは基本的に,ダミーデータです.したがって,分析例として導かれている結論も,現実の事象を表しているわけではないことに注意してください.研究などにも利用することはできません.



## シンタックスとコーディングスタイル

本講義ではプログラムブロックは以下のように黒い背景でシンタックスハイライトが適用されています.

~~~ py
import pandas as pd

print('Sample')

~~~

コピー&ペーストが可能なので, 自分のプログラムに利用してください.

::: warn

- Pythonのコーディングスタイルについて

Pythonの書き方は,基本的に可読性を高めるために,決まったルールで記載されます.
このルールを**コーディングスタイル**といいます.
Pythonにおける標準的なコーディングスタイルには[PEP8(Python Enhancement Proposal)](https://peps.python.org/pep-0008/)などがありますが,本講義では一部従っていません.

特に,リストや辞書型などの改行において

- コンマを改行後の先頭に記述する

- ブランケットの終わりを改行した最後に記述する

といった記法を採用しています.これは,関数型言語(特にHaskell)の講義との対応関係を持たせるために筆者が好んでいるものですが,
一般的な手続き型言語のコーディングスタイルではありませんので注意してください.

また, リストなどを`xs`などの`s`をつけた複数形で表現する命名規則も多用していますが,
こちらも一般的なコーディングスタイルではありません.

~~~ py

#通常の記法
animals = ['cat',
           'dog',
           'bird']

#本資料における記法
animals = ['cat'
          ,'dog'
          ,'bird'
          ]
~~~

:::

</details>

# イントロダクション
<details>
    <summary> 開く </summary>

データサイエンスは,データを利用して現象を発見したり,予測をする科学の総称です. データの作成やデータを分析する前の処理,利用するコンピュータ関連の技術なども対象となります.

データサイエンスと関連の深い分野/用語として統計や機械学習,AIがありますが,それらもデータサイエンスの一部とみなすことができます.

統計は,データ自体の取得・作成・集計から,データの構造を分かりやすく分析・可視化する学問です.機械学習は,データから予測モデルを作り,意思決定などに応用する学問です. 統計や機械学習・AIは明確に分割することができるものではなく,かなりの部分が共通しています. 特に皆さんがこの講義の先修科目である統計学入門で学んだ,初歩的な統計は,機械学習やAI,データサイエンス全般を学ぶ上で,前提知識となります.この他,どちらかに分類できるわけではない,モデリング,AI,シミュレーション,最適化,次元削減,など様々なトピックが含まれています.

![データサイエンス](/images/data-science-and-components.png)

この講義では,統計学入門で学んだ,可視化,数値化,検定,回帰などの統計学の手法をPCを使って行う他,統計学入門の範囲を超えたより発展的な手法に関しても学習します.

しかし,このようにデータサイエンスは非常に広範な学問なので,統計学入門のように個別の手法に関して,細かく理解することはせず,それぞれの概要と利用方に関してのみを扱います.



## プログラミング言語の種類

データサイエンスは言葉の通り,データを扱います. 現在ではデータは基本的に,電子データとして収集,処理されるため,それらの編集,処理にはコンピュータを利用し,操作は基本的にはプログラムによってなされます. したがってプログラミングは,データサイエンスのための前提知識となります.

この講義では最終的には,学生それぞれに研究のためのプログラムを組んでもらいます.
データサイエンスや統計でよく使われる言語は, Python, Julia, R, SPSS, matlab などいくつかありますが, この授業では, 現在世界的に広く使われており,習得も容易なPythonを利用します.


プログラミング言語には沢山の種類がありますが,言語によって機能や得意なことが異なります.

![Langage icons](/images/2024-04-08-lecture-DS-languages.png)


プログラミング言語は **実行方式**, **書き方**,**検査の仕方**などの特徴がそれぞれ異なり,大まかにはそれぞれ以下のような意味になります.

---

**実行方式**

プログラミング言語は,人間にとって理解しやすくデータ構造やアルゴリズムを記述するための手段です.しかし,コンピュータはプログラミング言語を直接理解することはできません.そのため,書かれたプログラムはコンピュータが解釈できる形式,すなわち0と1のビット列である機械語に翻訳される必要があります.この翻訳プロセスは,プログラムの実行方式を以下の二つに分ける要因となります.



::: note
- インタプリタ方式

インタプリタ方式では,プログラムは逐次的に機械語に翻訳されながら実行されます.この方式の特徴は,コンパイルする必要がないため,翻訳と実行が同時に行われる点です.これにより,プログラムの変更がすぐに反映されるため,開発中のテストやデバッグが容易になります.しかし,実行のたびに翻訳を行う必要があるため,実行速度が遅くなることが欠点です.
:::



::: note
- コンパイラ方式

コンパイラ方式では,プログラム全体が事前に機械語に翻訳され,その結果として得られる実行可能なプログラムが生成されます.コンパイラ方式の利点は,一度コンパイルされたプログラムは,何度も実行される際に追加の翻訳が不要であるため,実行速度が速いことです.また,コンパイル時にプログラム全体を分析できるため,エラーやバグの発見が早期に行え,より安全性が高まるという利点があります.
:::

この二つの実行方式は,プログラムの性質や用途に応じて選択されます.インタプリタ方式は開発の柔軟性が求められる場合に適しており,コンパイラ方式は性能が重視される場合に好まれます.プログラマはこれらの特性を理解し,それぞれの場面で最適な選択をすることが求められます.

---


**書き方**

プログラミングとは,基本的にコンピュータに対して実行してほしい命令を記述する作業です.現在主流のプログラミング言語には,大まかに**手続き型言語**と**関数型言語**の二つの記述方法が存在します.

::: note

- 手続き型言語

この言語タイプでは,プログラムが｢何を,どうするか｣を順番に記述していきます. Python,Java,VBAなど多くの広く使われている言語がこの方法を採用しています. これにより,処理の流れが直観的に理解しやすくなります.

:::

::: note

- 関数型言語

関数型言語では,プログラムを実行によってユーザーが得たい結果を抽象化し,関数の組み合わせで記述します.このアプローチは,安全性の向上やデバッグのしやすさといったメリットを提供しますが. 概念の抽象化により理解が難しくなることがあります.

:::

近年では,手続き型言語にも関数型の構文が取り入れられるようになり,手続き型言語内で関数型風に記述することや,その逆も可能になっています.

これらの違いについて更に詳しく知りたい方は,別の[講義資料](/lectures/2024-03-29-introduction-to-algebraic-programing.html)で更に詳しく説明しています.

---

**検査の仕方**

プログラミングは,データ構造とアルゴリズムを使用して命令を記述する作業です.ここではデータ型の詳細に深くは触れませんが,あらゆるプログラミング言語において,データはコンピュータのメモリ上に数値の羅列として存在します.それらの数値に意味を与えることでデータ型が形成されます.

プログラムは実行時に,これらのデータ型が適切に使用されているかを検査します.主な検査方法には**動的型付け**と**静的型付け**があります.動的型付けでは,プログラムの実行時にデータ型が決定され,静的型付けではコンパイル時にデータ型が固定されます.さらに,型付けには「弱い型付け」と「強い型付け」という区別も存在しますが,この講義ではその詳細には触れません.興味のある方は,このトピックについてさらに調査してみてください.

::: note
- 動的型付け
動的型付けのシステムでは,プログラマが変数の型を明示的に宣言する必要がありません.代わりに,コンピュータはプログラムの実行時に型を推論し,適切な型を自動で割り当てます.この柔軟性により,プログラマはより迅速に開発を進めることが可能になります.一方で,このシステムではコンパイル時の型チェックが行われないため,実行時に型関連のエラーが発生するリスクが高まります.そのため,安全性を確保するためには,プログラマ自身が型の整合性に注意を払い,エラー処理やテストにより問題を検出する必要があります.
:::

::: note
- 静的型付け

静的型付けでは,プログラマが変数や関数の型をコード内で明示的に宣言し,これらはコンパイル時にチェックされます.この事前の型チェックにより,プログラムの安全性が向上し,実行時のエラーが減少します.また,コンパイラが型情報を利用して効率的なコード生成を行い,パフォーマンスが向上することがあります.静的型付けは特に,大規模プロジェクトや高い信頼性が求められる場合に適しています.

:::

---

プログラミング言語は,このような区分や,それ以外の様々な機能によって,用途の向き不向きが決まります(大雑把な目安です).

![Languages and usages](/images/2024-04-08-language-and-purpose.png)

ではこれらの特徴を踏まえて,Pythonとはどのような言語なのかを見てみましょう.

::: note
Pythonは

    - インタプリタ型言語で
        - 機械語に翻訳しながら動く
        - 手軽に書けて手軽に試せる
        - でも少し安全性が低く,遅い

    - 手続き型言語で
        - 次に何をするかを順番に書く
        - (ただし関数型っぽい書き方もできる)

    - 動的型付け言語で
        - 型検査を動かしながら実施
        - 動かしてから型が違うと失敗する
:::

これらは,プログラミング言語の大きな分類からみたPythonですが, Python固有の特徴として以下のようなものがあります.


::: note

    - とにかく読みやすく書きやすく覚えやすい
        - ABC言語という教育用言語が元になっている
            - 予約語が少ない,      → 覚えることが少ない
            - インデントでかき分け → 間違いが少ない
            - だれが書いても同じになる(と言われてはいる)

    - ライブラリが豊富
        - 統計処理を全部0から自分で書くのは大変
        - 他の人が作ったものを使えるようにするのがライブラリ
        - 様々な大学や企業,研究者が膨大な量の統計処理,機械学習ライブラリを開発している

    - 遅いけど,Cなどと連携しやすい.
        - プログラミング言語ごとに速度は異なる.
        - Pythonは結構遅いので大きな計算に時間がかかる.
            - とても早い言語で遅い部分を書き換えやすい
            - ライブラリは基本的に早くなっている
:::

Pythonが教育用に良く使われるのは, インタプリタ方式の動的型付け言語であることから手軽に書けるだけではなく,もともと言語として簡単に書けるように作られていることが大きいです.

また, 統計,データサイエンス分野のライブラリが充実しており, これによって誰でも簡単に複雑な統計処理やデータサイエンスの技法が利用できることで,Pythonが広く普及しています.

簡単 → 教育用に → 多くの人が使う → ライブラリが充実 → もっと多くの人が使う

という流れがPythonの最大の強みと言えるでしょう.

実際にPythonはユーザー数が増え続けており,プログラムの共有サイトGitHubにおける2023年のすべての言語のなかで2番目にユーザ数が多い言語となっています.
[Top 10 Programming languages on GitHub](https://github.blog/2023-11-08-the-state-of-open-source-and-ai/)

![Top 10 Programming languages on GitHub](/images/top-programming-languages-2023.png)


## 授業準備

この講義では,プログラムを自分で作成し,様々な演習をこなしてもらいますが,その前段階として,いくつかの準備が必要となります. このあたりはこの科目の先修科目の統計学入門でも扱っていますが,履修していない人もいますので,順番にやっていきましょう. 非常に基礎的な内容なので, 問題のない人は飛ばしましょう.

### テキストエディタ

テキストエディタとは,プログラムを書くためのソフトウェアです.
プログラムを書くことをコーディング(Coding)といいます.

テキストエディタには沢山の種類があり,それぞれ独自の機能を持っています.
Windwosに最初から入っている｢メモ帳｣もテキストエディタですが,プログラムを書くために様々な機能が追加された高機能なテキストエディタも沢山あります.

例えば,シンタックスハイライト機能は,以下のプログラムのように,プログラムの記述を役割や意味に応じて色付けして見やすくしてくれます.

~~~ python
## シンタックスハイライト
from datetime import datetime

def greet_based_on_time():
    now = datetime.now()
    current_hour = now.hour

    if 5 <= current_hour < 12:
        greeting = "Good morning, world!"
    elif 12 <= current_hour < 18:
        greeting = "Good afternoon, world!"
    else:
        greeting = "Good night, world!"

    return greeting

# 関数を呼び出して結果を表示
print(greet_based_on_time())
~~~


また,スペースをタブに変換するなどの機能も非常に便利です. 最近では生成AIを利用した自動補完機能がついたエディタなどもありますが,本講義における生成AIの利用に関しては第2回でコードを書き始めた際に説明します.

今までメモ帳以外のテキストエディタを利用したことが無い方には,シンプルなSublime Text 3をおすすめします(今世界的に人気があるのは VSCodeです.非常に便利ですが機能が多すぎて皆さんが混乱する可能性があるので,こちらを選びました).

既に何かしらのテキストエディタを利用している方は,現在使用しているエディタをそのまま利用して頂いても構いません.

[このURL](https://www.sublimetext.com/3)をクリックして,ページ上部にあるDownloadをクリックします. 自分のPCに合わせたインストール方法を選択しましょう.

![Screenshot Sublime Text](/images/2024-04-08-lecture-DS-sublime-install.png)

::: warn
SublimeTextもいろいろな機能を追加することができます.
｢SublimeText 設定｣などで調べて,好きなようにカスタマイズして構い舞いません.
カスタマイズをしなくても基本的な機能には問題ありません.
:::



## IMEの設定

プログラムは基本的に **｢半角英数字｣** で記述されます. プログラム中に全角の空白や記号が交じるとエラーの原因となる場合があります. そのため,プログラムを書く前に,そういったミスが起きないようにIMEの設定をしましょう.

タスクトレーからIMEの設定ができます．基本的に記号をすべて半角に設定しましょう（スペースは必ず半角にしましょう）．特に，句読点をコンマとピリオドに変更しましょう．

![IME](/images/2024-04-08-lecture-DS-IME.png)

## CLIの基本操作

プログラムの開発環境にはマウスなどでクリックして操作するGUI(Graphical User Interface)をもったIDE(Integrated Development Environment)などもありますが,基本的には文字によってコンピュータに命令を送るCLI(Command Line Interface)を利用します. 映画やマンガなどで,ハッカーが黒い画面に文字を打っているあれのことです.

コンピュータのオペレーティングシステムとユーザー間のCLIを提供するプログラムをShellといい,Windowsでは,Command PromptやPowerShellなどがあります. MacなどのUnix系では,Bashやzshがあります. いずれも (Windows) Terminalというソフトウェアを介して利用します.

本講義では環境や好みによって好きな環境で開発して構いませんが,ここでは,PowerShellの利用法を解説します.

Windows11の検索バーで `Terminal`と検索して,出てきた `Terminal`をクリックしましょう.

![Screenshot Terminal](/images/Terminal-launch.png)

自動的に`Windows PowerShell`が起動します. 立ち上がった,黒色の画面に文字でコマンド(命令)を入力して,コンピュータを操作します.

![Screenshot Terminal](/images/Terminal-window.png)


### エンコーディング

実際にコマンドを入力する前に, 初心者がつまづきやすいポイントとして,Windowsのエンコーディングについて解説します.

PCは人間の使う文字（日本語，英語など）が理解できません.PCは機械語と呼ばれる言語で命令を受け付けます.一方で,人間は機械語を読むのが困難です.そこで,人間の使う文字と,機械語の間に変換ルールを設けて人間の文字でされた命令をPCにわかる文字に変換します.この変換ルールを文字エンコーディングと呼びます.

エンコーディングには複数の種類があります(日本語設定のWindowsはShift-JIS,Unix系はUnicodeが一般的です).
PythonはUTF-8という文字エンコーディングがデフォルトなので,Windowsにおいても可能な限りUTF-8を用いた方が良いです.

そこで,ターミナル上で利用するエンコーディングを変更します.
PowerShellを起動して, `chcp 65001` と打ち込み, PowerShell上で利用する文字エンコーディングをUTF-8に変更しましょう. `chcp`が利用する文字コードを変更するコマンド(change code page)で,その後に変更したい文字コードを入力します. `65001`は`UTF-8`のコードページ(Windows独自の文字エンコーディング)です. これはPowerShellを起動する度に行ってください．

~~~ sh
chcp 65001
~~~

と入力すると,

~~~ sh
Active code page: 65001
PS C:\Users\user>
~~~
のように表示されるはずです.

### 日本語表示

Power Shellの設定によっては日本語が表示されず,日本語部分が `□` で置き換えられて表示されます.これは,使用しているフォントに日本語が含まれていないために発生します.


![Screenshot PowerShell](/images/powershell-tofu.png)


設定を変更してて日本語を表示可能にしましょう(
適当な日本語を入力してみて,問題なく表示されるようであれば,変更は必要ありません).

左上の`下向きの矢印 > 既定値 > 外観 > フォントフェイス`の部分を日本語フォントに変更し`保存`をクリックすることで,日本語が表示されるようになります.

![Screenshot Terminal](/images/Terminal-setting1.png)

![Screenshot Terminal](/images/Terminal-setting2.png)

![Screenshot Terminal](/images/Terminal-setting3.png)

その他色やサイズなど,好きな設定に変更できます. あとで,好みにカスタマイズしましょう.

### ディレクトリ

基礎的なコマンドを学ぶ前に,ディレクトリに関して理解しておきましょう.
コンピュータの中のデータは,以下のような木構造になっています. このような木構造によるファイルの構造をディレクトリといいます.

~~~
C: -- Users -- hoge
          |
            -- hoge2 -- Desktop
                   |
                     -- Downloads
                   |
                     -- Documents -- huga
                                |
                                  -- huga2
~~~


CLIにおいて,ユーザはこの木構造のどこかに存在しており,この木構造を移動しながら様々な作業を行います. 現在いるディレクトリのことを `working directory`(以下wd)や`current directory`といいます.


## 基礎的なコマンド

ここでは, この講義で必要となる最低限のコマンド,特にディレクトリの移動に関するコマンドを学習します.

wdは, CLIの左側に表示されていることが多いです.

~~~ sh
PS C:/Users/hoge2>
~~~

のように表示されていれば今`C:`ドライブ下の`Users`下の`hoge2`がwdとなります.

::: warn

- Windowsでは,ディレクトリを区切る文字が`¥`あるいは`\`で表示されていると思います.

- Macでは,`/`です.

本資料では,`/`を利用しています. 自分の環境に併せて適宜読み替えてください.
:::

CLIの左側に表示されていない場合にも`pwd`コマンド (print working directory)を入力すると,現在のディレクトリが表示されます.

~~~ sh
PS C:/Users/hoge2> pwd

PATH
----
C:/Users/hoge2
~~~

wdの下に何があるかを調べるコマンドとして`ls`コマンド(list)があります.

::: warn
以下, `PS C:\Users\hoge2`の部分は省略します.
:::

~~~ sh
> ls

Desktop
Downloads
Documents
~~~

※実際の画面では,もう少しいろいろな情報が書かれているかと思います.

ディレクトリ構造を確認するCommandとして`tree`があります. `tree`と入力してEnterすることで,wd以下のディレクトリ構成が確認できます.

~~~ sh
> tree
Folder PATH listing
Volume serial number is 00000157 B8F4:6480
C:.
├───Contacts
├───Desktop
├───Documents
│   ├───hoge
│   └───slds
│       └───program
├───Downloads
~~~

`tree [PAHT]`と入力すると, wdではなく指定した`[PATH]`以下のディレクトリ構造が表示されます.
また, `/f` オプションを加えることでファイルも表示されます.

~~~ sh
> tree .\Documents\ /f
Folder PATH listing
Volume serial number is 000001D1 B8F4:6480
C:\USERS\AKAGI\DOCUMENTS
├───hoge
│       hello.py
│       slds-2-10.py
│
└───slds
    └───program
            hello.py
~~~

::: warn

Macの場合は,`tree`コマンドは入っていないので,`brew`などを利用してインストールする必要があります.
`brew`に関しては自分で調べてみましょう.

また,オプションもWindowsとは異なっています.

|コマンド| 意味  |
| :---:  | :---: |
| -d     |  ディレクトリのみ表示  |
| -L N   | N 階層まで表示 |
| -P X   | 正規表現Xに従って表示 |

~~~ sh
> tree
.
├── hoge
│      └── hoge.py
└── huga
        └── huga.py

3 directories, 2 files
> tree -d
.
├── hoge
└── huga

3 directories
> tree -L 1
.
├── hoge
└── huga

3 directories, 0 files
> tree -P "hoge*"
.
├── hoge
│     └── hoge.py
└── huga

~~~

:::


wdから別のディレクトリに移動するコマンドとして `cd` コマンド(change directory)があります

`cd [移動先]` と打つことで,lsコマンドで出てきた,ディレクトリに移動することができます.

~~~ sh
> ls
Desktop
Downloads
Documents

> cd Documents
> pwd
PS C:/Users/hoge2/Documents
~~~

::: warn

移動先のディレクトリ名はすべて自分で入力する必要はありません. 最初の数文字を入力して`Tab` Keyを押すと,自動で保管してくれます.

:::


`cd ..` と打つと一つ前のディレクトリ,`cd ~`と打つとホームディレクトリ(基本的には最初に開いた際にいた場所)に一挙に移動することができます.

~~~ sh
> pwd
PS C:/Users/hoge2/Documents
> cd ..
> pwd
PS C:/Users/hoge2
> cd Documents/huga
> pwd
PS C:/Users/hoge2/huga
> cd ~
> pwd
PS C:/Users/hoge2
~~~

`mkdir [作りたいディレクトリ名]` コマンド(make directory)で,新しいディレクトリを作成できます.

`rmdir [消したいディレクトリ名]` コマンド(remove directory)で,ディレクトリを消すことができます.

~~~ sh
> pwd
PS C:/Users/hoge2
> ls
Desktop
Downloads
Documents
> cd Documents
> ls
huga
huga2
> mkdir huga3
> ls
huga
huga2
huga3
> rmdir huga3
> ls
huga
huga2
~~~

`rmdir` コマンドでは中身のあるディレクトリは消せません. オプションを追加することで消せますが,危険なのでここでは教えません.興味があったら自分で調べてみましょう.

::: note
**練習:作業用ディレクトリを作ろう**

- これから,作業をするためのディレクトリをコマンドで作成しましょう.

    - Documentsに移動
    - Programs というディレクトリを作成し移動
    - Python というディレクトリを作成し移動
    - slds というディレクトリを作成し移動
    (名前は好きに設定して良いです. slds; special lecture data science)

これからこの講義で利用するプログラムなどはsldsに保存しましょう.
:::


## 環境構築

自身の環境(PC)でプログラムが動くようにすることを環境構築といいます. Python自分のPCで動くように設定をしましょう.

Pythonの環境構築に関して説明します.Pythonを動かす方法は沢山あります.一つのソフトの中でプログラムの編集から実行まで全て完結するIDE(統合開発環境)やブラウザ上で実行する方法もありますが,ここではCLIを利用して実行する方法を準備します.Windowsを所有している学生が多いと思われるので,Windowsを前提に説明します. それ以外のOSの方は分からなければ教員に聞いて下さい.

::: note

- pythonの環境

現在のPythonの開発環境は, [`pyenv`](https://github.com/pyenv/pyenv)や[`Docker`](https://www.docker.com)を利用した仮想環境でpythonのversionやライブラリをアプリケーションごとに分ける方法が主流です.

ただし,本資料では複数のアプリケーションを設定すると手順が煩雑になり,環境変数の設定など作業量が多くなるため扱いません.興味がある方は,調べて自分で導入してみましょう.

特にMacユーザーは,以下の手順で公式サイトのインストーラーを利用した場合,新しいversionのpythonを導入する際に苦労することになるかと思います. もし,自分で環境を再構築して上手く以下なかった場合は,教員に聞いてみましょう.

:::


Pythonの[公式サイト](https://www.python.org/downloads/windows/)からPythonをダウンロードしましょう.

Windows → 最新の一つ前のバージョンの Windows installer (64-bit) をクリック

最新版を入れたいところですが,最新のVersionは色々と不具合が起きる可能性があるので,上から1つ目のVersionにしておきましょう.ここでは最新が3.12.2なので,3.11台のバージョンを入れます.(バージョンの数字は,サイトを開いた時期によって変わります.)

![Download Python](/images/python-download.png)

ダウンロードした Python-3.11.8.exeをクリックするとウィンドウが開きます. 指示に従ってインストールしましょう．

::: warn
注意点

開いたWindowで

-  `Insatall launcher for all users(recommended)`

- `Add Python 11.8 to PATH`

というチェックボックスが出てきたら,それらにチェックを入れてから次に進みましょう.

この画面もVersionなどによって変わるため,分からなければ教員に聞いて下さい.

![Install Python](/images/install-python.png)

:::

インストールが終わったら **新しくPowerShellを開いて** `python --version` と入力しましょう.
**Macの人は** `python3 --version` とコマンドが変わりますので注意してください. Macの人は以降,`python` コマンドはすべて  `python3` コマンドに読み替えてください.

install したversionのPythonが表示されれば無事インストールされています.



~~~ sh
> python --version
Python 3.11.8
~~~

## Hello World

初めて作成するプログラムとして標準出力(PowerShellなどの画面)に`Hello World`と出力するだけのプログラムを作成してみます.

`PowerShell`を開いて,自分の作業用ディレクトリに移動します.

~~~ sh
> chcp 65001
> cd Documents/Programs/Python/slds
~~~


テキストエディタで新しいファイルを開き,**作業用ディレクトリ**に`hello.py`という名前で保存しましょう.

`.py`はPythonプログラムの拡張子です.

保存は,タブから`File > save with encoding > UTF-8`の順にクリックし`UTF-8`で保存しましょう.
それ以降は `Ctrl + s`などで上書き保存しても構いません.


作成したディレクトリに移動し, lsコマンドでファイルがあるか確認しましょう.
ょう.

~~~ sh
> ls
hello.py
~~~

確認できたらhello.pyに以下の2行を書き足して上書き保存します.

~~~ python
# -*- coding:utf-8 -*-
print(“Hello world!”)
~~~

保存できたら`python hello.py`コマンドでプログラムを実行します.
(Macの人は `python3 hello.py`です.)

標準出力に`Hello World!`と表示されれば成功です.

~~~ sh
> python hello.py
Hello World!
~~~

ここまでで，皆さんはPCでプログラムを書く準備が整いました．
あとはプログラムの書き方を学習すれば自身の環境で開発が行なえます．

## プログラミングの勉強の仕方

プログラミング言語は,文字通り**言語**です. 英語などと同じ様に,使わないと身につきません.なので,大事なことは**勉強するより使う**ことです.

英語の勉強で,一つひとつ文法を覚えることは大切かもしれませんが,実際に英文を書いたり,会話しないと使えるようにはなりません. プログラミングの学習で,学習用の資料や教科書を一つ一つ**読んだり,ノートに書き写す人**がたまにいますが,かなり効率が悪い方法です. 何も分からなくてもいいので,取り敢えずプログラムを書いて実行するようにしましょう. 資料に載っている例はすべて,実際に書いて実行してみましょう.

一番いい方法は,何も分からなくても取り敢えず,プログラムをつかってしてみたいことを実行することです.1からすべての構文などを覚えようとしないで,自分がやりたい,しなければならないことを,書き始めましょう. **分からないところだけを**調べれば十分です.

その際に,新しく知ったことは,メモなどして,あとで参照できるようにしておきましょう. その資料が後々役に立ちます.

プログラミングは,すべて暗記する必要はありません. 覚えていなくても,それの調べ方や載っている資料を知っていれば十分です. 何回も使う機能であれば自然に覚えます.

**ただし,** Pythonは非常に多くの人が利用しているので,みなさんがこの講義で習うレベルのことは検索すれば既に,完成したプログラムを参考にすることができます. また,ChatGPTなどの生成AIを利用すれば,皆さんの書いたプログラムよりかなりできの良いものをすぐに得ることができます. それらを見て勉強することは大切ですが,ただ,只管コピペしたり,生成AIの結果を利用しているだけでは,全く身につきません. 検索して出てきたコードや,生成AIによるコードを利用しても構いませんが, それをただ貼り付けるのではなく,**自分で読んで,理解する**ようにしましょう.

知らない部分,自分では書けない部分が見つかったらそれを理解するように勉強しましょう. ChatGPTに質問しても構いません. ただ,聞いたことを説明できるようにしましょう.

自分で説明できないものを使えるのは**せいぜい学校の課題くらいです**. 仕事や研究では,中身の良く分からないコードは利用できません. 最低限自分で理解したものだけを利用してください.

講義では,結果があっていればよいだけではなく,**なぜそのように書いたのか**,**どのような意味か**,**なぜこうなるのか**などを皆さんに聞きますので,理解に務めるようにしましょう.





::: note

**演習**

- <u>演習1 Shellコマンドの調査</u>

今回ならったいくつかのコマンド以外にも便利なコマンドが沢山あります. 3つ調べて,使い方を説明してください.

- <u>演習2 チートシートの作成</u>

チートシートとは「それだけを見れば必要な情報が分かるメモ書き」のことです.
プログラミングの学習では, 自分でノートを取って,それさえ見ればなんとかできる資料を作る のは非常に有用です.

`.ppt`でも`.txt`でも形態は何でも良いので, 自分だけのチートシートを作成して, 今日覚えたこと,調べたことなどをメモしていきましょう.

◦ CLI, Pythonの基礎, Pandasなど学習の区切りごとにシートを分けると便利です.

◦ スライドサイズを事前に大きく設定しておくと便利です

書き方は自由ですが, [検索して](https://www.google.com/search?newwindow=1&client=safari&sca_esv=635483f0705dd420&sca_upv=1&q=python+%E3%83%81%E3%83%BC%E3%83%88%E3%82%B7%E3%83%BC%E3%83%88&uds=AMwkrPtd7EXxieMQKehnHvZf8S6pNpVcJzQvmfSMfustdt5GVU4paqDezVS1KwRLLfnWdTPfRKqI2vio1swnpAMe65pQfFGqsLrhLm9FBruViqbu8gs83egRP5pID9cAUWbOoFAII8Bi4Sa9pC2nZB8uNY_Im_IVhSc-ISQJazoPJyS7Y5hq8j_wtxEDwusT0kav7Mhx_On2phc8rZKZ0rWb3f5AykS1iJlyMqAYBs5Ke8QgUSIasP6AkeP9P6DIKWQaXCglNYTRDcA_cLO0qv_MU45G2ng9BfipwmjcaL4l2EsY-TdRRazsTUowT__BAUHIv2lsEG32&udm=2&prmd=ivsnmbtz&sa=X&ved=2ahUKEwiA48qB-bOFAxU3aPUHHYJmBQsQtKgLegQICRAB&biw=1147&bih=1269&dpr=1)有名なチートシートを参考にしてください.

このチートシートは後で紹介してもらいます.

- <u>演習3 `print()`の利用</u>

今回作成した`hello.py`における
`print()` という関数は()内の文字(`""`で囲われている部分)を標準出力する関数です.
`print()`の括弧内の `Hello World!` 部分を好きな文字に書き換えて実行してみましょう.

:::

::: warn

**演習について**

この資料では,所々に演習が指定されています.
演習は自分で行い, どのようにやったのかを講義中に発表,あるいは宿題として提出してもらいます.

家で演習を進めた場合には,講義中にそれが再現できるように,

- どのようにやったのか
- なぜやったのか

などを人に説明できるように**必ず作成したメモ,資料**などを残しておきましょう.
:::


</details>

# Pythonことはじめ

<details>
    <summary> 開く </summary>



第2回では,Pythonの基礎の基礎を学習します.
本講義はプログラミング自体の学習を対象としているわけではないので,
講義内で必要な技能を紹介するのみに留めます.

基本的には,公式の[Pythonチュートリアル](https://docs.python.org/ja/3/tutorial/index.html)の内容で必要十分ですので,そちらにそって学習を進めます.
より詳しい内容を勉強したい場合には,プログラミングの講義を履修するか,
[千葉商科大学IEEESB](http://cucieee.stars.ne.jp/index.html)
などに参加するのが良いかと思います.

今回は取り敢えず,以下の作業を自分で行いながら,Pythonの体験をしてみましょう.

## REPLを使ってみよう



プログラムの対話環境全般をREPL (Read Eval Print Loop)と呼びます. 長いプログラム(スクリプト)を書かなくても対話的にプログラムを実行することができます.

Pythonには複数のREPLがあり,iPython, jupyternotebook, Google Colaboratoryなどがあります.ここではとりあえず,iPythonを利用します. 他のものについても,後ほど出てきます.

CLI(PowerShellやTerminal)を開いて,`python`(Mac の人はPython3)とだけ打ってEnter Keyを入力するとPythonのREPLが立ち上がります.

~~~ sh
~/Desktop
python3
Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>>
~~~

プログラムを書いてEnter Keyを押すとその行のプログラムが実行されます.

~~~ python
>>> 1 + 1
2
~~~


終了するには,`exit()`と入力します.

~~~ sh
~/Desktop 1m 7s
Python 3.10.11 (v3.10.11:7d4cc5aa85, Apr  4 2023, 19:05:19) [Clang 13.0.0 (clang-1300.0.29.30)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> 1 + 1
2
>>> exit()
~/Desktop 1m 7s
~~~

## コメントアウト

REPL上でもスクリプトでも,\#を先頭につけると,その行はコメントとして扱われます.

ただのコメントであり,プログラムとしては何も実行されません.

~~~ python
>>> # これはコメント何も起きない
>>>
~~~

## データ型

第1回の検査に関する説明で軽く触れましたが, プログラミングで扱うデータはコンピュータのメモリ上に数値の羅列として存在します. それらの数値に人間が解釈可能な意味を与えたものをデータ型といいます.

Pythonで最初から準備されているデータ型(**組み込み型**)には以下のようなものがあります.
詳細はこのあと順番に見ていきますので, こんなものがあるということだけ,頭に入れておきましょう.

::: note
Pythonのデータ型(一部)

- 数値型:数値を表す
    - 整数(`int`)
    - 浮動小数点数(`float`)
    - 複素数(`complex`)

- 文字列型(`str`):文字を表す

- リスト(`list`):いくつかのデータをまとめたもの

- タプル(`tuple`):データの組み合わせ

- 辞書型(`dict`):`key`と`value`からなる辞書を表す

- 真偽値(`bool`):正しいか正しくないかなどを表す
:::



### 数値型(Pythonを電卓として使う)

REPLの動きに慣れるために,電卓で行うような簡単な計算をREPL上で行いましょう.
電卓で行う計算なので,ここで扱うデータ型は数値型になります.
Pythonは,における数値型には整数を表す`int`,小数を表す`float`,複素数を表す`complex`があります. 本資料では,その内`int`と`float`について扱います.

基本的には`1`や`100`のような整数を書けば`int`型として認識され,`1.0`や`100.3`のように小数点をつけると`float`型として認識されます.


~~~ python
>>> 1
1
>>> 1.0
1.0
~~~

Pythonでは`type()`の丸括弧の中にデータを記述することで特定のデータのデータ型を確認することができます.

~~~ python
>>> type(1)
<class 'int'>
>>> type(1.0)
<class 'float'>
~~~

整数や後述の文字など変換可能なデータ型から,`int`型へ変換するには`int()`の中に,そのデータを書きます. `float`型に変換するには,`float()`を使います.`int`から`float`への変換では,小数点以下が切り捨てられます.


~~~ python
>>> int(4.9)
4
>>> float(2)
2.0
>>> float("2")
2.0
~~~

Pythonでは複数の数値型が混ざった計算に対応しています. 基本的に,特定の計算で複数の数値型が混じっている場合には, `int`型は`float`型に,`float`型は`complex`型に自動で拡張されるので本講義の範囲ではそれほど意識する必要はありません.

簡単な計算に用いる記号は以下のとおりです.

|  計算      | 記号      |
| ------     | ----      |
| 足し算     | `+`       |
| 引き算     | `-`       |
| 掛け算     | `*`       |
| 割り算     | `/`       |
| 整数除算   | `//`      |
| 剰余(余り) | `%`       |
| 累乗       | `**`      |
| 絶対値     | `abs()`   |
| 整数変換   | `int()`   |
| 小数変換   | `float()` |

実際の計算は以下のようになるはずです.

計算は普通の電卓と同じような感覚で使えます.

~~~ python
>>> # 足し算
>>> 1 + 1
2
>>> # 引き算
>>> 10 - 5
5
>>> # 掛け算
>>> 2 * 3
6
>>> # 割り算
>>> 100 / 5
20.0
~~~

計算には順序があり丸括弧 `()`で囲うことで計算する順番を変えることができます.

~~~ python
>>> (50 - 5 * 6) / 4
5.0
>>> 50 - 5 * 6 / 4
42.5
~~~


マイナスの数は数値の前に`-`をつけます

~~~ python
>>> 10 + -5
5
~~~

割り算の商と余りは`//`や,`%`で計算できます.

~~~ python
>>> # 整数除算(余りを表示しない)
>>> 6 // 4
>>> 1.0
>>> # 剰余(余り)
>>> 17 % 3
2
>>> 5 * 3 + 2
17
~~~

同じ数字をX回掛けたものを累乗といい,`**`といいます. `2 ** 3 = 2 * 2 * 2`

~~~ python
>>> # 累乗
>>> 2 ** 2
4
>>> 2 ** 3
8
~~~


`abs()` の中に数値を入れることで絶対値が計算できます

~~~ python
>>> abs(4)
4
>>> abs(-4)
4
~~~



::: note

**演習**

以下の計算をREPLを使って自分でしてみましょう.
Pythonの計算になれることが目的ですので,どのように計算したかを説明できるようにしましょう.

- 飴が40個あります.7人で同じ数ずつ分けると1人分は何個で何個あまりますか?

- 底辺5cm,高さ4cmの三角形の面積はいくつですか?

- 2の8乗はいくつですか?

- 累乗と掛け算の計算順序を丸括弧を使った計算で確かめてください.

:::

## 変数と代入



値に名前をつけることを`代入`といい,名前のついた値を`変数`といいます. `=` の左側に付けたい名前,右側に値を書きます.

プログラムでは,データはコンピュータの記憶領域(メモリ)に格納されています. メモリは,1バイト(8bit, 1bit は0/1の値)ごとにアドレスという連番がついています.


![memory null](/images/memory-null.png)

~~~ python
>>> x = 10
~~~

変数を宣言するということは,このアドレスにデータを割り当てることを意味します.
`x = 10`という変数が4バイト利用するとしたら以下のようにアドレス 201 から204 に 10という数字(int)を割り当てるイメージです.

![memory 10](/images/memory-10.png)

`x`のアドレスは`id(x)`で確認できます.

~~~ python
>>> id(x)
4438639120
~~~

`y = x` と変数に別の名前を宣言する場合は,同じ場所が参照されます.

~~~ python
>>> y = x
>>> id(x)
4438639120
>>> id(y)
4438639120
~~~

![memory 10](/images/memory-10y.png)

`x=15`と再度代入する(再代入)と,新しくメモリが確保されます.
その際に,`x`をコピーしていた`y`の値も変わることに注意しましょう.

~~~ python
>>> x = 15
>>> id(x)
4438639280
>>> id(y)
4438639280
~~~

![memory 15](/images/memory-15.png)

数値の場合には`y`に再代入しても,`x`の値は変わりません.

~~~ python
>>> x = 10
>>> y = x
>>> id(x)
4438639120
>>> id(y)
4438639120
>>> y = 15
>>> y
15
>>> x
10
>>> id(x)
4438639120
>>> id(y)
4438639280
~~~

この挙動は,後に出てくる配列では,異なるので注意が必要です.


::: note
変数に名前をつける際には,以下の点に注意しましょう.

- 小文字で始まる英数字を使う

- 複数の単語を使うときは`_`(アンダーバー)でつなげる

- 英語で名前をつける
    - `nagasa` ではなく `length`
    - `namae` ではなく `name`

- 長くなっても良いので他の人が見たときに意味がわかる名前をつける
    - 三角形の高さを表したいとして
        - `x`や`h`などの一文字よりも
        - `height`のほうが良い
        - 他にも高さを表す変数が登場するなら
        `triangle_height`のほうがより分かりやすい

~~~ python
>>> # 長方形の面積を求める
>>> width = 20
>>> height = 5
>>> area = width * height
>>> area
900
>>> #定義されていないものはエラーがでます
>>> space
Traceback (most recent call last):
 File "<stdin>", line 1, in <module>
NameError: name 'space' is not defined
~~~
:::

::: note
**演習**

- 変数を利用して以下の猫型ロボットのBMIを計算してください
    - BMI = 体重(kg)÷身長(m)の2乗
    - 猫型ロボットの身長 129.3cm
    - 猫型ロボットの体重 129.3kg
:::

### 代入演算子

代入は`=`でつなげる以外にもいくつかのパターンがあります.
まずは,普通に変数に数値を代入してみます.

~~~ python
>>> x = 1
>>> x
1
>>> x = 2
>>> x
2
~~~

このように具体的な値を`=`の右側に記入するのは直感的に分かりやすいのですが,pythonのプログラムを見ていると,左右に同じ変数名が登場する場合があります.

~~~ python
>>> x = 1
>>> x = x + 1
>>> x
2
~~~

これは,**右側に登場する**`x`は過去の`x`を表しており,**左側に登場する**`x`は,過去の`x`を利用して作られた新しい`x`であると解釈しましょう.

上の例では,`x=1`という過去の変数を使って, `x(=1) + 1`という新しい`x`を作っています.
これは**自己代入**と呼ばれ,数学や関数型言語における**再帰**とは異なり,手続き型言語独特の記法なので注意しましょう.

足し算`+`を使った自己代入は省略して, `x += 1`のように書けます. これは `x = x + 1`の省略形で**複合代入演算子**といいます.

同様に,引き算`-=`,掛け算 `*=`, 割り算 `/=`などの代入演算子も存在します.

~~~ python
>>> x = 10
>>> x -= 5
>>> x
5
>>> x *= 5
>>> x
25
>>> x /= 5
>>> x
5
~~~

## 文字列型

ここまでは,数値のみを扱ってきましたが,Pythonには数値以外にもいくつものデータ型が存在します.
次に, 文字を表す`文字列型(str)`の利用法について見ていきましょう.

文字列型は,文字を`""`(ダブルクオーテーション),あるいは`''`(シングルクォーテーション)で囲みます.

~~~ python
>>> "イヌ"
'イヌ'
>>> 'ネコ'
'ネコ'
>>> type('ネコ')
<class 'str'>
~~~

三連引用符`"""`で囲むことで複数行書くことができます.

~~~ python
>>> """ あ
... い
... う"""
'あ\nい\nう'
~~~

`\n`は改行を表しています.

~~~ python
>>> print('あ\nい\nう')
あ
い
う
~~~

### 文字列の演算

文字列は`+`で連結,`*`で反復させることができます.

~~~ python
>>> name = '太郎'
>>> '私は' + name  + 'です!'
'私は太郎です!'
>>> name * 3
'太郎太郎太郎'
~~~


変数を文字列の中で使いたいときには,上の例のように`+`で連結することもできますが,変数が文字列型ではないときには,`str()`を利用して文字列型に変換してから,結合する必要があります.

~~~ python
>>> cat_num = 10
>>> type(cat_num)
<class 'int'>
>>> #そのまま文字列と結合するとエラーが出る
>>> '私はネコを' + cat_num + '匹飼っています.'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: can only concatenate str (not "int") to str
>>> #str()を利用して文字列に変換する
>>> '私はネコを' + str(cat_num) + '匹飼っています.'
'私はネコを10匹飼っています.'
~~~

また,変数の値を文字列の中で利用する場合には`str()`や`+`を利用して結合する以外にも`f'文字列'`という記法を利用することができます.文字列の前に`f`と書くと,文字列内の`{変数名}`の部分が変数の値に変更されます.

~~~ python
>>> f'私はネコを{cat_num}匹飼っています.'
'私はネコを10匹飼っています.'
~~~

`{変数名}`の部分には式を入れることも可能です.

~~~ python
>>> f'私はネコを{cat_num*10}匹飼っています.'
'私はネコを100匹飼っています.'
~~~

文字列が代入された変数の後ろに`[]`をつけて,番号を`[]`の中に入れると,指定した番号番目の文字が取得できます. このような`[]`で指定する数字を`index`といいます.

`Python`という文字に対して,indexは以下のように振られています.
`-`をつけて後ろから数えることもできます.

|文字列    |  P    |  y    |  t    |  h    |  o    |  n    |
| :---:    | :---: | :---: | :---: | :---: | :---: | :---: |
|前から    |  0    |  1    |  2    |  3    |  4    |  5    |
|後ろから  |  -6   |  -5   |  -4   |  -3   |  -2   |  -1   |

Pythonの文字列は,最初の文字を`0`番目と数えるので注意しましょう.

~~~ python
>>> word = 'Python'
>>> word[0]
'P'
>>> word[5]
'n'
>> word[-6]
'p'
~~~

インデックスの`[]`の中で,`[Start:End]`のようにはじめと終わりのインデックスを指定することで,文字列の1文字ではなく,部分的な文字列を取得することもできます. これを**スライス**といいます.

終わりは,一つ手前までになるので注意しましょう.

~~~ python
>>> word[0:2]
'Py'
>>> word[2:5]
'tho'
~~~

はじめか終わりのインデックスを省略すると,**以降全て**という意味になります.

~~~ python
>>> word[:4] #0から4まで
'Pyth'
>>> word[2:] #2から最後まで
'thon'
~~~



::: note
**演習**

- 演習1

`'abcdefg'` から `'cde'`をスライスで抜き出してください.

- 演習2

`x = 'abcdefg'`と定義して, xに操作を加えて`'abfg'`を作ってください.

- 演習3

`x = 'abcdefg'`と定義して, xに操作を加えて`'bbbeee'`を作ってください.

:::

## リスト

複数の値をまとめるデータ型の一種に**リスト型**があります. コンマで区切って角括弧の中に複数の値を書くことで,ひとまとまりのデータを作れます.
リストの中身一つ一つを**要素**といいます.

~~~ python
>>> squares = [1,4,9,16,25]
>>> squares
[1,4,9,16,25]
~~~

リストは文字列と同じ様に,インデックスやスライスで要素を取得できます.

~~~ python
>>> squares[0]
1
>>> squares[-1]
25
>>> squares[-3:]
[9,16,25]
~~~

### リストの演算

リストも`+`で連結,`*`で反復させることができます.

~~~ py
>>> [1,2,3] + [4,5,6]
[1,2,3,4,5,6]
>>> [1] * 3
[1,1,1]
>>> [1,2] * 3
[1,2,1,2,1,2]
~~~

リストはインデックスやスライスで指定した要素に値を再代入して変更することができます.

~~~ py
>>> animals = ['cat','dog','bird']
>>> animals[1] = 'mouse'
>>> animals
['cat','mouse','bird']
>>> animals[1:] = ['fish','pig']
>>> animals
['cat','fish','pig']
~~~

::: warn

リストは,数値などとは**変数に別の名前をつけたときの挙動が異なる**ので注意が必要です.

数値や文字列は,別の名前をつけた変数に再代入した場合もとの変数は,変更されません.

~~~ py
>>> cat = 'cat'
>>> cute_cat = cat
>>> cute_cat = 'cute_cat'
>>> cat
'cat'
>>> cute_cat
'cute_cat'
~~~

`cute_cat`を変更しても`cat`は変わりません. 代入の説明箇所で見たように,数値などの変数の場合は,`cute_cat`には`cat`の値が渡されており,値が同じ場合には同じアドレスを参照するが,値が変更された場合には,
新しいメモリが確保されます.

しかし,リストは別名の変数を変更すると元の変数の値も変更されます.

~~~ py
>>> animals = ['cat','fish','pig']
>>> species = animals
>>> species[0] = 'horse'
>>> animals
['horse', 'fish', 'pig']
>>> species
['horse', 'fish', 'pig']
~~~

`animal`の別名`species`を変更すると`animal`も変更されています. これは,配列などは新しい名称の変数に,値ではなくアドレスを渡していることによります.どちらの変数もずっと同じアドレスを参照しつづけるため,片方が変化すると同じアドレスを参照しているもう片方の値も変わります.

~~~ py
>>> id(animals)
4441753024
>>> id(species)
4441753024
>>> species[0] = 'cat'
>>> id(species)
4441753024
~~~

しかし,インデックスやスライスによる要素の変更ではなく,全体を再代入した場合には新しいアドレスが割り当てられます.

~~~ py
>>> species = ['a','b','c']
>>> id(species)
4441834752
~~~

同じ値を持つが異なる場所を参照するリストを作りたい場合には,`copy()`を利用します.

~~~ py
>>> animals = ['cat','fish','pig']
>>> species = animals.copy()
>>> id(animals)
4441764160
>>> id(species)
4441798208
>>> species[0] = 'dog'
>>> animals
['cat', 'fish', 'pig']
>>> species
['dog', 'fish', 'pig']
~~~

このような挙動は今後出てくる`pandas`などの配列でも同様なので,注意が必要です.

:::

`append()`というメソッドを使って,リストの末尾に要素を追加することができます.

~~~ py
>>> animals
['cat','fish','pig']
>>> animals.append('dog')
>>> animals
['cat','fish','pig','dog']
~~~

リストの長さ(要素数)を知りたい場合には,`len()`関数を利用します.

~~~ py
>>> animals
['cat','fish','pig','dog']
>>> len(animals)
4
~~~

::: warn
`メソッド`,`関数`という言葉が説明無しに突然でてきました.
これらの違いについて理解するには段階が必要なため,後に説明します.
ここでは,変数などの後ろに`変数.f()`の形で`.`を利用してつけるものを`メソッド`,単独で`f()`のように利用するものを`関数`ということだけ覚えておきましょう.
なお,いずれも`()`の中に値や変数を書いたり書かなかったりしますが,その意味についても後ほど扱います.
:::



リストはリストも要素にすることができます. このようなリストを**多重リスト**と呼びます.

~~~ py
>>> x = [1,2,3]
>>> y = [4,5,6]
>>> z = [x,y]
>>> z
[[1,2,3],[4,5,6]]
>>> z[1]
[4,5,6]
>>> z[0][1]
2
~~~

::: note
**演習**

`xs = [[1,2,3],[4,5,6],[7,8,9]]` というリストを作り,以下の操作を行ってください.

- `xs`の長さを求める

- スライスを使って以下を抽出する
    - `[[4,5,6],[7,8,9]]`
    - `[[1,2,3]]`
    - `[[7,8,9]]`
    - `[8,9]`

- `[4,5,6]`を`[-4,-5,-6]`に更新する

- `1` を `-1`に,`9`を`-9`にする

- `[7,8,-9]`のあとに,`[10,11,12]`を追加する

:::

## タプル

Pythonにはリスト以外にも複数のデータ型の組み合わせを表すデータ型が存在します. タプルは,データの組を表すデータ型であり,`()`の中に`,`で区切ってデータを入れることでリストのようにデータを格納することができます.
タプルも複数のデータをまとめることができます.

しかし, リストのように扱うことは推奨されません.
(後に扱う関数などで返り値を複数返したいときなど)基本的に2,3個のデータの組を扱いたい場合に利用して,3個以上のデータを扱い場合にはリストなどを使うようにしましょう.

~~~ py
>>> name_and_age = ('Taro',10)
>>> name_and_age
('Taro', 10)
~~~

タプルの値の取り出しには,同じ形のタプルに変数を格納することで値を取り出す`パターンマッチ`が良く利用されます.

~~~ py
>>> (name,age) = name_and_age
>>> name
'Taro'
>>> age
10
~~~


インデックスによる値の取得も可能です.

~~~ py
>>> name_and_age[0]
'Taro'
>>> name_and_age[1]
10
~~~


ただし,インデックスを利用した要素の変更はできません.

~~~ py
>>> name_and_age[0] = 'Hanako'
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: 'tuple' object does not support item assignment
~~~

## 辞書型

名前とその意味, 商品と在庫数,日本語名と英語名など,特定のデータに対応する別のデータの組み合わせを沢山扱いたい場合には,`辞書型`(dict)が利用されます. 辞書型は`key`と`value`と呼ばれるデータの組み合わせからなります.

辞書型のデータは,`{key1:value1,key2:value2,...}`のように,`key`と`value`の組み合わせを`:`で表して,`{}`にコンマで区切るかたちで作成します.

例えば, 学生と学生の出席回数の組み合わせを表すデータは以下のように作成されます.

~~~ py
>>> attendance = {'Taro':10,'Hanako':12,'Kenta':9,'Shizuka':10}
>>> attendance
{'Taro': 10, 'Hanako': 12, 'Kenta': 9, 'Shizuka': 10}
~~~

`dict()`に`key`と`value`のタプルのリストを渡すことで生成する事もできます.

~~~ py
>>> attendance = dict([('Taro',10),('Hanako',12),('Kenta',9)])
>>> attendance
{'Taro': 10, 'Hanako': 12, 'Kenta': 9}
~~~

特定の`key`でそれに対応する`value`を呼び出すには,`key`をインデックスとして`[]`を利用します.

~~~ py
>>> attendance['Taro']
10
>>> attendance['Shizuka']
10
~~~

新しい`key:value`を加える場合にも,`value`を変更する場合にも,インデックスによる代入が利用できます.

~~~ py
>>> attendance['Taro'] = 11
>>> attendance['Taro']
11
>>> attendance['Shinzi'] = 12
>>> attendance['Shinzi']
12
~~~

`key`を削除するには`pop(消したいkey)`メソッドを利用します.

~~~ py
>>> attendance.pop('Hanako')
>>> attendance
{'Taro': 11, 'Kenta': 9, 'Shizuka': 10, 'Shinzi': 12}
~~~

辞書型のデータから,`key`のみ,`value`のみを抜き出すには,`keys()`,`values()`メソッドを利用します.
リストとして取得したい場合には,`list()`関数で囲みます.

~~~ py
>>> attendance.keys()
dict_keys(['Taro', 'Kenta', 'Shizuka', 'Shinzi'])
>>> attendance.values()
dict_values([11, 9, 10, 12])
>>> list(attendance.values())
[11, 9, 10, 12]
~~~

これらの処理はリストを利用しても可能ですが,辞書型のほうが計算が速いため辞書型を利用するようにしましょう. プログラミングにおいては用途に応じて適切なデータ型を選択することが重要です.

::: warn
同じような処理が可能なデータ型でも,その処理を実行するのにコンピュータが必要な計算の数や,使用するメモリの量,速度などに違いがあります.

この資料では扱いませんが,大規模なデータを扱う場合や,大量の計算を行うプログラムを書く際には,`計算量`を考慮して`アルゴリズムとデータ構造`を適切に設計する必要があります. 興味のある方は,調べてみましょう.
:::

::: note

**演習**

- 5種類の果物の日本語名と英語名を変換する辞書を作成し,実際に機能する様子を紹介してください.

- 上で作成した辞書にもう一つ果物を追加してください.


:::


## 論理演算

それが正しいか間違っているか判別できる文を**命題**といいます. 命題の結果を表すものとして真(正しい),偽(間違っている)という値を用います. 真と偽を併せて**真偽値**といいます.

例えば,`1は2より大きい`という命題は,間違っているので**偽**となります. `人間は必ず死ぬ`という命題は,今のところ不老不死の人間がいないので**真**です.

プログラミングではこのような命題の判断がしばしば必要となるため,それらを扱うデータ型が提供されています.

真偽値を表すデータ型として`Bool`があります. `Bool`は`True`(真),`False`(偽)のいずれかです.

Pythonには命題の判定を行う演算子として,以下のようなものが準備されています.

|記号       | 意味         |
| :---:     | :---:        |
| `>`       |  より大きい  |
| `>=`      |  以上        |
| `<`       |  より小さい  |
| `<=`      |  以下        |
| `==`      |  等しい      |
| `!=`      |  等しくない  |
| `in`      |  含む        |

数値などの大小関係を調べるときには,比較演算子 `>`,`>=`.`<`,`<=`を利用します. 演算子の左右に数値を書くと,結果に応じて真偽値が帰ってきます.

~~~ py
>>> 1 > 2
False
>>> 1 < 1.5
True
~~~

リストや文字列に特定の要素(文字列)が含まれているかは,`in`で判定できます.

~~~ py
>>> 'ab' in 'abcd'
True
>>> 1 in [3,4,5]
False
~~~

値が等しいか/等しくないかを判定するには,`==`と`!=`を利用します.

~~~ py
>>> 4 == 4
True
>>> 'cat' != 'cat'
False
~~~

これ以外にもPythonにはいくつかの演算が準備されていますし,自分で作ることも可能です.

`True` や `False`などの`Bool`値は, `AND`(かつ),`OR`(または),`NOT`という演算で計算することができます(`XOR`というのもあるが省略).
PythonではAND は `&`, OR は `|`, NOT は `not` という演算子が提供されています.

A,Bが命題だとして,`A & B`は両方`True`のときに,`True`となります. `A | B`は片方どちらかが`True`のときに`True`となります.

例えば,

- `1は2より大きい かつ 2は0より大きい` という命題は,`2は0より大きい`は正しいですが,`1は2より大きい`が間違っているので全体として,`False`です.

- `ネコは哺乳類である または ネコは鳥類である`という命題は `ネコは鳥類である`が間違っていますが全体としては`True`です.

演算の結果は,それぞれ以下のようになります. これを真偽値表といいます. ここでは,最低限の例だけを紹介しますが,より深く理解したい人は論理学などの講義を受講しましょう.

| 命題Aの値 | Bの値 | `A & B` | `A | B`|
| :---:     | :---: | :---:   | :---:  |
| True      | True  | True    | True   |
| False     | True  | False   | True   |
| True      | False | False   | True   |
| False     | False | False   | False  |

Pythonではそれぞれの命題を丸括弧で囲んで,`&`,`|`演算子で論理演算を行うことができます.


~~~ py
>>> (1 > 2)
False
>>> (2 > 0)
True
>>> (1 > 2) & (2 > 0)
False
>>> (1 > 2) | (2 > 0)
True
~~~

`not` は命題の否定を表しており `True`が`False`,`False`が`True`になります.`not`は命題の前に書きます.

~~~ py
>>> (1 > 2)
False
>>> not (1 > 2)
True
~~~

::: note

**演習**

ある値が偶数かどうかは,2で割った余りが0かどうかを判定することで判定できます.
`x=101`,`y=202`として, 以下の命題の真偽をPythonで計算してください.

- xが偶数
- yが偶数
- xが偶数かつyが偶数
- xが偶数またはyが偶数
- x + y が奇数

:::


## (復習)スクリプトの実行

これまでは,対話環境でプログラムを実行してきましたが,対話環境は複雑な処理には適しません.
これから,1行1行プログラムを記述して対話環境で実行するのではなく,複数行のプログラムをまとめて記述して一気に実行する方式に切り替えます. 複数行のプログラムを一つのファイルにまとめたものをスクリプトファイルと呼び, 書かれているプログラムをスクリプトといいます. 初回に行った`Hello World`はスクリプトを実行していました.

::: note
**Pythonのスクリプト実行の方法**

- テキストファイルにプログラムを書く

- ファイルの拡張子を`.py`にして保存する
    - 講義で作成したプログラムは,あとで自分が参考にできる最高の資料です.
        - あとで何をやっているのか自分が理解できるように
        プログラムにはできるだけ沢山のコメントを付けましょう

        - ファイル名は,後からみて,中身が何であるかわかるよう`英数字`で名付けましょう
            - `a.py`や`file.py`,`課題.py`などはやめましょう

- Shell上でそのファイルが保存されている場所に移動する

- `python ファイル名` コマンドで実行する
    - ※ `python` は空白の後に続くプログラムを実行するためのコマンドです.
    - ※ Macの人は `python3` を利用します.
:::


::: note

**演習**

第1回で行ったことを参考に,`Let's start the Python programming!!` と表示されるプログラムを作成しましょう. ファイル名などは適切に名付けてください.
まだ,今後沢山のスクリプトを書いていきますので,適切にフォルダなどを整理しましょう.

:::

## エラーへの対応法とよくある間違い

これまで皆さんは1行のプログラムを記述して, REPL上で実行してきました. これから何行かに渡るプログラムを記述すると, うまくできない人が出てきます. ここでは,学生のつまずきやすいポイントとその対策について,事前に学習しておきましょう.

### エラーへの対応方法

間違った手順,プログラムの記述方法でプログラムを実行すると,エラー文がTerminalに表示されます. どれだけプログラミングが得意な人でも, 完璧な作業はできません. 必ずエラーが発生します. そのような意味でも,プログラミングをするというのは,プログラムを書いて発生したエラーに対応するということでもあります.

エラーへの対応は, プログラミングにある程度習熟した人でも,Webなどで調べて解決する場合が多いです.PCやプログラミングができるということは,すべての場合をすべて記憶して対応できるということではなく,**問題が起きたら自分で解決できる**ということを意味しています.

したがって, まず必要なのは問題が起きたら解決策を**自分で調べること**です.

解決方法を調べるためには,検索するためのワードとして,**機能や名称を知っていること**が重要です.
例として, Excelのフィルター機能の存在を知っていれば, **Excel フィルター 使い方**などで検索することができます.しかし,Excelも,フィルターも知らなければ,調べることすらできません.

すべての概念や機能を最初から完全に理解する必要はありませんが,概念の存在や名称を覚えるようにしましょう. そのためにも自分の**メモやチートシートを作成,整理しておくこと**が重要です.

プログラミングの学習において, 何かが間違っている場合には, Terminalにエラー文が表示されます. Pythonはエラー文が親切なので,エラー文を読めば大抵のことは解決できるようになっています.

しかし,エラーが起きても**エラー文を一切読まない人**が一定の割合で存在します. そのような人に理由を尋ねると最も多い理由は**英語で書かれていること**,2番目に**どこを見ればいいのか分からないこと**を挙げます.

それほど難しい英語は使われていませんが,まず英語で書かれていても読んでみましょう. 英語が理解できなければ,機械翻訳にかけましょう.

pythonのエラー文は,基本的にエラー文の**一番最初にスクリプトのエラーが発生している場所**が書かれています.また,**一番最後にどのようなエラーが起きているのか**が書かれています. 複雑なプログラムになると,エラー全体を理解する必要が出てきますが,この講義で扱う程度の事例に関してはその2個所のみを読めばほとんどが解決します. ただし,検索結果は珠玉混合です, 正しい情報の取捨選択に関しては,情報入門の教科書などで復習しておきましょう.

![エラー文の読み方](/images/python-error-sample1.png)

![](/images/translate-error.png)

エラー文を読んでも意味が理解できない,あるいは対処方法が分からない場合には,エラー文の最後をそのまま検索しましょう. なお,Googleでは,`""`で囲うことで文章ごと検索(センテンス検索)できます. Pythonは日本語ユーザーも非常に多いため,`エラー文 Python`で検索すれば,大抵の問題は日本語で解決方法を読むことができます.

解決しない場合には,AND検索で情報を付加して,結果を絞りましょう. 検索条件に加えるべき情報の候補としては以下のようなものがあります.

::: note

- OS (Windows, Mac)

- Pythonのversion

- 利用しているライブラリ

- やろうとしている作業

:::

例えば,これから行う`pandas`を利用した`ファイルの読み込み`において`No such file or directory`というエラーが出た場合には `pandas ファイル読み込み "No such file or directory" Windows`などで検索してみましょう.

**エラー文を読んで**,**Webで検索しても**問題が解決しない場合には, 教員に聞いて下さい. 専門的な内容になるほど,日本語のページは少なくなります.また,新しい情報に関しては,日本語に翻訳されておらず公式のドキュメントなどを読む必要があります. それでも解決しない場合には, Pythonのコミュニティなどで質問をする必要があります. 最終的にはこれらを自分でできるようになる必要がありますが,最初は難しいと思います. 講義の教員は,それらを代替するためにいますので,教員に聞きましょう.

しかし,繰り返しになりますが,PCが使える,プログラミングができる,ということは自分で問題の解決策を調べて解決できるということです.したがって,まずは自分で調べて解決する癖をつけるようにしましょう.

### エラーを体験してみよう

皆さんのプログラムが上手く動かない理由の圧倒的No1が**スペルミス,タイプミス**です. プログラミングの作業は,プログラムもコマンドも英語で記述します. プログラムは,1文字でも間違っていると上手く動かないので,しっかりとタイピングしましょう. 特にこれから行う作業で非常に多いスペルミスは以下のようなものです.

::: note
- 学生のスペルミス例
    - Data → Date  (なぜか3割くらいの学生が間違えます.)
    - industry → indusutry, industly, indstry
    - python → pyton, pyhon
    - answer →  anser, answere, ansewer
    - salary → sarary, saraly, sarasly
    - python --version → python version, python --vertion
:::

スペルミスに対応するには注意するしかありません. 単純に英語の単語を覚えていない or タイピングミスが原因なので注意しましょう.英単語の意味がや綴がわからない場合には検索しましょう.
基本的にプログラミングにおいて無意味な英単語は利用していないので,意味を考えましょう(意味がない単語の例としてhoge,hugaなどは良く使いますが).特に`Date (日付)`, `Data(データ)`などは頻出ですが, 単語の意味を考えればミスしづらいかと思います. また,エラー文を読めばどこが間違っているか教えてくれています.
エラー文にでてきた文字列’industry’や’Data’に該当する部分が間違っていないかチェックしましょう.

事例として,以下のプログラムの実行結果と,エラーについて見てみましょう. なお,プログラムの内容や詳細に関しては,このあとやるので理解できなくても問題ありません.

プログラムを実行するにあたって作業ディレクトリに以下のプログラム`error_sample.py`とプログラム内で読み込むデータ`data/error_sample.csv`が存在することを前提とします. ここでは,あくまで事例として紹介するので皆さんはデータとプログラムを用意する必要はありません.


::: warn

やる必要はありませんが,同じ作業を試してみたい場合は,作業ディレクトリで以下のコマンドをコピーして実行しましょう

- Windowsの人

~~~ sh
pip install pandas
echo "name,salary\ntaro,100" > data/error_sample.csv
echo "import pandas as pd\ndf = pd.read_csv('data/error_sample.csv') \nprint(df['salary'])" > error_sample.py
~~~

- Macの人

~~~ sh
pip3 install pandas
echo "name,salary\ntaro,100" > data/error_sample.csv
echo "import pandas as pd\ndf = pd.read_csv('data/error_sample.csv') \nprint(df['salary'])" > error_sample.py
~~~


ファイルの構成が以下のようになっていれば問題ありません.

~~~ sh
❯ ls
error_sample.py
❯ ls data
salary_data.csv
~~~

それぞれ,以下のようなファイルができているはずです(コメントは入っていません).

- error_sample.py

~~~ python
import pandas as pd
#dataフォルダにある,error_sample.csvファイルを読み込み
df = pd.read_csv('data/error_sample.csv')
#読み込んだファイルのsalary列を表示
print(df['salary'])
~~~

- error_sample.csv

~~~ python
name,salary
taro,100
~~~

:::

このプログラムを実行してみると,`error_sample.csv`の`salary`列の値が表示されます.

~~~ sh
❯ python error_sample.py
0    100
Name: salary, dtype: int64
~~~

プログラムを以下のように修正して実行してみます.

~~~ python
import pandas as pd
# error_sample.csvをarara_sample.csv に変更
df = pd.read_csv('data/arara_sample.csv')
print(df['salary'])
~~~

以下のようなエラーが表示されます.

~~~ sh
❯ python3 error_sample.py
Traceback (most recent call last):
  File "/Users/akagi/Documents/Programs/Python/slds/error_sample.py", line 2, in <module>
    df = pd.read_csv('data/arara_sample.csv')
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 912, in read_csv
    return _read(filepath_or_buffer, kwds)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 577, in _read
    parser = TextFileReader(filepath_or_buffer, **kwds)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1407, in __init__
    self._engine = self._make_engine(f, self.engine)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/parsers/readers.py", line 1661, in _make_engine
    self.handles = get_handle(
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/io/common.py", line 859, in get_handle
    handle = open(
FileNotFoundError: [Errno 2] No such file or directory: 'data/arara_sample.csv'
~~~

先ほど解説したように最初の部分と,最後の部分だけを見てみましょう.
最初の部分ではエラーの発生場所を説明しています. このエラーは
`line 2, in <module>`の`df = pd.read_csv('data/arara_sample.csv')`
部分で発生しています.先ほど変更を加えた2行目のファイル名の部分ですね.

最後の部分では,発生したエラーの中身について説明しています. エラーの詳細は

`FileNotFoundError: [Errno 2] No such file or directory: 'data/arara_sample.csv'`

であり,`data`フォルダに`arara_sample.csv`というファイルがないという意味です.

プログラムの`arara_sample.csv`の部分を修正して,今度は,最後の行をエラーが出るように変更しています.

~~~ python
import pandas as pd
#dataフォルダにある,error_sample.csvファイルを読み込み
df = pd.read_csv('data/error_sample.csv')
#salaryをsararyに変更
print(df['sarary'])
~~~

実行すると以下のようなエラーが発生します.

~~~ sh
❯ python3 error_sample.py
Traceback (most recent call last):
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 3652, in get_loc
    return self._engine.get_loc(casted_key)
  File "pandas/_libs/index.pyx", line 147, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/index.pyx", line 176, in pandas._libs.index.IndexEngine.get_loc
  File "pandas/_libs/hashtable_class_helper.pxi", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item
  File "pandas/_libs/hashtable_class_helper.pxi", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item
KeyError: 'sarary'

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/akagi/Documents/Programs/Python/slds/error_sample.py", line 3, in <module>
    print(df['sarary'])
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/frame.py", line 3761, in __getitem__
    indexer = self.columns.get_loc(key)
  File "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/indexes/base.py", line 3654, in get_loc
    raise KeyError(key) from err
KeyError: 'sarary'
~~~

今度のエラー文を見てみると,先ほど変更を加えた

`File "/Users/akagi/Documents/Programs/Python/slds/error_sample.py", line 3, in <module>`

の  `print(df['sarary'])`でエラーが発生しており,エラーの内容は,`sarary`という`Key`が存在しないという意味の`KeyError: 'sarary'`です. pandasのDataFrameにおけるKeyについてはまだ扱っていませんが,辞書型で発生するエラーと同様なので,辞書型を参考にして大まかな意味を掴みましょう.

~~~ python
>>> xs = {"name":"taro","salary":100}
>>> xs['salary']
100
>>> xs['sarary']
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
KeyError: 'sarary'
~~~
上のコードでは,`sarary`という`xs`に存在しない`key`を呼び出したことで`KeyError`が発生しています.

::: note

- 演習

以下のプログラムをコピーして保存・実行し,エラーを確認しましょう.
どのようなエラーが含まれているのか,エラー文を読んで修正し,説明してください.

- error_sample2.py

~~~ py
#Morningの"Good"部分を抽出したい.
#わざとエラーを含むプログラム
greetings = {"Morning":"Good Morning"
            "Noon":"Hello"
            ,"Night":"Good Night"}

  print(greeting["Morming"]["0":"4"]
~~~

:::

</details>

# ライブラリの利用

<details>
    <summary>開く/閉じる</summary>

通常のPythonの学習では,基礎的な演算のあとにFor文やIF分を学びます.
しかし,この講義では通常のリストなどよりもむしろPandasとよばれるライブラリの機能を多用します.なので, 先にライブラリの利用法と,Pandasの利用法を学び,それらを事例としてFor文やIF文,関数などの基本的な構文を学習します.



## ライブラリとは

第1回の講義で説明しましたが,プログラムをまとめて他の人がプログラム内で利用できるようにしたものをライブラリといいます. Pythonは,統計,データサイエンス関連のライブラリが充実しているために,利用者が多いということでしたね.

Pythonは様々なライブラリを呼び出して利用するための言語として利用されている側面が大きいです.なので, 目的に応じた様々なライブラリの使用法を知って初めてPythonの力を活用できます.

ライブラリのより細かい概念として, **モジュール(Module)**と**Package(Package)**があります.

::: note

- `Module`
    - 他のプログラムに読み込む(`import`)することができるスクリプトファイル

- `Package`
    - Moduleの名前空間をドットで構造化する手段

- `Library`
    - 用途に応じてPackageをまとめたもの
:::

`import`や名前空間の意味は,実際に使っていきながら見ていきましょう.

## ライブラリを使ってみよう

データサイエンスは,名前の通りデータを扱いデータから何かしらの示唆を得るための学問です.
データサイエンスにおける分析は概ね以下のように推移します.

![データ分析の流れ](/images/data-science-flow.png)

それぞれの段階の処理は,基本的には特定のライブラリを利用して行います. したがって,Pythonによるデータ分析の学習では,基本的にそれぞれの段階でのライブラリの利用法を学ぶことが中心になります.それぞれのライブラリの利用法はこれから順次扱いますが,ここでは,第1段階として,データの読み込みに関するライブラリ`pandas`を事例に一般的なライブラリの使用法を体験します.


何かしらのデータを分析するにはプログラムにデータを読み込む必要があります. Pythonにおいてデータの読み書き,整形,編集,生成などを支援する代表的なライブラリに `pandas`があります.


ライブラリを利用するためには,まずライブラリを皆さんのコンピュータにインストールする必要があります.
Pythonでライブラリをインストールするには`pip`を利用します. `pip`は,Pythonのインストール時に資料の指示に従ってチェックをつけていれば,既にインストールされているはずです.Shellに`pip --version`と入力してバージョンが表示されていれば,インストールされているので確認してみましょう(Macの人は`pip3 --version`).

~~~ sh
> pip --version
pip 24.0 from /Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pip (python 3.10)
~~~

`pip install`のあとに,インストールしたいライブラリ名を入力すると,ライブラリがインストールされます. 今回は `pandas`をインストールするので`pip install pandas`と入力しましょう. 文字が沢山表示されますが, 問題ありません. 画面が止まるまで待ちましょう.

pandasはデータを扱うためのライブラリなので利用するには,何かしらのデータが必要です. 本格的な利用はこのあとでしますが,取り敢えず利用確認用の簡単なデータを作成しましょう.

データはテーブルデータの形式を取ることが多く,pandasで読み込みや書き込みができるテーブルデータとして代表的な形式に`CSV(Comma  Separated Values)`があります.

:::warn

最近はテーブルデータの高速な処理が可能な[`Polars`](https://pola.rs)なども流行っていますが,本資料ではPandasに限定します.

また,`JSON`や`XML`などの他のデータ形式に関しては扱いません.
:::

CSVは,テキストファイルの一種で,その名の通りコンマ(`,`)で列を区切り,改行で行を区切ることでテーブルを表現します. 標準的なCSVでは最初の1行は,データではなく,各列の中身を表すタイトルとなっており`Header`と呼びます.2行目以下のデータ部分を`Body`と呼びます.

Excelを開いて1行目に`Weight`,`Height`,`BMI`と記入し,それぞれの列に以下のように数値を記入しましょう.`BMI`列は空欄で構いません.

![csv sample](/images/pandas-csv-sample.png)

このファイルを`UTF-8`の`CSV`として保存します. ファイル名は`body_data.csv`としましょう. Excelでは`ファイルの種類`で`CSV UTF-8 (コンマ区切り)(*.csv)`と選択することで保存できます.

![excel file type](/images/excel-file-type.png)


このファイルを作業ディレクトリの中に作った`data`という名前のフォルダに保存します.

試しに,`body_data.csv`をテキストエディタで開くと,中身がコンマと改行で表現されていることが分かります.

~~~ python
Height,Weight,BMI
170,55,
180,72,
155,42,
~~~

Sublime Textでは `File`タブの`Save with Encoding`内の`UTF-8 with BOM`で保存し,拡張子を`.csv`にすることで保存できます.

![](/images/Sublime-utf-8-bom.png)

::: warn
**`BOM(Byte Order Mark)`について**

Unicode(UTF-8, UTF-16など)で文章が記載されていることを示す符号をBOMといい,ファイルの最初に記載されます.
これがついていることで,ソフトウェアでデータを読み込む際に自動的に,適切なエンコーディングが選択されます.
Excelの場合は`CSV UTF-8 (コンマ区切り)(*.csv)`でBOM付きのUTF-8として保存されますが,日本語環境ではBOMがついていないCSVファイルを`Shift-JIS`で開くため,外部ファイルの読み込みから適切なエンコーディングを選ばないと文字化けします.

この知識は後ほどpandasを利用したCSVの保存で再度扱います.
:::

このファイルを読み込んで,pandasの機能をいくつか確認するためのスクリプトを書きます.
テキストエディタで`pandas_test.py`という空のファイルを作成し,以下のように記述しましょう.

~~~ python
import pandas as pd

df = pd.read_csv('data/body_data.csv')
print(df)

print(df['Weight'].mean())
print(df['Weight'].median())

df['BMI'] = df['Weight'] / (df['Height']/100) ** 2

print(df)
~~~

このプログラムを実行すると,以下のように表示されるはずです.

~~~ sh
❯ python3 pandas_test.py
   Height  Weight  BMI
0     170      55  NaN
1     180      72  NaN
2     155      42  NaN
56.333333333333336
170.0
   Height  Weight        BMI
0     170      55  19.031142
1     180      72  22.222222
2     155      42  17.481790
~~~

このプログラムで何をしているのかを簡単に見ていきましょう.

まず一行目の`import pandas as pd`ですが,`import pandas`によってこのプログラム内で`pandas`を利用可能にしています. しかし,`pandas`と毎回かくのは面倒なので,省略形の`pd`という名前を定義しています. Pythonでは`import [利用したいライブラリ名] as [省略形] `
の形で,ライブラリのインポートと省略形の定義を同時に行えます. pandasは`df`と省略するのが一般的です.

続いて2行目の `df = pd.read_csv('data/body_data.csv')`で,先ほど作成した `data`フォルダにある`body_data.csv`というファイルを読み込んでいます. `df = `で読み込んだデータに`df`という名前をつけています.

3行目の`print(df)`で,読み込んでdfという名前をつかたデータを表示しています. REPLでは,変数を書くだけで表示されていましたが,スクリプトでは標準出力したいものは`print()`関数を適用する必要があります.

4,5行目は`Weight`列の平均(`mean`)と中央値(`median`)を求めて表示しています.

6,7行目はもともとのデータで空だった,`BMI`列を計算し,計算結果を表示しています.

これらの操作の具体的な使い方や意味はこのあと細かく扱いますが,取り敢えず,pandasではこのようにして,データを読み込み,処理することができます.

## なぜExcelではだめなのか

先ほど行った簡単な表計算であれば,Excelでも簡単に同じことができます. では,なぜややこしいPythonなどを学んでプログラムで同じことをするのでしょうか.

まず,第1にPythonを利用することで

**機械学習や高度な検定など,Excelではできない高度な処理が可能**になります.

先ほど行った簡単な集計であればExcelでも可能ですが,後ほど行う機械学習や自然言語処理などの高度な処理はExcelでは実行が困難です. また, データに関してもExcelでは最大で1,048,576行程度のデータしか扱うことができませんが,(この講義では扱いませんが)データサイエンスでは,それを超えた大容量のデータを扱うことがしばしばあります.

第2の理由として,

**Excelが複数の連続した複雑な処理には向いていない**ということが挙げられます.

Excelでは,何か作業をしても**過程が記録されず,結果だけが残ります.** また,折角複雑な処理を実行しても,異なるデータなどで同じ処理を行う場合にはゼロから繰り返す必要があり,**処理自体を再利用できません**(VBAやマクロなどはありますが,それらは結局プログラミングをしているので同じです.)

Excelのこのような特性から,Excelによる複雑な処理の実行は,人間の作業に依存する傾向が強く,ミスの原因となります.

このことについて,以下のデータを事例にもう少し具体的に意味を見ていきましょう.

![](/images/excel-procedure1.png)

例としてこのデータに対して以下の3つの処理をすることを考えます.

::: note
    1. いらない部分を削除する
    2. 右側に行和を表す列を足す
    3. とある列の値が一定以下のデータを抜き出す
:::
![](/images/excel-procedure2.png)

この場合,EXCELでは最後の抜き出したデータだけが残り,中間の段階の処理も,過程も記録されません.
したがって,

:::note

1. 途中で間違っていても気づけない,チェックできない
2. 何をしたのかを他の場所にメモして置かなければ分からない,メモも難しい
3. 別のデータを使う場合同じ処理をもう一度する必要がある

:::

などの問題がよく起こります.

また,削除の作業などは(これくらいなら自動でもできますが)一つ一つ消していく必要があり大きなデータ(数万行とか)では非常に手間がかかります.

Excelはこのように,処理が人間の記憶や,正確さに依存しています. その結果,ミスの原因となっています.

::: note

例えば, こちらの記事([The 7 Biggest Excel Mistakes of All Time](https://www.teampay.co/blog/biggest-excel-mistakes-of-all-time))では,過去にExcelの操作ミスで起きた大規模なミスの事例が紹介されています.

- 2008年 バークレイズ・キャピタル

隠していたセルがPDF板では表示されていたことによる数百万ドルの損失が発生.

- 2012年 ロンドン オリンピック

作業員のタイプミスで,10万枚のチケットが払い戻し

- 2012 JPモルガン

Excelのコピペミスにより60億ドルの損失

ちなみにこの記事には載っていませんが,2019年に話題になった,厚生労働省の毎月勤労統計における不正問題もExcelのミスが原因の一つです.
:::

一方プログラムでは,これらの問題が解決可能です.

先程のExcelの作業と同じ例で考えてみましょう.
以下のコードは先程のExcelで行った作業を実行しています.

~~~ python
import pandas as pd

#データの読み込み
df = pd.read_csv('sample.csv')

#いらない部分を削除する
df.dropna(axis='index',how='any',inplace=True)

#右側に行和を追加する
df['sum'] = df.sum(axis=1)

#data3が50以下の行を消す
df = df[df['data3']>50]

#確認
print(df)
~~~

実行結果は当然Excelで行ったものと同じです.

~~~ sh
> python sample.py
    data1   data2   data3    sum
3    86.0      75    67.0  228.0
4    66.0      92    90.0  248.0
7     6.0      66    93.0  165.0
10   27.0      32    52.0  111.0
18   27.0      61    91.0  179.0
~~~

しかし, Excelと異なりこのプログラムを実行しても**元データは変わっていない**ため,元の状態にいつでも戻ることが可能です. 更に何を行ったかの**処理がプログラムとして残っている**ため,途中で間違いがないかのチェックや修正が可能です.
(本講義では扱いませんが, 実際にはプログラムでもミスは絶対におきます.そのためテストを実施するのですが,人間の行うチェックリストなどのテストよりもプログラムで行うテストの方が信頼性が高いのも利点です.)

また,
**プログラムがそのまま作業メモ**になっているので,他人に何を行ったかの説明が可能です. 加えて,このプログラムを再利用することで,**異なるデータで同じ作業をする場合に再利用することが可能**です. また,実際の処理はすべてプログラムが自動で行うため**作業の手間がデータの容量にかかわらず一定**です.

確かに,これまでに皆さんが講義で習ってきたような内容であれば,Excelのほうが学習コストが低く手軽に行えます. しかし,今後行うような統計処理やデータサイエンスなど,処理が複雑になればなるほど,プログラムで処理するメリットが大きくなります.

</details>

# データの取得と編集

<details >
    <summary> 開く/閉じる </summary>

これからデータをプログラム上で扱うにあたって,まずはデータについて考えてみましょう.

データは,基本的に誰かが収集・作成したものです. 自然科学においては,**実験**によってデータを収集し,人文・社会科学では,**調査**によってデータが集められます. データの作成はそれ自体でそれぞれ,**実験計画法**,**社会調査法**といった学問分野になっています.

データサイエンスは,データを利用して行う学問でセウが,データを収集・作成するのは専門知識,技能,時間,お金などが必要であり,データの作成自体が大きな仕事です.

データを自分で集めるには大変な手間がかかるので, 研究機関や,行政,大学,調査会社など他者が集めて公開しているデータを利用するのも一つの手段です. この講義では, 基本的に公開されているデータを利用して, 学習,研究を行います.

実験や調査によって集められたデータを**原データ**といい,調査によって集められた個別の調査票などを**個票**といいます.

原データに統計処理を加えてまとめたものを,**統計資料**あるいは**調査結果**などと呼びます.もともと**統計(statistics)**という言葉は,(国家の状況に関して)
集められた原データを編集してわかりやすく加工することを意味していました. データサイエンスではしばしば原データが利用されますが,公開されているデータはこれらの統計資料である場合が多く,原データを利用するのは困難な場合が多いです.

統計資料に更に加工を加えたものを,**二次統計**や**加工統計**と呼びます.

例えば, 国税調査によって個票が収集され,統計処理を加えた統計資料が公開され, それを更に複数の統計と併せて加工することで,国民経済計算などの加工統計が作成されます.

![データの作られ方](/images/statistical-data-flow.png)

## データの尺度

データには様々な種類があり,種類ごとに利用可能な統計やデータサイエンスの手法が異なります.したがって,今後データを分析するにあたって,大まかなデータの区分を知っておく必要があります.

データの区分として,一番大きいものに **量的データ(quantitative data)**,**質的データ(qualitative data)**があります. 質的データはカテゴリーデータなどとも呼ばれます.

::: warn
このどちらにも当てはまらないような,画像,音声,文字データなどの**非構造データ**もありますが,それらはそれぞれ個別の対処手法があります.

この講義では後半で文字データを扱う自然言語処理について扱いますが,しばらくはこれらに関しては無視して,量的データと質的データを扱います.
:::

::: note

- 量的データ

数量的なデータであり,データを数直線上の位置で表現できます.

例:

    - 年齢(5歳,10歳,100歳)
    - 身長(150cm,170cm,200cm)
    - 年収(200万円,500万円,1億円)

    など基本的には,何かしらの**単位**で計量されます.

---

- 質的データ


あらかじめ定められたカテゴリーのいずれかに属する値を持つデータです.

例:

    - 性別(F or M)
    - 犬の種類(ゴールデンレトリバー, 柴犬, プードル,など)
    - 天気(晴れ,雨,曇など)
    - 満足度(とても満足, まぁ満足, どちらでもない,など)

:::

質的データと量的データは更に細かい区分として**尺度(scale)**に分解されます.
それぞれは,｢比較が可能か｣,｢和差積商｣の計算が可能かどうかという観点で定義されます.

::: note

- 質的データ

---

**名義尺度(nominal scale)**

質的データの中で区分にしか意味がなく順序が定義されていないもの.

    - 例:氏名,性別,種類,など

和差積商のどれも定義されない.

    - 例:青木 + 赤木 = 紫木 のような計算はできない.
    - 青木は赤木よりも良い,大きい,などの比較もできない

---

**順序尺度(ordinal scale)**

質的データの中で順序が定義されているもの.

    - 例: 満足度,成績区分(S,A,B,...)など

和差積商のどれも定義されないが比較が可能

    - 例: ｢満足｣は｢不満足｣よりも満足度が高いが,
    - 普通 + 普通 = 満足 のような計算はできない.

---

- 量的データ

---

**間隔尺度(interval scale)**

量的データの中で,間隔は定義されているが,比率が定義されていないもの.

    - 例: 時刻,日付,IQ,気温,西暦,etc

足し算引き算はできるが,掛け算割り算ができない.

    - 1月1日 + 364日 = 12月31日 ですが,
    - 1月1日 * 2 = 2月2日 のような計算は定義されていません.

---

**比率尺度(ratio scale)**

量的データの中で,比率が定義されているもの.

    - 例: 長さ,速度,重さ,面積など

和差積商すべて計算できます.

    - 10Kg + 10Kg = 20Kg
    - 10Kg * 2    = 20Kg

:::

これらの区分は今後データの処理手法を選択する際に使用するので,
データがでてきたら尺度は何かを判断するようにしましょう.

::: note
**演習問題**

以下のデータの尺度を判断し,なぜそのように判断できるかを説明してください.

1. ペットの種類 (イヌ,ネコ,鳥,魚,爬虫類,その他から選択)

2. 偏差値

3. 電話番号

4. 一週間に読んだ本のページ数

:::

## データの種類

これまでは一つの種類の値の区分に関して,説明してきましたが,データは複数の値の組み合わせで表現され,**観測対象(ケース)**別の**観測項目(変数)**の**テーブルデータ**として表現される場合が多いです.

![観測対象と観測項目](/images/case-and-variable.png)

データの観測項目の数のことを**次元**と呼び, 1観測項目のデータを**1次元データ**,2観測項目以上のデータを**多次元データ**と呼びます.

![データの次元](/images/data-dimention.png)

また,観測項目や観測対象の組み合わせによって,特別に呼称され,それぞれ独自の分析手法が存在するデータもあります(時系列データと時系列解析,パネルデータと,パネルデータ分析など)

![](/images/data-types.png)

上の時系列データは,特定の時間単位で集計,区分されたデータを指しますが,データの発生時刻が記録されたデータを**点過程データ**と呼びます. 点過程データを日毎,月ごとなどで集計することで時系列データが作成されます.


## データの取得と編集

先に述べたように,データは自分で集めることもできますが,手間,時間,お金がかかるため,目的に応じて他人の集めたデータを利用することも一般的です.

現在では,無料で利用可能なデータも多数あり,この講義ではそれらのデータを利用します.

::: warn

後ほど自分で選んだ研究テーマによっては自分でデータを収集することも可能です.
過去のこの講義でも, HPの視線移動データ,アンケートデータ,ヒットチャートの音楽の特徴量を表すデータ
などを自分で作った事例もあります.

それらに関しては,個々の研究計画に応じて必要であれば補足的に講義内で扱います.
:::

無料で誰でも利用可能なデータの代表として,**公的統計**があります. 公的統計は,国や地方自治体の作成する統計情報であり,政策立案と評価のための基礎資料として利用されており, 作成過程なども含めて一般に公開されているものが多数あります. プライバシーや個人情報保護の観点から個票は公開されていない場合が大半ですが,研究目的であれば申請して利用することも可能です.しかし,通常データが手元に来るまでに1年程度時間を要するのでこの講義では扱いません.

日本の統計情報を活用するには,"日本の統計が閲覧できる政府統計ポータルサイト"[`e-stat`](https://www.e-stat.go.jp)があります. e-statでは日本の各種公的統計の情報を統計別,分野別,地域別,時系列などで取得することができます.

![e-stat screenshot1](/images/e-stat-screenshot1.png)
![e-stat screenshot2](/images/e-stat-screenshot2.png)

日本以外のデータに関しては,各国の統計サイトに直接アクセスするか,あるいは国連による各種子交際期間の統計の横断検索サイト[`UNData`](https://data.un.org)などを利用することで取得可能です.

![UNData](/images/UNData-screenshot.png)

各国の公的統計は通常数量的なデータを扱っていますが,大学の講義や研究などで欲利用される質的データとして,[`World Value Survey`](https://www.worldvaluessurvey.org/wvs.jsp)があります. 世界各国の価値観に関する調査のパネルデータを提供しており, ｢人生において大切にするもの｣｢男女平等意識｣ などの調査結果が利用できます.

その他にも各種研究機関や企業が提供するデータも利用可能です.

統計ではありませんが, [`EDINET`](https://disclosure2.edinet-fsa.go.jp/WEEK0010.aspx)のWEB APIを利用して有価証券報告書のデータを取得する,Wikipediaのテキストデータを自然言語処理してデータを作成するなどもこの講義における過去の研究事例があります.

これらに関しては後ほど, Webページから自動でデータを取得する`Webスクレイピング`やTwitterなどのSNSの提供する`API`を利用してデータを取得する方法も扱います.

Webから取得可能な情報以外にも, 特定の組織に直接交渉してデータを活用することも可能です. この講義では千葉商科大学の授業データや,電力消費データを提供してもらい分析した事例もあります.

## e-statを使ってみよう

e-statを利用して,pandasで読み込める形式のCSVを作成しましょう. 練習用なので,何を持ってきてもいいのですがここでは,｢産業別の給与｣に関するデータを取得します. 就職活動中の学生も多いかと思います. 産業別の給与はどのように異なるのでしょうか.

まずは使いたいデータを探します. 方法はいくつもありますが,ここでは`分野`から探してみましょう. `分野→労働・賃金→民間給与実態調査`の順にクリックしてください.

![](/images/e-stat-salary1.png)


`データベース`形式か,`ファイル`形式か選択することができます. データベース形式は必要なデータをブラウザ上で選択,編集してデータを取得することができます. ファイル形式は,ExcelやPDFファイルをダウンロードできます. どちらでも問題ありませんが,ここではただのExcelファイルと,pandasで読み込めるデータの違いを確認するために,`ファイル`をクリックしましょう.

![](/images/e-stat-salary2.png)

官庁の作成する統計は,`民間給与実態調査`などの調査名,統計名の下位分類として,年次推計,延長推計などの提供分類,その下に対象年次,更にその下に表レベルの分類があります. これはそれぞれの統計にある程度詳しくないと意味がわからないかと思いますが,今は練習ですので気にせず,`結果表`の`年次推計`の`2018年度`の`第1表` の`Excel`をクリックしましょう.

![](/images/e-stat-salary3.png)

これで,民間給与実態統計調査結果表年次推計2018年度第1表 給与所得者数・給与額・税額 全国計表 がダウンロードできましたので,開いてみましょう.

![](/images/e-stat-salary4.png)

省庁から取得したExcel表は,このような形態になっていることが多いですが,このデータはそのままではpandasなどで読み込める形になっていません. もちろん,データベース機能で編集したデータや2018年ではなくもっと新しいデータの場合には,ある程度利用できる形態になっていますが,このようなデータは未だに多数存在します.

ではこのデータは一体,何が駄目なのでしょうか. プログラムで利用できるデータを作成するにあたって,e-statのデータは良い**悪い見本**なので,e-stat全般の問題点に関して少し見ていきましょう.

先ほどダウンロードしたような形式のデータを俗に**ネ申Excel**といいます(以降**神Excel**と記載).

神Excelとは,紙で印刷した際の見栄えを重視した,データとしての利用性のまったくないExcelシートです(参考:[｢ネ申Excel｣問題](https://okumuralab.org/~okumura/SSS2013.pdf)).

例えば, 見栄えを重視するために**セルを結合する**,**画像を挿入する**,**スペースなどで幅を揃える**,**1セルに1文字を入力する(Excel方眼紙)**などが神Excelの特徴となります.

先程の民間給与実態統計調査を見てみるとこれらの特徴が見事に当てはまります.

![神Excel](/images/kami-excel.png)

人間は,文字の間に空白が挟まっていてもつながった単語として認識できますが,コンピュータにはできません.また,セルが結合されている場合に表として解釈することもできません,画像も処理できません.このようなExcel表はデータとしては利用できないので,データとして形式を揃えるだけで大変な手間がかかります(このあと実際にやります).

逆に, プログラムで利用可能なデータを**機械判読可能**なデータといいます.
機械判読可能な分析しやすいExcelデータをまとめる基本チェック表として,以下のものが有名です([Data Organization in Spredsheets](https://www.tandfonline.com/doi/full/10.1080/00031305.2017.1375989)).

::: note

- 一貫性を保つ

    - 名前の一貫性
    - 記述方法の一貫性

- 名前の付け方
    - スペースを使わない
    - 意味が分かる名前をつける
    - 日付の書き方をYYYY-MM-DDに

- 空のセルを作らない

- セルに一つしかものを入れない

- 長方形にする

- データと計算を分離する

- フォントの色やハイライトをデータとして使用しない

- データ検証機能を使う

- CSVファイルに保存する
:::


データをプログラムで処理するのが一般的な現代において,自分で使うためのみならず,データの価値を保つためにも機械判読可能な形式でデータを作成するように心がけましょう.Excelを使っていると,せっかくある機能を使いたくなりますができるだけデータとしての利用可能性を高める意識が必要です.

::: warn

ちなみにe-statの問題点は,神Excelにとどまりません.

一番恐ろしいのは一部のデータにおいて,**データベース機能に登録されている元データが神Excel**のままになっていることです.

Excel表上でスペースが混じっているセルの値などがデータベース機能で編集してもそのまま出てくる場合があります.

その他にも,

- 同じ統計でも作成年毎に表記が異なる
- データの構造が統計名-提供分類-年次-表の形式になっているため,APIなどでデータを一意に絞り込めない.
- 公表が遅い
- APIにおける一部文字化けやJSONの構造が崩れている

など沢山の問題があります. 徐々に改善されているようですので,今後に期待したいところですね.

:::

## 機械判読可能データの作成

では,e-statからダウンロードしたデータを編集して, **機械判読可能なBOM付きUTF-8のCSV**に変換してみましょう. ここでは, Execelを用いて行います.

今回ダウンロードしたデータの中で扱いたいのは,**業種別の給料**です.
それ以外の邪魔な情報は削除しましょう(コピーして別のシートに貼り付けた方が早いです).
新しいファイルを作成し,一行目にHeaderとして,`industry`, `salary` と記入しましょう.
日本語でも大丈夫ですが英語の方が安心です.
しかし,業種名を他のファイルにそのまま貼り付けようとすると,セルが結合されているためうまくいきません.このままでは,プログラムで上手く処理できないので編集します.

ダウンロードしたファイルの業種の部分をコピーしたら,新しいファイルのindustry の下のセルを右クリックして｢値だけ貼り付け｣をしましょう.
一部の行の中に句読点が入っていますがこれもエラーのもととなるので,編集します.

![ヘッダーの作成](/images/e-stat-salary5.png)

![データのコピー](/images/e-stat-salary6.png)


プログラムは基本的に英語の利用を想定されているのでデータも英語の方が安定します.
日本語でも問題ない場合が大半なので,日本語のままでも良いですが,プログラム記述中に日本語と英語が混ざるとエラーの原因となりやすいので,文章自体に意味があるテキストデータなどを除いて可能な限り英語に変換しておくことをおすすめします.
自分で適切な英語に変換できない場合はExcelでは翻訳したいセルを右クリックして`翻訳`を選ぶと翻訳してくれます. Excelでは現状自分でコピーアンドペーストしなければなりません.
Google Sheetなど使うと一発で全て変換する関数 GoogleTranslate などがあります.

![翻訳されたデータ](/images/e-stat-salary7.png)

![Excelの翻訳機能](/images/e-stat-salary8.png)

CSVでデータを詠み込む場合, 空白,コンマ,Tabなどはエラーになりやすいので,可能な限り消しましょう.
特に今回の業種データは｢電気･ガス･熱供給・水道業｣ などなぜか中点を利用しているものもあれば,句点で区切られているものもあり,意味が分かりません.
長過ぎるのも問題が起きやすいので,頭文字のみにするなどわかりやすい形に変更しましょう(電気･ガス･熱供給・水道業 → インフラ, 卸売,小売業 → 商業 など)

再度に作成されたデータをCSVとして保存して終了です.保存する際のデータ型式にCSV  UTF-8があるのでそれを選択し, ｢`salary.csv`｣という名前で作業ディレクトリのDataフォルダに保存します.

![データの保存](/images/e-stat-salary9.png)


## pandasによるデータ処理

ここから,先ほど作成したデータを利用してpandasの基本的な機能について確認していきます.
**いくつもの処理を一気に扱いますが,すべて暗記する必要はありません. ただし,それぞれの処理の存在を覚えるようにしましょう. 可能な処理やその名前を覚えておけば,この資料に戻る,あるいは検索して調べることで自分で利用できます.**
取り敢えず,講義内ではそれぞれの処理が何をやっているのかの概要を掴み,体験してみましょう.

### ファイルの読み込み


先程作成したデータを作業ディレクトリのdataフォルダに保存して,Pythonで読み込んでみます.
以下,`salary.py`というファイルを作成してプログラムを追記していきます.

::: warn
スクリプトのファイル名は,中身がなにか後で見ても分かるような英数字であれば何でも構いません.
ただし, `pandas`,`numpy`,`matplotlib`などの今後利用するライブラリと同名のスクリプトを作成すると,
正常にライブラリを利用できなくなるので注意しましょう.
:::

以下の用に `import pandas as pd` でpandasをimportしたあとに, `pd.read_csv('ファイルパス')`でファイルが読み込めます. また,同じ行で読み込んだファイルを`df`という変数に代入しています.
最後の行で読み込んだファイルを表示しています.

最初のうちは自分で作成するプログラムにも以下の例の用に,それぞれの行で何をしているのかコメントを書くようにしましょう.

~~~ py
# pandas を import し, pdと呼ぶという意味
# pandas はデータの読み込みなどに利用します
import pandas as pd

#------------------------------------------------------------------
# データの読み込み
#------------------------------------------------------------------
# CSVファイルを読み込んでデータフレーム(後述)に格納
# pd.関数名 でpandasの関数が利用できる
# Dataフォルダを作成し,そこにデータを入れておきましょう
# data/salary_data.csv は  data フォルダの salary.csv という意味
df = pd.read_csv('data/salary.csv')
print(df)
~~~

正しくプログラムをデータが作成できていれば,プログラムを実行することで以下のようにデータの中身が標準出力に表示されるはずです. プログラムを記述する際には,このようにところどころで,`print()`を利用して,自分のイメージしたとおりにデータが編集されているか確認する癖をつけるようにしましょう.

~~~ sh
❯ python3 salary.py
         industry  salary
0    Construction    4503
1   Manufacturing    4756
2       Wholesale    3186
3    Accomodation    1949
4         Finance    5711
5     Real Estate    3786
6       Transport    3909
7          Energy    8199
8            Info    5337
9             Edu    3913
10            Med    4144
11          Other    4239
12       Suervice    3086
13           AFFC    2190
~~~

::: note

pandasでは主に`DataFrame`というデータ型を利用してデータを扱います.
`DataFrame`オブジェクトには様々な機能が実装されており,様々な形に変形することができますが,基本的には以下の3属性から成っています.

- values : データ部分 n × m の行列

- columns: 列名, header情報

- index: 行名

![DataFrameのイメージ](/images/data-frame-image.png)
:::

クラスの属性には`オブジェクト名.属性名`でアクセスできます. 以下のコードを追加して実行し,それぞれどのように表示されるか確認してみましょう.

~~~ py
# dfのvalues属性を表示したい場合
print(df.values)

#dfのcolumns属性を表示したい場合
print(df.columns)

#dfのindex属性を表示したい場合
print(df.index)
~~~

~~~ sh
[['Construction' 4503]
 ['Manufacturing' 4756]
 ['Wholesale' 3186]
 ['Accomodation' 1949]
 ['Finance' 5711]
 ['Real Estate' 3786]
 ['Transport' 3909]
 ['Energy' 8199]
 ['Info' 5337]
 ['Edu' 3913]
 ['Med' 4144]
 ['Other' 4239]
 ['Suervice' 3086]
 ['AFFC' 2190]]
Index(['industry', 'salary'], dtype='object')
RangeIndex(start=0, stop=14, step=1)
~~~

## データの確認

DataFrameを読み込んだらまずは,その全体像と特徴を確認しましょう.
確認すべき項目として,columnsやindexの他に以下のようなものがあります.

::: note

- データの中身

データが長い場合は標準出力には省略して表示されます.
意図的に,先頭n行を取得したい場合には`df.head(n)`を利用します.
後ろからn行を取得したい場合には`df.tail(n)`を利用します.

~~~ py
print('head --- \n',df.head(5))
print('tail --- \n',df.tail(5))
~~~

~~~ sh
head ---
         industry  salary
0   Construction    4503
1  Manufacturing    4756
2      Wholesale    3186
3   Accomodation    1949
4        Finance    5711
tail ---
     industry  salary
9        Edu    3913
10       Med    4144
11     Other    4239
12  Suervice    3086
13      AFFC    2190
~~~
:::

::: note

- データの形

    - `df.shape` でデータの行数,列数を取得できます.

    - `len(df)` でデータの長さ(行数)を確認できます.

~~~ py
#形の確認
print(df.shape)
print(len(df))
~~~

~~~ sh
(14, 2)
14
~~~
:::

::: note

- 記述統計量

`df.describe()`を利用することで `データ数(count)`,`平均(mean)`, `母標準偏差(std)`,`最大値(max)`,`最小値(min)`, `四分位数(25%,50%,75%)`などの特徴量が把握できます.
`df[列名].max()`で最大値,`df[列名].mean()`で列の平均,`df[列名].median()`で中央値など個別に把握することも可能です.

~~~ py
#特徴量の把握
print(df.describe())
print('最大値:',df['Salaray'].max())
print('平均値:',df['Salaray'].mean())
print('中央値:',df['Salary'].median())
~~~

~~~ sh
            salary
count    14.000000
mean   4207.714286
std    1561.247008
min    1949.000000
25%    3336.000000
50%    4028.500000
75%    4692.750000
max    8199.000000
最大値:8199.000000
平均値:4207.714286
中央値:4028.500000
~~~


:::

## 行名,列名の変更

columnsとindexは任意の値を設定できます(ただし重複はなし).
リストでそれぞれの属性を更新することで新しい列名,行名を指定できます.

- 列名の変更

`df.columns = [新しい列名のリスト]`

- 行名の変更

`df.index = [新しい行名のリスト]`

試しに列名を`['Ind','Sal']`に,行名を`a~n`のアルファベットに変更してみましょう.

~~~ py

#列名の変更
df.columns = ['Ind','Sal']

#行名の変更
df.index = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n']

print(df)
~~~

指定した通りに列名や行名が変更されていれば成功です.

~~~ sh
             Ind   Sal
a   Construction  4503
b  Manufacturing  4756
c      Wholesale  3186
d   Accomodation  1949
e        Finance  5711
f    Real Estate  3786
g      Transport  3909
h         Energy  8199
i           Info  5337
j            Edu  3913
k            Med  4144
l          Other  4239
m       Suervice  3086
n           AFFC  2190
~~~

ただし,indexは0から始まる連番を利用するのが一般的なので,むやみに文字などにしないほうが良いです. indexに0から始まる連番を振り直すには,`.reset_index()`メソッドを利用するのが一般的です.

~~~ py
#元の連番に戻す
## ()の中に inplace = True と書くことでdfのindexが変更される
## inplace=Trueを書かないと変更されません.
## drop = True と書くことで,変更前に使用していたindexが消去されます.
## drop = False の場合先ほどのアルファベットが列として残ります.
df.reset_index(inplace=True,drop=True)
print(df)
~~~

実行結果

~~~ sh
             Ind   Sal
0    Construction  4503
1   Manufacturing  4756
2       Wholesale  3186
3    Accomodation  1949
4         Finance  5711
5     Real Estate  3786
6       Transport  3909
7          Energy  8199
8            Info  5337
9             Edu  3913
10            Med  4144
11          Other  4239
12       Suervice  3086
~~~

## 値の抽出

valuesにアクセスする方法は多数存在し,用途によって使い分けます.

列の指定は,`df['列名']`あるいは`df.列名`のように記述することで指定するのが最も単純な方法です.
`df`の`'Ind'`列を抽出したい場合には`df['Ind']`あるいは`df.Ind`と書きます.

~~~ py
#列の指定(Ind列)
print(df['Ind'])
#列の指定(Sal列)
print(df.Sal)
~~~

実行結果

~~~ sh
0      Construction
1     Manufacturing
2         Wholesale
3      Accomodation
4           Finance
5       Real Estate
6         Transport
7            Energy
8              Info
9               Edu
10              Med
11            Other
12         Suervice
13             AFFC
Name: Ind, dtype: object
0     4503
1     4756
2     3186
3     1949
4     5711
5     3786
6     3909
7     8199
8     5337
9     3913
10    4144
11    4239
12    3086
13    2190
Name: Sal, dtype: int64
~~~

`X列の,Y行`のように`範囲選択`したい場合には`df.loc[行番号,列名]`と書きます.
また,行名,列名それぞれにスライスを利用することでの,`X~Y列のV~W行`のような指定ができます.
スライスで表せない行名や列名を指定する場合には,それぞれに抜き出したい列や行のリストを渡します.

~~~ py
#範囲指定(3行目,'Sal'列)
print(df.loc[3,'Sal'])

#スライスを利用した範囲指定
print(df.loc[3:5,'Ind':'Sal'])

#リストを利用した範囲指定
print(df.loc[[2,4,7],['Ind','Sal']])
~~~

実行結果

~~~ sh
1949
            Ind   Sal
3  Accomodation  1949
4       Finance  5711
5   Real Estate  3786
         Ind   Sal
2  Wholesale  3186
4    Finance  5711
7     Energy  8199
~~~


特定の一つのセルにアクセスする場合には`loc`を利用することもできますが,`df.at[行名,列名]`を使うほうが高速です.

~~~ py
#atによる単独のセルの指定
print(df.at[3,'Sal'])
~~~

実行結果

~~~ sh
1949
~~~

行名や列名ではなく,行番号,列番号による指定も可能です. 行番号,列番号は行名や列名とは無関係に,0から始まる連番として指定されています.
範囲選択は`df.iloc[行番号,列番号]`,単独の値は`df.iat[行番号,列番号]`で行うことができます.

~~~ py
#行番号,列番号による範囲指定
print(df.iloc[3:5,0:])

#行番号,列番号による単独のセルの指定
print(df.iat[3,1])

~~~

実行結果

~~~ sh
         Ind   Sal
2  Wholesale  3186
4    Finance  5711
7     Energy  8199
1949
~~~

::: note

- 演習問題

以下の画像における範囲①~③を一度に抽出してください.
単独の値を抜き出す際には`.at`,`.iat`メソッドを利用しましょう.

![値の抽出](/images/pandas-select-practice.png)

:::

## 条件による抽出

Excelにおけるフィルター機能のように,pandasでも条件による値の抽出が可能です.
pandsでは,DataFrameオブジェクトに,真偽値のリストを渡すと,`True`の行のみを抜き出すことができます.

以下のコードでは,`False`と`True`を交互に繰り返し,奇数行のみを抜き出しています.

~~~ py
#真偽値のリストによる抜き出し
bools = [False,True] * 7
print(bools)
print(df[bools])
~~~

実行結果

~~~ sh
[False, True, False, True, False, True, False, True, False, True, False, True, False, True]
              Ind   Sal
1   Manufacturing  4756
3    Accomodation  1949
5     Real Estate  3786
7          Energy  8199
9             Edu  3913
11          Other  4239
13           AFFC  2190
~~~

DataFrameオブジェクトに対して,真偽値計算をすると,各行に対する真偽値計算の結果が帰ってきます.
以下のコードでは,Salaryが3000以上の行のみ`True`の値が帰ってきているのが確認できます.

~~~ py
#DataFrameオブジェクトに対する論理演算
print(df['Sal'])
print(df['Sal'] >= 3000)
~~~

実行結果

~~~ sh
0     4503
1     4756
2     3186
3     1949
4     5711
5     3786
6     3909
7     8199
8     5337
9     3913
10    4144
11    4239
12    3086
13    2190
Name: Sal, dtype: int64
0      True
1      True
2      True
3     False
4      True
5      True
6      True
7      True
8      True
9      True
10     True
11     True
12     True
13    False
Name: Sal, dtype: bool
~~~

この**①BoolのリストをDataFrameに渡すとTrueの行が返ってくる**,**②DataFrameに論理演算を行うとBoolのリストが返ってくる**という機能の2つを組み合わせて,条件抽出を行うことができます.
例えば,`Sal列`から3000以上の行のみを抜き出すには以下のように書きます.

1行のプログラムですが,いくつかの機能を組み合わせているので,どの部分で何が起きているのかを分解して理解するようにしましょう.

~~~ py
#DataFrameオブジェクトに対する条件抽出
print(df[df['Sal'] >= 3000]['Sal'])
~~~

実行結果

~~~ sh
Name: Sal, dtype: bool
0     4503
1     4756
2     3186
4     5711
5     3786
6     3909
7     8199
8     5337
9     3913
10    4144
11    4239
12    3086
Name: Sal, dtype: int64
~~~

::: note

- 演習

Salary.csvから以下の条件に従った抽出を行ってください.


1. `Sal列`が偶数の`Sal列`の行

2. `Sal列`が奇数の`Ind列`の行

3. `Ind列`が`Info`,`Edu`のいずれかで,`Sal列`が`4000以上`の`Sal列`

:::

## 値の変更

DataFrameでは選択したセルなどに`=`を利用して代入することで,値の更新が可能です.
以下のコードで,どの部分が変更されているか,コードと結果の対応関係を見てみましょう.

~~~ py
print(df)
df.at[1,'Sal'] = 0
print(df)
~~~

実行結果

~~~ sh
              Ind   Sal
0    Construction  4503
1   Manufacturing  4756
2       Wholesale  3186
3    Accomodation  1949
4         Finance  5711
5     Real Estate  3786
6       Transport  3909
7          Energy  8199
8            Info  5337
9             Edu  3913
10            Med  4144
11          Other  4239
12       Suervice  3086
13           AFFC  2190
              Ind   Sal
0    Construction  4503
1   Manufacturing     0
2       Wholesale  3186
3    Accomodation  1949
4         Finance  5711
5     Real Estate  3786
6       Transport  3909
7          Energy  8199
8            Info  5337
9             Edu  3913
10            Med  4144
11          Other  4239
12       Suervice  3086
13           AFFC  2190
~~~

範囲の変更は,同じ形のリストを渡すことで可能です.

~~~ py
#範囲の変更
df.loc[3:5,'Sal'] = [1,2,3]
print(df)
~~~

~~~ sh
              Ind   Sal
0    Construction  4503
1   Manufacturing     0
2       Wholesale  3186
3    Accomodation     1
4         Finance     2
5     Real Estate     3
6       Transport  3909
7          Energy  8199
8            Info  5337
9             Edu  3913
10            Med  4144
11          Other  4239
12       Suervice  3086
13           AFFC  2190
~~~

リストではなく,単一の値を渡すと一括で変更されます.

~~~ py
#一括変更
df.loc[3:5,'Sal'] = 4
print(df)
~~~

~~~ sh
              Ind   Sal
0    Construction  4503
1   Manufacturing     0
2       Wholesale  3186
3    Accomodation     4
4         Finance     4
5     Real Estate     4
6       Transport  3909
7          Energy  8199
8            Info  5337
9             Edu  3913
10            Med  4144
11          Other  4239
12       Suervice  3086
13           AFFC  2190
~~~


~~~ py
~~~

::: warn

- 配列のコピーと変更

リストの節でも説明しましたが,pandasのDataFrameは,別の変数に代入して,そのDataFrameの値を変更すると**元のDataFrameの値も変更されます.**

~~~ py
df2 = df
df2['Sal'] = 0
print('df --- \n',df)
print('df2 --- \n', df2)
~~~

`df`を`df2`に代入して,`df2`の値を変更すると,元の`df`の値も変更されていることが確認できます.

~~~ sh
df ---
               Ind  Sal
0    Construction    0
1   Manufacturing    0
2       Wholesale    0
3    Accomodation    0
4         Finance    0
5     Real Estate    0
6       Transport    0
7          Energy    0
8            Info    0
9             Edu    0
10            Med    0
11          Other    0
12       Suervice    0
13           AFFC    0
df2 ---
               Ind  Sal
0    Construction    0
1   Manufacturing    0
2       Wholesale    0
3    Accomodation    0
4         Finance    0
5     Real Estate    0
6       Transport    0
7          Energy    0
8            Info    0
9             Edu    0
10            Med    0
11          Other    0
12       Suervice    0
13           AFFC    0
~~~

これは割り当てられるメモリの大きなDataFrameを効率的に利用するための仕様ですが,仕様を理解していないと思わぬ結果となるので注意してください.

~~~ py
print('id df:',id(df))
print('id df2:',id(df2))
~~~

実行結果

~~~ sh
id df: 5013939024
id df2: 5013939024
~~~

大きなデータを扱う際に,部分的に抜き出したデータに編集を加えて,元のデータを変更したくない場合があります.そのような場合には,`.copy()`メソッドを利用します.`.copy()`メソッドを利用することで,元のデータとは異なるメモリが割り当てられます.

~~~ py
#元のDataFrameを保つコピー
## 一度変更を元に戻す
df = pd.read_csv('data/salary.csv')
df.columns = ['Ind','Sal']

df2 = df.copy()
df2['Sal'] = 0
print('df --- \n',df)
print('df2 --- \n', df2)
print('id df:',id(df))
print('id df2:',id(df2))
~~~

実行結果

~~~ sh
df ---
               Ind   Sal
0    Construction  4503
1   Manufacturing  4756
2       Wholesale  3186
3    Accomodation  1949
4         Finance  5711
5     Real Estate  3786
6       Transport  3909
7          Energy  8199
8            Info  5337
9             Edu  3913
10            Med  4144
11          Other  4239
12       Suervice  3086
13           AFFC  2190
df2 ---
               Ind  Sal
0    Construction    0
1   Manufacturing    0
2       Wholesale    0
3    Accomodation    0
4         Finance    0
5     Real Estate    0
6       Transport    0
7          Energy    0
8            Info    0
9             Edu    0
10            Med    0
11          Other    0
12       Suervice    0
13           AFFC    0
id df: 5013945696
id df2: 4559223920
~~~

:::

::: note

- 演習

作成したsaraly.csvを読み込み,以下の処理を順に行ってください.

1. columnsを日本語(産業,給与)に変更してください.

2. サービス業だと思われる行のみを抽出して,`.copy()`してください.

3. コピーされたDataFrameに0から始まるインデックスを振り直してください.

4. 元のDataFrameとコピーされたデータフレームの中身とidを確認してください.

:::

## データ型の確認と変更

pandasでデータを読み込むと列ごとに自動でデータ型が推論,設定されます. pandasのDataFrameにおけるデータ型(`dtype`)はこれまでに見てきたpythonの標準的なデータ型(`type`)とは異なるものですが,同じ名前でも変更が可能です. ただし,`str`型だけ扱いが特殊なので注意が必要です.


pandasの列ごとのデータ型を確認するには,`.dtypes`を利用します.
`type`の`int`に対応する`dtype`は`int64`,`float`は`float64`と表示されます. `str`型は,`object`型になりますがそれぞれの要素は`str`型です.


~~~ py
print('DataFrameのデータ型:\n',df.dtypes)
print('Sal列のデータ型:',df['Sal'].dtypes)
print('Ind列のデータ型:',df['Ind'].dtypes)
print('Ind列の要素のデータ型:',type(df.at[0,'Ind']))
~~~

~~~ sh
DataFrameのデータ型:
 Ind    object
Sal     int64
dtype: object
Sal列のデータ型: int64
Ind列のデータ型: object
Ind列の要素のデータ型: <class 'str'>
~~~

`.astype()`メソッドでデータ型を変更することができます. 元の列に代入する形で変更しましょう.
今回は,`Sal`列を`int型`から`float型`に変更します.

~~~ py
#データ型の変更
df['Sal'] = df['Sal'].astype(float)
print('Sal列のデータ型:',df['Sal'].dtypes)
~~~

~~~ sh
Sal列のデータ型:float64
~~~

データを読み込む際に,`dtyp=列名とデータ型の辞書`を記述することで,データ型を指定することも可能です.

~~~ py
#読み込み時のデータ型の指定
df = pd.read_csv('data/salary.csv'
                ,dtype = {'Industry':str
                         ,'Salary':float})
df.columns = ['Ind','Sal']
print(df.dtypes)
~~~

~~~ sh
Ind     object
Sal    float64
dtype: object
~~~


## DataFrameの作成

これまでは,`salary.csv`を読み込んでDataFrameとして編集してきましたが,プログラム内で新たなDataFrameを作成することも可能です.

新たなDataFrameを作成するためには,`pd.DataFrame()メソッド`を利用します.丸括弧内には, `data`, `index`, `columns`を設定する必要があります. ただし, `index`と`columns`は省略可能です.
`data`には,必要な`行×列の多次元リスト`, `index`と`columns`には1次元のリストを渡します.

以下の表のようなDataFrameを作成するコードは次のようになります.

~~~ py
#DataFrameのvalues部分をリストで作成する
## 行のリストになっていることに注意

#DataFrameのvalues部分をリストで作成する
## 行のリストになっていることに注意
values = [['dog',5.0,60.0]
         ,['cat',3.0,30.0]
         ,['fish',0.15,0.5]]

#DataFrameの作成
df = pd.DataFrame(data=values
                 ,columns=['animal_kind'
                          ,'weight'
                          ,'size'])
print(df)
~~~

~~~ sh
  animal_kind  weight  size
0         dog    5.00  60.0
1         cat    3.00  30.0
2        fish    0.15   0.5
~~~

::: warn

行のリストよりも列のリストを渡すほうがデータ型が統一されており簡単です.
そのような場合には,数値計算用ライブラリ`numpy`の`.T`を利用して,転置行列を渡します.
Shell上で`pip install numpy`を実行して, `numpy`をinstallしたのち,`import numpy as np`と記述して
`numpy`をプログラムで利用しましょう.

`numpy`では,ndarray配列というデータ型を利用します.そのため,`np.array()`メソッドで一度`ndarray`に変換し,
`.T`によって転置した配列を`.tolist()`メソッドでリストに戻しています.

~~~ py
#転置行列によるデータの作成
import numpy as np
animal_kinds = ['dog','cat','fish']
weights  = [5.0,6.0,0.15]
sizes    = [60.0,30.0,0.5]

values = np.array([animal_kinds
                 ,weights
                 ,sizes]).T.tolist()

df = pd.DataFrame(data=values
                 ,columns=['animal_kind'
                          ,'weight'
                          ,'size'])
df['weight'] = df['weight'].astype(float)
df['size']   = df['size'].astype(float)
print(df)
~~~

実行結果

~~~ sh
  animal_kind weight  size
0         dog    5.0  60.0
1         cat    6.0  30.0
2        fish   0.15   0.5
~~~

:::

また,列名を`key`その列のリストを`value`とした辞書型によって`DataFrame`を作成することも可能です.

~~~ py
df = pd.DataFrame({'animal_kind':['dog','cat','fisg']
                  ,'weight':[5.0,6.0,0.15]
                  ,'size':[60.0,30.0,0.5]})
print(df)
~~~

実行結果

~~~
  animal_kind weight  size
0         dog    5.0  60.0
1         cat    6.0  30.0
2        fish   0.15   0.5
~~~

::: note

- 演習

以下の`data`をもつDataFrameを作成し,`age`列を`float64`に変更しましょう.

|name   | age |
| :--:  | :--:|
|Taro   | 20  |
|Jiro   | 30  |
|Hanako | 40  |

:::

## データの追加と削除

列や行の追加には`.assine()`や`.insert()`,`.append()`などが利用可能ですがここでは分かりやすく代入を利用する方法を紹介します.

`df[新規列名]=新しい列`,のようにDataFrameに新しい列を代入することで列の追加が可能です.行の追加は`df.loc[追加したいindex] = 新しい行`で行えます.

~~~ py
#データの追加
#列の追加
print('追加前:\n',df)
df['name'] = ['pochi','tama','kintaro']
print('列の追加:\n',df)
df.loc[4] = ['bird',1.0,3,'piyo']
print('行の追加:\n',df)
~~~

~~~ sh
追加前:
   animal_kind weight  size
0         dog    5.0  60.0
1         cat    6.0  30.0
2        fish   0.15   0.5
列の追加:
   animal_kind weight  size     name
0         dog    5.0  60.0    pochi
1         cat    6.0  30.0     tama
2        fish   0.15   0.5  kintaro
行の追加:
   animal_kind weight  size     name
0         dog    5.0  60.0    pochi
1         cat    6.0  30.0     tama
2        fish   0.15   0.5  kintaro
4        bird    1.0     3     piyo
~~~

特定の列の値を利用して計算された新たな列を作成することも可能です.ここでは,`size`あたりの`weight`を計算してみます. 列を用いた計算では各行の値が計算されます.

~~~ py
#列を利用した計算
df['density'] = df['weight'] / df['size']
print(df)
~~~

~~~ sh
  animal_kind  weight  size     name   density
0         dog    5.00  60.0    pochi  0.083333
1         cat    6.00  30.0     tama  0.200000
2        fish    0.15   0.5  kintaro  0.300000
4        bird    1.00   3.0     piyo  0.333333
~~~


列や行を削除するには`.drop(行名 or 列名, axis= 0/1 )`を利用します.行を削除する場合は`axis=0`,列を削除する場合には`axis=1`と書きます. `axis='column'`,`axis='index'`でも行と列を選択することができます.
`inplace = True` を設定すると元のDataFrameが変更されますが,設定しないと新しいDataFrameが返され,元のDataFrameは変更されません.

~~~ py
#列の削除
df.drop('name'
       ,axis='columns' # or axis = 1
       ,inplace=True)
print(df)

#行の削除
df.drop(4
       ,axis='index' # or axis = 0
       ,inplace=True)
print(df)
~~~

~~~ sh
 animal_kind  weight  size   density
0         dog    5.00  60.0  0.083333
1         cat    6.00  30.0  0.200000
2        fish    0.15   0.5  0.300000
4        bird    1.00   3.0  0.333333
  animal_kind  weight  size   density
0         dog    5.00  60.0  0.083333
1         cat    6.00  30.0  0.200000
2        fish    0.15   0.5  0.300000
~~~

行名や列名にリストを指定すると,複数行/列が削除されます.

~~~ py
#複数行の削除
print(df.drop(['weight','size','density'],axis=1))
~~~

~~~ sh
  animal_kind
0         dog
1         cat
2        fish
~~~

## 欠損値の処理

例えば特定の日だけ値を入力し忘れている出勤簿や授業をサボった学生の成績など,現実のデータには抜けや欠損が付き物です. 抜けや欠損の無いデータを**完備データ**といい,欠損している部分を**欠損値**といいます.

pandasでは,csvの空白, 作成時の未定義部分 などが欠損値`NaN`(Not a Number) として表現されます.

例えば,DataFrame作成時にvaluesのサイズが合わない場合にはデータのない部分が`NaN`となります.
また,以下のように空白のあるCSVを読み込んだ場合には空白のセルが`NaN`となります.

![salary_nan.cav](/images/salary_nan.png)

~~~ py
#DataFrameの作成
df = pd.DataFrame(data = [['dog',5,60]
                         ,['cat',30]
                         ,['fish',0.15]]
                  ,columns=['animal_kind'
                          ,'weight'
                          ,'size'])
print(df)


#かけたデータの読み込み
df = pd.read_csv('data/salary_nan.csv')
print('-----\n',df)
~~~

~~~ sh
  animal_kind  weight  size
0         dog    5.00  60.0
1         cat   30.00   NaN
2        fish    0.15   NaN
-----
          Industry  Salary
0    Construction  4503.0
1   Manufacturing  4756.0
2       Wholesale  3186.0
3   Accommodation     NaN
4         Finance  5711.0
5     Real Estate  3786.0
6       Transport  3909.0
7          Energy  8199.0
8            Info     NaN
9             Edu  3913.0
10            Med  4144.0
11          Other     NaN
12       Suervice  3086.0
13           AFFC  2190.0
~~~

`NaN`はすべての値に対して,`==`で`True`,`!=`で`False`を返すため,欠損値か否かの判定には,`==`や`!=`を使うことはできません. なお,`float('NaN')`で`float`型の`NaN`を生成できます.

~~~ py
df = pd.read_csv('data/salary_nan.csv')
print(df.at[8,'Salary'])
print('NaN == 1:',df.at[8,'Salary'] == 1)
print('NaN == NaN:',df.at[8,'Salary'] == float('NaN'))
print('NaN != 1:',df.at[8,'Salary'] != 1)
print('NaN != NaN:',df.at[8,'Salary'] == float('NaN'))
~~~

~~~ sh
nan
NaN == 1: False
NaN == NaN: False
NaN != 1: True
NaN != NaN: False
~~~

欠損値の判定には`isna()`メソッドを利用します.DataFrameに利用することで`NaN`のあるセルにはTrue,`NaN`以外のセルに`False`を返します.また,`pd.isna(値)`の形で,値の判定も可能です.

~~~ py
df = pd.read_csv('data/salary_nan.csv')
print(df.isna())
print(pd.isna(df.at[8,'Salary']))
~~~

~~~ sh
    Industry  Salary
0      False   False
1      False   False
2      False   False
3      False    True
4      False   False
5      False   False
6      False   False
7      False   False
8      False    True
9      False   False
10     False   False
11     False    True
12     False   False
13     False   False
True
~~~


欠損のあるデータを分析する際には,欠損値をどのように処理するかを決める必要があります.
欠損値の処理の方法には**除外**, **置換**, **補定(ほてい)**,**補完**などいくつかの方法があり,pandasにおいても
`dropna()`,`fillna()`などのメソッドが提供されています.

元も単純な欠損値の処理方法は,欠損値のある行や列を除外してしまうことです. pandasにおいて欠損値の除外行うメソッドは`dropna()`です.

`df.dropna(subset=除外したい列名のリスト,how=‘any’,inplace=True)`と記述することで
指定された列においてNaNの含まれる行の削除ができます.
`subset`を指定しないと,すべての列が対象となります.
`how ='any'`と指定すると, subsetに含まれる列のどれか, `how='all'`にすると, subsetに含まれる列の全てがNaNの場合にその行が削除されます.

~~~ py
#欠損値の削除
print(df.dropna(subset=['Salary'],how='any'))
print('---\n',df)
~~~

~~~ sh
         Industry  Salary
0    Construction  4503.0
1   Manufacturing  4756.0
2       Wholesale  3186.0
4         Finance  5711.0
5     Real Estate  3786.0
6       Transport  3909.0
7          Energy  8199.0
9             Edu  3913.0
10            Med  4144.0
12       Suervice  3086.0
13           AFFC  2190.0
---
         Industry  Salary
0    Construction  4503.0
1   Manufacturing  4756.0
2       Wholesale  3186.0
3   Accommodation     NaN
4         Finance  5711.0
5     Real Estate  3786.0
6       Transport  3909.0
7          Energy  8199.0
8            Info     NaN
9             Edu  3913.0
10            Med  4144.0
11          Other     NaN
12       Suervice  3086.0
13           AFFC  2190.0
~~~

inplace = True と設定するともとのDataFrameが変更されますが Falseに設定すると, もとのDataFrameには反映されません.

~~~ py
df.dropna(subset=['Salary']
         ,how='any'
         ,inplace=True)
print(df)
~~~

~~~ sh
         Industry  Salary
0    Construction  4503.0
1   Manufacturing  4756.0
2       Wholesale  3186.0
4         Finance  5711.0
5     Real Estate  3786.0
6       Transport  3909.0
7          Energy  8199.0
9             Edu  3913.0
10            Med  4144.0
12       Suervice  3086.0
13           AFFC  2190.0
~~~

## データの保存
pandasで作成, 加工したDataFrameオブジェクトは, csv等の形式でファイルに書き出すことができます.
DataFrameオブジェクト`df`をcsvファイルとして書き出すには`df.to_csv('ファイルパス')` と記述します. `encoding=`を追加して記述することでエンコードを指定することも可能です.
BOM付きのUTF-8の場合は`encoding='utf-8-sig'`と記述します.

~~~ py
values = [['dog',5.0,60.0]
         ,['cat',3.0,30.0]
         ,['fish',0.15,0.5]]

#DataFrameの作成
df = pd.DataFrame(data=values
                 ,columns=['animal_kind'
                          ,'weight'
                          ,'size'])
#csvとして保存
df.to_csv('data/animal.csv'
         ,encoding='utf-8-sig')
~~~

::: warn

pandasで可能なデータの処理のうち,今回紹介したのは基本となるほんの一部です.ライブラリの活用方法や機能は日々増えており, 全部を扱うことはできません.必要に応じて自分で利用方法を調べて学習しましょう.

:::

## データの取得と編集全体の演習
::: note

演習問題

---

1. データの作成と編集

以下の条件に従うDataFrameを作成してください.

- columnsが'`1`',`'2'`,`'3'`,`'4'`,'`5`'

- indexが`0~19`

- '`1`'列の値はindexの1倍,`'2'`列の値はindexの2倍,`'3'`列の値はindexの3倍,`'4'`列の値はindexの4倍,'`5`'列の値はindexの5倍の値が入る.

作成したDataFrameに以下の操作を加えてください.

- `'3'`列を2倍した`'6'`という列を追加してください.

- `index`が`20`となる行を追加してください

- `'mean'`という各行の平均値からなる列を追加してください

- `index`が偶数の列のみを残して,すべての列を`int`型に変更した後`BOM付きのUTF-8`の`csv`で保存してください.


2. 外部データの読み込みと編集

以下のURLから近世経済データのEXCELファイルをダウンロードし, 米相場の列に欠損値がないように変更し,データの基本的な構造を確認したのち米相場の平均値を求めてください.

[https://www.rieb.kobe-u.ac.jp/project/kinsei-db/database_excel.html](https://www.rieb.kobe-u.ac.jp/project/kinsei-db/database_excel.html)


:::


</details>

# アルゴリズムとPythonの基本構文

<details>
    <summary> 開く/閉じる </summary>

データを適切に処理するためには,これまで扱ったpandasの技法だけでは足りません.ライブラリに含まれていない処理をPythonで自分で記述する必要があります.

Pythonで処理を記述するためにはそのための**構文**を学習する必要があります.

プログラムで記述される処理を,**アルゴリズム**といいます. **アルゴリズム**とは,**有限回適用することで問題を解くことができる規則の集まり**です. プログラミングとは,プログラミング言語の**データ型と構文を利用してアルゴリズムを記述すること**にほかなりません.

基本的に,アルゴリズムを構成する要素は以下のの3つしかありません.

::: note
- **逐次処理**

手続き型言語は,プログラムは(基本的には)上から順番に実行されます.
プログラムの記述に従って処理を順番に実行することを**逐次処理**といいます.

- **分岐**

条件に従って処理が分岐します. Pythonでは,`if`と`case`という構文によって実装されます.


- **反復**

指定の回数または,条件が満たされるまで処理を繰り返すこと. Pythonでは,`for`,`while`によって実現します. より高度な実装方法として,再帰や高階関数なども利用できますがこの講義では扱いません.

:::

この3つの組み合わせがプログラムで記述できれば,プログラムを利用してどのような問題でも解けます.

### フローチャート
アルゴリズムを視覚的に表現する方法の一つにフローチャートがあります. 原理的には,フローチャートを描くことができれば,アルゴリズムが構築できますし,プログラムが記述できます.

フローチャートは,以下の図形と矢印の組み合わで記述されます(厳密には,ループやデータ代入などもっと細かいですが,ここでは簡単のために省略しています.)

![フローチャートの部品](/images/flow-chart1.png)

例えば,逐次処理は以下のようなフローチャートで表現されます. 以下は,なにかの野菜を育てる手順をフローチャートで表現したものになります.矢印の順番で処理が行われます.

![逐次処理](/images/flow-chart2.png)

そこに,**実がならなかった場合は刈り取る**という分岐を加えると以下のようになります.

![分岐処理](/images/flow-chart3.png)

更に,**実がならなかった場合には,肥料をやり,もう一度水を撒くところからやり直す**という形で,前のステップに戻る反復を追加すると以下のようになります.

![反復処理](/images/flow-chart4.png)

プログラムを用いてアルゴリズムを記述するには,①自然言語(日本語)で解きたい問題が書ける,②その解き方をフローチャートなどを利用してアルゴリズムに変換できる,③アルゴリズムをプログラムに変換できる,というステップを踏むのが一般的です.

最近小学生などの教育で良く耳にする**論理的思考力**や**プログラミング的思考**とは,この①から②への変換ができることを意味しているようです.
プログラミングの学習では,①から②への変換は前提として,プログラムの知識,文法,記法,利用方法などを学習し,②から③への変換を扱います.

最近の小学生は,この学習のために,フローチャートを作ることでプログラムが書ける[Scratch](https://scratch.mit.edu )などを使っているようですが,この講義ではそこからやっている時間はないので,この章でフローチャートの作成と,そのプログラムへの変換の基礎を学習します.

::: note

- 演習問題

以下の処理を表すシンプルなフローチャートをPowerPointを利用して作成してください.

1. カレーの作り方
カレーの作り方を想像して,フローチャートで表現してください.

2. 自動販売機
以下の条件で自動販売機を想定して,
自動販売機にお金が投入されてから商品が出てくるまでのフローチャートを作成してください.

    - 100円の炭酸飲料と,150円のお茶を販売している自動販売機

    - 商品を選択してくださいと最初にアナウンスする

    - 選択された商品に応じた金額を投入してくださいとアナウンスする

    - 投入された金額が足りている場合は商品を出しお釣りを返す

    - 投入された金額が足りない場合は,投入されたお金を全部返して,最初に戻る

3. 100個の1から100までのランダムな数字が書かれたボールが入っている箱Aから,1つずつボールを取り出し,偶数のボールを箱B,奇数のボールを箱Cに入れる作業を小学生に説明するためのフローチャートを作成してください.

:::

## input関数

これからアルゴリズムをプログラムで記述する練習をするにあたって,スクリプト内のプログラムと人間がやり取りをする場面が何度か出てきます.そのために利用される関数に`input()`があります. `input()`はプログラム内で標準入力(Terminalなどに記述された文字列)を受け取る関数です. `()`内には標準入力を受け取る前に出力される文字列を記述します. 文字列が表示された後,ユーザーが入力した文字列が入力されます.

以下のプログラム(`input_test.py`)を記述して実行してみましょう.

~~~ python
x = input('あなたの名前を入力してください. \n')
print(f'あなたの名前は{x}ですね. よろしく!')
~~~

以下のように,`()`の中の文字列が表示されて,入力された文字列が代入された`x`が表示されるはずです.

~~~ sh
❯ python3 input_test.py
あなたの名前を入力してください.
akagi
あなたの名前はakagiですね. よろしく!
~~~



## 逐次処理,インデント,ブロック

pythonは手続き型言語なので,基本的に上から1行ずつプログラムが実行されます. これによってアルゴリズムの構成要素における逐次処理が実現されています.

これまでにこの講義で扱ってきたプログラムは,基本的に1行に1処理を記述しており,処理は上から1行ずつ実行されていました.

しかし,プログラムがより複雑になると,一つの処理が1行では記述できなくなってきます. そこで,プログラムの中で**処理の塊**を作って,その処理の塊を順番に実行していくことになります. Pythonではこの処理の塊を`ブロック`と呼び,`インデント`によって表現します.

~~~ python

def sample():
    print('sample')
####
#↑def のブロックをインデントで表現

total = 0
for i in range(10):
    total += i
    print(total)

####
#↑forのブロックをインデントで表現


for i in range(10):
    for j in range(10):
        print(i*j)
####
########
#↑ インデントの中でインデントを使って
# ブロックの中のブロックを表現
~~~


`インデント`は,パワーポイントと同様に,行頭からの空白を意味します.`半角スペース4つ`が一つの単位となります. `TABキー`で入力される`TAB`をインデントとして利用することもできますが,プログラム中にタブとスペースが混在することになり編集が複雑になるので推奨しません.

しかし,いちいちスペースキーを4回テキストエディタで入力するのは面倒なので,
テキストエディタの設定でタブをスペース4つに変換するように設定しておくと楽です.

Sublime Textでは, `View > Indentation > Indent Using Spaces` をクリックして, `TAB Width: 4`を選択すると,自動でタブがスペースになります.

![TAB to Spaces](/images/tab-to-spaces.png)


## 分岐

Pythonで分岐を表す基本的な構文として`if`文があります. `if`は英語の通り,**もしXXなら**を表しており, `if XXX:` のインデントブロックに,その条件下で行って欲しい処理を記述します.`XXX`の部分には,`Bool`型あるいは`Bool` 型を返す論理式が入ります. 文の最後に`:`を記述するのを忘れないようにしましょう. また,続けて, `elif YYY:`と記述することで条件を追加することができます. 条件を網羅的に書かなくても, `if`と`elif`で指定された条件以外のすべての場合の処理を`else:`で指定することができます.

![ifのイメージ](/images/if1.png)

具体的に,ある変数`x`の大きさによる分岐は以下のように書かれます. 以下の例では,変数`x`と`10`の大小比較の結果によって,表示する文字列を変更しています.

![ifのイメージ](/images/if2.png)

::: warn

- 発展:Bool以外のデータ型とif文の挙動

`if`文の`if`,`elif`以下の部分には基本的にBool型を記述しますが,`Int`型や`str`型なども利用可能です. `int`の場合`1`は`True`,それ以外の数値は`False`とみなされます. `str`の場合,`''`(空白)のみが`False`それ以外は`True`となります.こちらのほうが`True`や`False`より高速なので,サンプルコードなどで利用されている場合がありますので注意しましょう.

例として下のプログラムを実行すると,`1`,`b`,`c`とだけ表示されるはずですので,確認してみましょう.

~~~ py
if 1 :
    print('1')

if 0:
    print('0')

if '':
    print('a')

if ' ':
    print('b')

if 'c':
    print('c')
~~~

:::

それでは,具体的なプログラムを記述してみましょう.以下のプログラムについて考えてみます.

::: note

動物の名前をうけとって,それが`'イヌ'`なら,`'ワンワン'`,`'ネコ'`なら,`'ニャンニャン'`,どちらでもないなら`'???'`と表示するプログラム

:::

これを,まずは日本語のフローチャートで表現して,日本語の部分をプログラムの構文に書き換えてみると以下のようになります.これくらいのプログラムだと,フローチャートを経由して変換するのは面倒かと思いますが,今後より複雑なプログラムを書くにあたっての訓練ですので,取り組んでみましょう.

![動物の名前](/images/if5.png)

フローチャートが書けたら,そのままプログラムを上から書いてみましょう. `else`の部分だけはフローチャートに現れていないので注意してください.

~~~ py
kind = input('動物の名前をカタカナで入力してください. \n')

if kind == 'イヌ':
    print('ワンワン')
elif kind == 'ネコ':
    print('ニャンニャン')
else:
    print('???')
~~~

実行して,意図通りに動くかを確認しましょう.

~~~ sh
❯ python3 pet_name.py
動物の名前をカタカナで入力してください.
イヌ
ワンワン
❯ python3 pet_name.py
動物の名前をカタカナで入力してください.
サカナ
???
~~~



::: note

- 演習問題

以下の,処理のフローチャートと,プログラムを作成してください.

1. 関東の都県を標準入力から受け取り,その都県の県庁所在地を返す.

2. `input()`関数で数値を受け取って, 偶数なら偶数, 奇数なら奇数という文字列を返す.

    - ヒント: `input()`関数の返り値は文字列なので, `int(input(‘数値を入力してください /n’))` のように`int()`関数を利用することで数値に変換できる.

3. ランダムな1から10の数値を発生させて, その数値が5より大きければ`'BIG'`,小さければ`'SMALL'`と表示する.

    - ヒント: 一行目に `import random` と記述して(意味は,後の回で) `x = random.randint(1,10)`と書くと, 1から10のランダムな整数がxに入ります.

:::

## 反復

Pythonにおいて,処理を繰り返す**反復**を実装する構文はいくつか存在しますが,代表的なものに `while文` 及び `for文`があります. 利用頻度からしても,`for文`の方が重要ですが, ここではフローチャートと相性が良く,意味が分かりやすい`while文`で反復の感覚を掴んでから`for文`を学習しましょう.

### While文

プログラムは,コンピュータに何かしらの命令をする文を書くものです. `while文`も`if`のように英文の意味に沿って,プログラムに命令を与えています.
例えば以下の英語の命令文について考えてみましょう.

- Keep working while the timer is running.

(タイマーが動いている間は,働いてください.)

- Water the plants every day while they are not bearing fruit.

(植物に実が成っていない間は毎日水やりをしてください.)

- Continue reading while the light is on.

(電気がついている間は, 読み続けてください.)

これらの文はいずれも, **命令 while 条件** という形を取っており, **条件が真である限り,命令を実行してください**という意味になっています.

プログラムにおける`while文`は上の英文の順序を少し入れ替えて, `while 条件: 命令`の形をとり,`条件`が真である限り,`命令`を実行するという意味のプログラムになります.

![whileのイメージ](/images/while1.png)

`while文`は条件によって,プログラムの継続を判断するので,条件の真偽値が変更されない限り,プログラムが終了しません.
試しに以下のコードを実行してみましょう.

~~~ python
#実行されない
while False:
    print('never printed')

#永遠に数字が増え続ける
#終了するには Ctrl + C
x = 1
while True:
    x += 1
    print(x)
~~~

1つ目の`while文`は,条件が最初から`False`になっているため実行されません.
2つ目の`while文`は,条件が永遠に`True`なので`x`が`1`から増え続けて`print()`によって標準出力され続けます(このように反復を表すプログラムでは, 変数への再代入を多用しますので,忘れている人は復習しましょう.) **プログラムを強制終了するには`Ctrl + C`を押しましょう.**

ウェブサイトやソフトウェアの表示などでは`while文`を利用して永遠にプログラムを動かし続けることをしますが,通常の反復では,何かしらの条件の変更によってプログラムを終了するように条件部分を変更する必要があります.

先程のプログラムを少し変更して, `x`の値が`1`ずつ増えていき,`10`になったら終了するようにしてみるとどうなるでしょうか.

`while文`のような反復は, フローチャートでは,**条件**部分を表す分岐と,前に戻る矢印で表されます. 分岐のあとに,何かしら条件に関わる値が変更されることで,分岐が終了します.

![whileのイメージ](/images/while2.png)


~~~ py
x = 1
while x < 10:
    x += 1
    print(x)
~~~

~~~ sh
❯ python3 while_and_for.py
2
3
4
5
6
7
8
9
10
~~~

続いて,標準入力の結果によって反復の終了条件を判定するための,以下のフローチャートで表されるプログラムを考えてみましょう.

![whileのイメージ](/images/while3.png)

このフローチャートをプログラムに直すと以下のようになり, 問題文に正解しない限りプログラムが終了しません.

~~~ py
x = 0
while 2 != x:
    x = int(input('1 + 1 = ? \n'))
~~~

~~~ sh
❯ python3 while_and_for.py
1 + 1 = ?
3
1 + 1 = ?
4
1 + 1 = ?
2
~~~

#### `while文`と`if文`の組み合わせ

`while文`の中でも`if文`を利用した分岐が可能です. 先程のプログラムに少し加えて,ヒントを出すようにしてみましょう.

![whileのイメージ](/images/while4.png)

`while文`のインデントの中で,更に`if文`のインデントが組まれていることに注意しましょう.

~~~ py
x = 0
while 2 != x:
    x = int(input('1 + 1 = ? \n'))

    if x > 2:
        print('少し数が大きいかも')
    elif x < 2:
        print('少し数が小さいかも')
    else:
        print('大正解!!')
~~~

~~~ sh
❯ python3 while_and_for.py
1 + 1 = ?
3
少し数が大きいかも
1 + 1 = ?
1
少し数が小さいかも
1 + 1 = ?
2
大正解!!
~~~

#### 真偽値以外の`while文`

`while文`では,`if文`と同様にTrue,False以外の値を条件部分に用いることがあります.
`while文`では`リスト`を条件部分に与えることが可能で, **リストが空のとき`False`**として判定されます.

![whileのイメージ](/images/while5.png)

以下のプログラムでは,リストの先頭要素を表示したのち,リストの先頭要素を`pop()`メソッドによって削除しています. 削除を続けて,リストが空になるとプログラムが終了します.

~~~ py
xs = [1,2,3]
while xs:
    print(xs[0])
    #リストの先頭要素を削除(pop())
    xs.pop(0)
~~~

~~~ sh
❯ python3 while_and_for.py
1
2
3
~~~


::: note

- 演習問題

以下のプログラムのフローチャートを作成し,while文を利用してプログラムを記述してください.

1. `x = 1` に`3`ずつ数を足しながら`x`の値を`print()`する. `x`が`1000`を超えたら終了する.

2. `1`から`100`までの数の和を求める.

    - ヒント: 1ずつ値が増える変数とは別に,増えた値を足す変数を最初に作ろう.


3. `100`から`150`までの数のうち,`５`で割り切れるかつ`２`で割り切れる数の和を求める


:::

### for文

続いてもう一つの代表的な反復の表現方法である`for文`について見ていきましょう `while文`の説明の最後に扱った,**リストから一つずつ値を取り出す操作**のように**なにかから値を取り出す操作を繰り返すこと**に特化したのが,`for文`です.

`for文`は, `for x in y: z`の形で,**yから一つずつxを抜き出して,zをしてください**という意味になります.

英文では,以下の用に**For each x in y,**の形で表されますが, プログラムでは**each**が省略されます.

::: note

- For each mandarin in the box, take it out and peel it

(みかんの入った箱から一つずつみかんを取り出して,皮を剥いてください)

- For each student in the class, collect the handout from them.

(クラスの学生一人ひとりからプリントを回収してください.)

- For each panda in the cage, take it out one by one and line them up.

(パンダの檻からパンダを一匹ずつ連れ出して並べてください.)

:::

`for文`をプログラムの世界で利用するには,現実を対象とした英語における,**みかんの入った箱**や**パンダの檻**のようになにか中身を取り出せる入れ物が必要になります.

`python`では,この入れ物として,リストのように,中身を順番に取り出せるもの(オブジェクト)を**イテラブルオブジェクト( iterable object)**を利用します.

::: note

- Iterable object の例

    - リスト  `xs = [1,2,3,4,5]`

    順番に取り出すと, `1,2,3,4,5`の順番で一つずつ出てくる

    - 辞書 `animal_count = {'cat':2,'dog':4,'bird':8}`

    `key`を順番に取り出すと, `'cat','dog','bird'`が順番に一つずつ出てくる

    - タプル `xs = ('a','b','c','d')`

    順番に取り出すと,`'a','b','c','d'`の順番で一つずつ出てくる

    - DataFrameの列や行(`DataSeries`)
:::

![forのイメージ](/images/for1.png)

`for文`における終了条件は,`iterable object`が空になることです.
リストから要素を取り出して,以下のフローチャートで表されるような合計を求める処理について考えてみましょう.

![forのイメージ](/images/for2.png)

プログラムに直すと以下のようになります.

~~~ py
xs = [1,2,3,4,5]
total = 0

for x in xs:
    total += x

print(total)
~~~

~~~ sh
❯ python3 while_and_for.py
15
~~~

xs から取り出した値が`x`に毎回代入されていることに注意しましょう.なお,`x`は変数名なので,任意の名前をつけることが可能です.
また,`pop`を利用した`while文`と違って,元の`xs`の要素数は実際には減っていません.

- `range()`関数

上の例のように,**`m`から`n`**までの連続した数値が欲しい場合に,毎回リストを作っていると大変です. 単純に数値の列が欲しい場合は`range()`関数を使いましょう.
`range(x)`は`0`から始まる`x`個の`シーケンス`(インデックスで位置を指定できるイテラブルオブジェクト)を返します.

~~~ py
for x in range(5):
    print(x)
~~~

~~~ sh
0
1
2
3
4
~~~

`range(始端,終端,ステップ)`の形で複数の引数を指定することで,始端,終端,ステップ(何個とばしにするか)を定めることができます.
終端は,一つ前の値までしか出力されないので注意しましょう.

~~~ py
for x in range(5,10):
    print(x)
~~~

~~~ sh
5
6
7
8
9
~~~

~~~ py
for x in range(5,10,2):
    print(x)
~~~

~~~ sh
5
7
9
~~~

#### `for文`と`if文`の組み合わせ

`while文`と同様に`for文`の中で`if文`を利用したり,`if文`の中で`for文`を利用することができます.

以下のプログラムでは, `1`から`5`までの値の中で偶数のものだけを`print()`しています.

~~~ py
for x in range(1,6):
    if x % 2 == 0:
        print(x)
~~~

~~~ sh
❯ python3 while_and_for.py
2
4
~~~

#### 多重ループ

`for文`の中で`for文`を複数回繰り返して多重ループを実現できます. 以下のプログラムでは, リスト`xs`からリスト`x`を取り出し,取り出したリスト`x`から取り出した要素`y`の合計値を求めています.

~~~ py
xs = [[1,2,3],[4,5,6],[7,8,9]]
total = 0
for x in xs:
    for y in x:
        total += y
print(total)
~~~

~~~ sh
❯ python3 while_and_for.py
45
~~~

::: note

- 演習

以下の処理のフローチャートとプログラムを作ってみよう.

1. `x=0`に`[1,3,5,7,9,12]`を順番に足して更新する. `x`の値を更新するたびに`print()`する.

2. 人物の名前と成績を記録した辞書型`xs={'taro':'S','hanako':'B','yumi':'A','jiro':'D'}`から成績A以上の人物名だけをリスト`upper=[]`に追加し,`upper`を表示する.

3. `100`から`150`までの数のうち,`５`で割り切れるかつ`２`で割り切れる数の和を求める

4. `xs = [[1,2,3,4,5],[6,7,8,9,10],[11,12,13,14,15],[16,17,18,19,20]] `として, `for文`の多重ループを利用して, `xs[0]`から`xs[3]`の合計値が入ったリストを求める

5. `FizzBuzz`とはプログラミングの動作確認でよく用いられる欧米圏の言葉遊びゲームです..
以下のルールに則って`1`から`100`までの数を順番に`FizzBuzz`の判定を行ってください.

    - 3の倍数ならFizzと表示する

    - 5の倍数ならBuzzと表示する

    - 両方の倍数ならFizzBuzz

    - どちらでもないならその数をPrintする.


:::

#### 発展: リスト内包表記

Pythonには`for文`と`while文`以外にも,反復を実現するための手法がいくつか存在します.
そのうち良く使われるものに**リスト内包表記**があります.

リスト内包表記は,集合論の記法を取り入れた書き方で,関数型言語からPythonに取り入れられました.
リスト内包表記は,短い行で簡潔に書けるためリストを生成する際に良く用いられます.

例えば, 1~10までの数値のうち,偶数だけが入ったリストを生成することを考えます.
`for文`を利用すると,例えば以下のようにして得ることができます.

~~~ py
even_numbers = []
for i in range(1,11):
    if i % 2 range(1,11):
        even_numbers.append(i)
print('for文:', even_numbers)
#結果>>> for文: [2, 4, 6, 8, 10]
~~~

1つのリストを生成するのに4行かかっています.
リスト内包表記で同じリストを作成してみます.

~~~ py
even_numbers = [x for x in range(1,11) if x % 2 == 0]
print('内包表記:',even_numbers)
#結果>>> 内包表記: [2, 4, 6, 8, 10]
~~~
同じ結果を一行で得ることができました.

リスト内包表記では`[]`の中に

`[得たい値 for 使いたい要素を代入した変数 in イテラブルオブジェクト if 条件]`

という書き方で,`for文`や`if文`を利用します.

詳細は省きますが,これは数学の集合論における内包表記を真似た記述法になります.

![リスト内包表記](/images/list_complehension.png)

なれるまで難しいかと思いますので,いくつか他の例も見てみましょう.

- 1から10までの数字のうち,奇数のものに3を足したリスト

~~~ py
xs = [x + 3 for x in range(1,11) if x % 2 == 0]
print(xs) #[5, 7, 9, 11, 13]
~~~

- `animals_small = ['cat','dog','bird']`を大文字に変換したリスト

~~~ py
animals_small = ['cat','dog','bird']
animals_large = [x.upper() for x in animals_small]
print(animals_large) #['CAT', 'DOG', 'BIRD']
~~~

- `['CAT', 'DOG', 'BIRD']`のうち文字数が3文字以下の文字列の頭文字のリスト

~~~ py
xs = [x[0] for x in animals_large if len(x) <=3 ]
print(xs) # ['C', 'D']
~~~

- `100`から`150`までの数のうち,`５`で割り切れるかつ`２`で割り切れる数の和

~~~ py
print(sum([x for x in range(100,150) if x % 10 == 0]))
#600
~~~


#### 発展: breakとcontinue

`for文`や`while文`の処理を途中で分岐させたい場合には `break`, `continue`, `else` が利用できます.

::: note
- `continue`: `continue` 後の処理を行わずに反復の最初に戻る

- `break`: 反復を抜け出す

- `else`: `break`以外で`while文`が終了したら実行

:::

例として,あなたはなんの動物か尋ねてニンゲンですと答えないと終わらない以下のような処理を作ってみます.

![whileのイメージ](/images/while6.png)

::: note

この処理はユーザーに「あなたはなんの動物ですか?(カタカナで回答)」と質問し,正しい答え「ニンゲン」を得るまで質問を繰り返します.

まず,answer変数にユーザーの入力を格納します.whileループを使って,answerが「ニンゲン」でない限りループを続けます.

もし,ユーザーが「ウチュウジン」と答えた場合,「本当に!? 怖いのでさようなら!」と表示し,`break`でループを終了します.

ユーザーが「カミサマ」と答えた場合,「わお!! 初めて会いました! … 馬鹿にしないでちゃんと答えてください!」と表示し,再度質問を行います.`continue`で反復の最初に戻ります.

「ニンゲン」「ウチュウジン」「カミサマ」以外の答えの場合,「嘘をつかないで! XXX は喋れません.」と表示し,再度質問を行います.

ループが「ニンゲン」の答えで終了すると,`else`で定義された「そうですよね!ニンゲンに決まっています!」と表示されます.

プログラムに直すと以下のようになります.
それぞれの行がどのような条件で実行されるのか, 確認してみましょう.

:::

~~~ py
# 先に変数を用意します
answer = input("あなたはなんの動物ですか?(カタカナで回答)\n")

# while文でニンゲンですと答えるまで終わらないプログラムを書きます
while answer != "ニンゲン":
    # 答えによっていろいろな反応を組み込んでみましょう
    if answer == "ウチュウジン":
        print("本当に!? 怖いのでさようなら!\n")
        # while ループを終了します
        break

    elif answer == "カミサマ":
        print("わお!! 初めて会いました! ... 馬鹿にしないでちゃんと答えてください!")
        # もう一度値を更新して,ループの最初に戻ります
        answer = input("本当はあなたはなんの動物ですか?\n")
        continue

    # ニンゲンとウチュウジン,カミサマ以外は多分喋れないのでもう一度訪ねます
    else:
        print("嘘をつかないで!" + answer + "は喋れません.")
        # もう一度値を更新します
        answer = input("本当はあなたはなんの動物ですか?\n")

    # これはcontinue も breakもされなかった場合だけ実行されます
    print("今度は真面目に答えましたか?\n")

# breakで終わらなかった == ニンゲンだった場合の処理を書きます
else:
    print("そうですよね!ニンゲンに決まっています!")
~~~


::: note

- 演習問題

質問に対する回答をinput関数で受け取り,それに対して返答をする簡単なBotプログラムを作成してください. なお,分岐は最低5つ以上とすること.
:::

#### `pandas`における`for文`

PandasのDataFrameの処理においても`for文`は良く利用されます.
例えば以下のようなデータについて考えてみます.


なお,こちらのデータは以下のコードで作成できます. コピーして使いましょう.

~~~ py
import pandas as pd

years = [str(x) + '年' for x in range(1800,1820)]
values = ['101','187','150','117','','168'
         ,'195','140','151','123','192','137'
         ,'なし','184','136','192','150','163','141','122']

df = pd.DataFrame({'year':years
                  ,'value':values})

print(df)
~~~

~~~ sh
❯ python3 while_and_for.py
     year value
0   1800年   101
1   1801年   187
2   1802年   150
3   1803年   117
4   1804年
5   1805年   168
6   1806年   195
7   1807年   140
8   1808年   151
9   1809年   123
10  1810年   192
11  1811年   137
12  1812年    なし
13  1813年   184
14  1814年   136
15  1815年   192
16  1816年   150
17  1817年   163
18  1818年   141
19  1819年   122
~~~

このデータは`year`列に`'年'`がついており,`value`列に空白や`'なし'`があります.こういったデータを数値として処理できるようにすることを考えてみましょう.
まずは,`print(df.dtypes)`でそれぞれのデータ型を確認して見ましょう.

~~~ sh
year     object
value    object
dtype: object
~~~
すべて文字列型であることが分かります.

このまま,`year`列を`astype()`を利用して`int`型に変換してみます.


~~~ py
df['year'] = df['year'].astype('int')
~~~

~~~ sh
Traceback (most recent call last):
  File "/Users/akagi/Documents/Programs/Python/slds/while_and_for.py", line 74, in <module>
    df['year'] = df['year'].astype('int')
                 ^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: invalid literal for int() with base 10: '1800年'
~~~

`年`の部分が数値に変換出来ないというエラーが出ます.
では,`year`列から`'年'`を除外してみましょう.

先程の`df['year'] = df['year'].astype('int')`をコメントアウトしてから,以下を実行してみましょう.

~~~ py
# ------------------------------------------------
# year列をInt型にする
# ------------------------------------------------
# year列から"年"を抜かす
# 上から順番に処理するので
# 行番号をdf.indexで取得
for i in df.index:
    # 一番右の文字をなくせば良い
    df.at[i, 'year'] = df.at[i, 'year'][:-1]

# 確認してみる
print(df['year'])

# 年が消えたので,変換してみる
df['year'] = df['year'].astype(int)
print(df['year'].dtype) #int64
~~~

~~~ sh
0     1800
1     1801
2     1802
3     1803
4     1804
5     1805
6     1806
7     1807
8     1808
9     1809
10    1810
11    1811
12    1812
13    1813
14    1814
15    1815
16    1816
17    1817
18    1818
19    1819
Name: year, dtype: object
int64
~~~

上手く数値に変換できたようです.

続いて,`value`列を数値のみにして`float`型にしてみましょう.

 こちらも同じように` df['value'] = df['value'].astype(float)`ではエラー(`ValueError: could not convert string to float: 'なし'
`)が出ます.

色々方法はありますが,`NaN`と`'なし'`の列は適当な値(`150`)を入れることにしてみましょう.
数値に変換できる場合に`True`を返す`str.isdecimal()`を利用して,上から一つひとつ数値に変換可能か識別して,できない場合"150"を入れてみます.

~~~ py
# ------------------------------------------------
# value列をfloat型にする
# ------------------------------------------------
for i in df.index:
    # 数値型に変換可能か調べるには
    # isdecimal関数を使う
    # 変換可能でない場合'150'を入れる
    if not(str.isdecimal(df.at[i, 'value'])):
        df.at[i, 'value'] = '150'

# 確認
print(df['value'])
~~~

~~~ sh
0     101
1     187
2     150
3     117
4     150
5     168
6     195
7     140
8     151
9     123
10    192
11    137
12    150
13    184
14    136
15    192
16    150
17    163
18    141
19    122
Name: value, dtype: object
~~~

無事に数値に変換できました.

このように, `pandas`ではイテラブルオブジェクトとして`index`や`columns`を利用し,`.at[]`などを利用してデータを編集する必要がある場合が多いです.
`pandas`で`for文`,`if文`などの組み合わせた処理に慣れておきましょう.

もう少し複雑な例についても見てみましょう. 以下のDataFrameにおいて,グループ別の平均点を求めてみましょう.

~~~ py
data =  {'group': [5, 5, 1, 2, 1, 5, 4, 3, 2, 1
                  , 5, 3, 4, 1, 2, 4, 5, 1, 1, 1
                  , 2, 3, 1, 2, 4, 4, 3, 4, 4]
        ,'points': [ 45, 23, 58, 96, 38, 41, 33, 30, 82, 42
                   , 42, 89, 66, 94, 25, 36, 52, 40, 93, 80
                   , 44, 39, 79, 67, 38, 43, 100, 62, 54]}
df = pd.DataFrame(data)
print(df)
~~~

~~~ sh
    group  points
0       5      45
1       5      23
2       1      58
3       2      96
4       1      38
5       5      41
6       4      33
7       3      30
8       2      82
9       1      42
10      5      42
11      3      89
12      4      66
13      1      94
14      2      25
15      4      36
16      5      52
17      1      40
18      1      93
19      1      80
20      2      44
21      3      39
22      1      79
23      2      67
24      4      38
25      4      43
26      3     100
27      4      62
28      4      54
~~~

一度, 以下の解答例を見ずに,自分でフローチャートとプログラムを考えてみましょう.


- 回答例
---

::: note
このような処理のプログラムが書けるかどうかは,どのような手順でデータを操作するかをイメージできるかで決まります. これは,プログラミングの文法知識などの問題ではなく,パズルを解くようないわゆる論理的思考や,プログラミング的思考と呼ばれる能力です.

こういった処理を｢どのようにしたら思いつくことができるのか｣はなかなか教えるのが難しいのですが,一つの手順として,**一旦プログラムのことは忘れて,紙と鉛筆でどのように同じ問題を解くかを考えてみる**というのが有効な場合があります.プログラムになれると,直接プログラムで記述することが出来るようになります. しかし,プログラム的思考に慣れていない人はそもそも何をすればいいのかわからないのでプログラムが書けません.

何をすればいいのか皆目検討がつかないという人は, 自分が,紙と鉛筆でどのように解くのかを考えて,それをアルゴリズムに変更し,プログラムに直すという手順を踏んでみましょう.
:::

求めなければならないものをまずはイメージしてみましょう.求めるものは,以下のように,グループごとの値の合計値をグループの数で割ったものです.

![問題のイメージ](/images/average_by_group.png)

この問題を紙と鉛筆で求める場合,どのような方法があるでしょうか.

一例として,上から1行ずつデータを確認し,グループ別に`points`の値をメモする. すべての行を記録し終えたら,グループ別のメモの値を合計値とデータの数を求めて,平均値を計算するという方法でやってみましょう.
小学生でも順番を守れば可能な方法ですね.

![解き方のイメージ](/images/average_by_group2.png)

まず,グループ別のメモをする場所を作成します.どのようなデータ型でも構いませんが,ここでは辞書型で作成してみます.

~~~ py
memo = {1:[]
       ,2:[]
       ,3:[]
       ,4:[]
       ,5:[]}
~~~

データを1行ずつ確認し,メモ帳に記録していきます.

~~~ py
for i in df.index:
    memo[df.at[i,'group']] += [df.at[i,'points']]

print(memo)
"""
{ 1: [58, 38, 42, 94, 40, 93, 80, 79]
, 2: [96, 82, 25, 44, 67]
, 3: [30, 89, 39, 100]
, 4: [33, 66, 36, 38, 43, 62, 54]
, 5: [45, 23, 41, 42, 52]}
"""
~~~

記録したメモを元に,一つひとつ合計値とデータ数を求め,平均値を計算してみます.

~~~ py
#グループ1からグループ5まで順番に作業する
for i in range(1,6):
    total = 0
    length = 0
    #グループ別のリストを足していく
    for x in memo[i]:
        total += x
        length += 1
    print(f'group:{i}, mean:{total/length}')

"""
group:1, mean:65.5
group:2, mean:62.8
group:3, mean:64.5
group:4, mean:47.42857142857143
group:5, mean:40.6
"""
~~~

これで無事に,グループ別のデータが求められました.
当然,もっと効率的で,簡単に計算する方法は沢山あります. 似た問題を検索し特定のメソッドを利用したり,AIに聞くことで,何も考えずに計算することも可能です.

例えば, 先程の平均値の計算部分に関しては,組み込み関数の`sum()`や`len()`を利用することで,自分で書かなくても計算が可能です.

~~~ py
for i in range(1,6):
    print(f'group:{i},mean:{sum(memo[i])/len(memo[i])}')

"""
group:1,mean:65.5
group:2,mean:62.8
group:3,mean:64.5
group:4,mean:47.42857142857143
group:5,mean:40.6
"""
~~~

また,以前扱った条件抽出を使えば,一つ一つデータを集める必要もありません. 以下の3行で同じことが可能です.

~~~ py
for i in range(1,6):
    group_mean = df[df['group'] == i]['points'].mean()
    print(f'group:{i},mean:{group_mean}')
"""
group:1,mean:65.5
group:2,mean:62.8
group:3,mean:64.5
group:4,mean:47.42857142857143
group:5,mean:40.6
"""
~~~

更に,`pandas`の`.groupby()`メソッドを使えば,1行で同じような処理が可能です.

~~~ py
print(df.groupby('group').mean())
"""
         points
group
1      65.500000
2      62.800000
3      64.500000
4      47.428571
5      40.600000
"""
~~~

しかし,そのような方法ではいつまでたっても自分でプログラムが書けるようにはなりません.また,便利なメソッドが使えても,その背後で何をしているのかを理解していないと,計算した値の意味を理解したり,正しく利用することができません.

まずは自分の頭でどのように処理することで,求めたい値が計算できるのかの手順を考えて,プログラムが書けるようになったあとで,便利なメソッドなどの利用法を覚えることをおすすめします.


#### 発展: 高階関数

Pythonで反復を実現するもう一つの方法として**高階関数**があります.高階関数とは,関数を引数にとる関数であり,こちらも関数型言語から取り入れられ,`pandas`において**各行に同じ処理を適用したい場合**に良く用いられます.

本資料ではまだ,関数に関して扱っていませんが,発展的内容としてここだけで完結する範囲で簡単に説明します.
興味がある方は, 関数を学んだあとにもう一度こちらを見てみると理解が深まるかもしれません.

以下の`DataFrame`を事例に考えてみましょう.

~~~ py
df = pd.DataFrame({'kind':['cat','dog','fish']
                  ,'weight':[30,20,10]})
print(df)

"""
   kind  weight
0   cat      30
1   dog      20
2  fish      10
"""
~~~

このデータの`kind列`を大文字にすることを考えてみましょう.
`for文`を利用すると以下のようになりますね.

~~~ py
for i in df.index:
    df.at[i,'kind'] = df.at[i,'kind'].upper()

print(df)

"""
   kind  weight
0   CAT      30
1   DOG      20
2  FISH      10
"""
~~~

この処理を,高階関数`map()`を利用すると,以下のようになります.

~~~ py
df['kind'] = df['kind'].map(lambda x : x.upper())
print(df)
"""
   kind  weight
0   CAT      30
1   DOG      20
2  FISH      10
"""
~~~

`map()`は,与えられた`DataFrame`の列(`DataSeries`)の各行に対して,`()`内の処理を適用した結果を返す高階関数です.

`lambda x: x.upper()`を`無名関数`や`ラムダ式`といい, `lambda 変数: 変数に適用したい処理`の形で書きます. 今は,`DataSeries`の各行を`x`として,その`x`に`.upper()`を適用しています.

`条件に合致する場合の処理 if 条件 else 合致しない場合の処理` と書くことで条件を加えることも可能です.

以下の例では, `weight`列の値が`15`より大きければ2倍するという処理を行っています.

~~~ py
df['weight'] = df['weight'].map(lambda x: x*2 if x > 15 else x)
print(df)
"""
   kind  weight
0   CAT      60
1   DOG      40
2  FISH      10
"""
~~~

高階関数を使った処理は,`for文`より簡潔に書けて,かつ処理速度も高速です. 機会があれば積極的に使ってみましょう.


例えば,先程の以下のDataFrameで,`year列`から年を削除し,`value列`の空白を`150`に置き換える処理は高階関数を利用すると以下のように書けます.

~~~ py
import pandas as pd

years = [str(x) + '年' for x in range(1800,1820)]
values = ['101','187','150','117','','168'
         ,'195','140','151','123','192','137'
         ,'なし','184','136','192','150','163','141','122']

df = pd.DataFrame({'year':years
                  ,'value':values})
print(df)

f['year'] = df['year'].map(lambda x : x[-1])
f['value'] = df['value'].map(lambda x: '150' if str.isdecimal(str(x)) else x)
~~~

::: note

- 演習問題

以下のURLから近世経済データのEXCELファイルをダウンロードし, 米相場の列に欠損値がないように変更し,データの基本的な構造を確認したのち米相場の西暦ごとの平均値を計算し,辞書型に格納してください.

[https://www.rieb.kobe-u.ac.jp/project/kinsei-db/database_excel.html](https://www.rieb.kobe-u.ac.jp/project/kinsei-db/database_excel.html)
:::

## 関数とクラス

これまでは,基本的に記述したコードは,コード内で一度しか利用せず,上から順番に一つずつ行いたい処理を記述してきました. しかし,プログラミングを続けていると,同じ処理を何度も適用する場合が出てきます.その場合に,毎回処理を記述するのは労力がかかります.

ここでは, 同じ処理を再利用可能な形でまとめ,様々な場所で利用する方法を学びます.

Pythonでは,基本的に小さな一つの処理は**関数**にまとめて利用します.また,複数の関数やデータ型などを**クラス**という単位にまとめることも可能です.そのようなまとめた塊を組み合わせてプログラムを構築する手法を**オブジェクト指向プログラミング**といいます. 更に,いくつかの関数やクラスをまとめたものを再利用形なファイルにまとめることで**モジュール**が作られ,モジュールに階層構造を設定したものを**ライブラリ**といいます.

この講義では関数の利用法を学びますがクラスやオブジェクト指向プログラミング,モジュールの作成に関しては少し触れるだけにします. 興味がある方は, より専門的なプログラミングの講義などで学習してください.


### 関数

皆さんはこれまでにもいくつかの関数を利用してきました. 例えば,リストの要素数を求めるための`len()`や,標準出力するための`print()`などは関数です.

関数は基本的に`()`の中に**引数**を与えられて,**返り値**を返します.

~~~ py
print(len([1,2,3])) #>>> 3
~~~

上の`len()`では,`()`の中に引数としてリスト`[1,2,3]`が与えられ,返り値として`3`を返しています.

::: warn

一方で,関数と似ているが異なる概念として,`.mean()`のようにオブジェクトの後ろに`.`を利用してつなげる**メソッド**もあります.
メソッドに関しては, クラスの部分で説明します.

~~~ py
df = pd.DataFrame({'x':[1,2,3]})
print(df.mean()) #>>> 2.0
~~~
:::

このように,最初からPythonに実装されている関数を**組み込み関数**といいます.また,特定のライブラリで定義された関数もあります.

組み込み関数としては,`len()`以外にも,合計値を返す`sum()`や,文字列に変更する`str()`などを利用してきました. Pythonの組み込み関数は[こちら](https://docs.python.org/ja/3/library/functions.html)で確認できます. 利用法のわからないものに関しては調べてみましょう.

これまでは,無名関数を除いて基本的にすでに作成された関数を利用してきましたが,関数は自分で作成することも可能です.

関数は, `def 関数名(引数を表す変数):`という構文で定義することができます. インデントブロックの中で,引数に加えた処理を記述し, `return 返り値`の形で,関数の返り値を定義します.

例えば,要素数を数える`length()`という関数を実装してみましょう.
まずは,関数にする前に今まで通りにリストの要素数を数えるプログラムを書いてみましょう.

~~~ py
xs = [1,2,3]
count = 0
for x in xs:
    count += 1
print(count) >>> 3
~~~

これを関数にしてみます. `def length(xs):`のインデントブロックに,行いたい処理を記述し,`return count`で`count`を返します.

~~~ py
def length(xs):
    count = 0
    for x in xs:
        count += 1
    return count

print(length([1,2,3])) #>>> 3
print(length(['a','b'])) #>>> 2
~~~

関数として定義することで,毎回数を数える処理を記述しなくても,様々なリストの要素数を数えることが可能になりました.

引数には複数の値を指定することも出来ます. 例えば以下の関数`get_larger_than(xs,y)`は`xs`の中から`y`より大きな値のみを返します.

~~~ py
def get_larger_than(xs,y):
    result = []
    for x in xs:
        if x > y:
            result.append(x)
    return result

get_larger_than([2,3,4,5,6],3) #>>>[4, 5, 6]
~~~


`引数名=デフォルト引数` と書くことで,引数にあらかじめ値を指定することも可能です. 関数を実行する際に,何も指定しなければ,デフォルト引数が利用されます.

~~~ py
def greet(name='guest', greeting="Hello"):
    return f'{greeting}, {name}!'

# デフォルト引数を利用する場合
print(greet())  # Hello, guest!

# デフォルトの引数の一部を上書きする場合
print(greet(name='Taro'))  # Hello, Taro!
print(greet(greeting='Good Morning'))  # Good Morning, guest!

# 全ての引数を指定する場合
print(greet(name='Taro', greeting='Good Morning'))  # Good Morning, Taro!
~~~


::: note

- 演習問題

1. 与えられた数値のリストの合計値を返す関数

2. 与えられた数値のリストの最大値を返す関数

3. 与えられた数値にFizzBuzzの結果を,文字列で返す関数

4. 組み込み関数の`filter()`の仕様を調べて自分で実装してください.

:::

### 発展:クラスとインスタンス

先ほど関数を自分で定義して使う方法を学習しましたが,Pythonではデータ型も自分で定義することができます.

::: note

**クラス(Class)** とは, データ型,データ型の保有するデータ(**属性**)とデータ型に付随する機能(**メソッド**)を定義する機能です.
クラスを具体化したものを**インスタンス**と呼びます.
また,インスタンスが生成される際に実行されるメソッドを**コンストラクタ**といいます.

:::

これまでに見た例では,pandasの`DataFrame`などはライブラリによって新たに定義されたデータ型です. `.DataFrame()`という**コンストラクタ**によって具体的な`DataFrame`**インスタンス**が生成されます.

`DataFrame`オブジェクトの属性として,`shape`などを取得することができ,`.to_csv()`などの`DataFrame`に特有の機能(**メソッド**)を利用することができました.

それでは,具体的にクラスを作成してみましょう.
ここでは,これまでに何度か出てきた,`FizzBuzz`専用のクラスを作成してみましょう.

::: note

新しいクラスを宣言するには `class クラス名:` と記述します. インデントブロック内に,コンストラクタと,メソッドを記述します. クラス名は大文字で始めます.

コンストラクタは, `def __init__(self,必要な情報):`の形で宣言します. `__init__(self,`の部分は基本的にすべてのクラスで共通です.
`FizzBuzz`クラスでは, 数値を引数に取ります. `self`は,属性やメソッドが属するインスタンスを表す変数でコンストラクタやメソッドの第1引数は常に`self`となります. `self.属性`,`self.メソッド()`などの形で,そのインスタンスの属性やメソッドを定義します.

各インスタンスに固有の属性をインスタンス属性といい,ここでは生成された`FizzBuzz`インスタンスは`.number`という属性を持ちます. `self.number = number`と買うことで生成時に引数として与えられた数値`number`をインスタンス属性として保存します.

最後に,`FizzBuzz`ゲームを実行するための機能,`.evaluate()`を実行します.

:::

~~~ py
# FizzBuzzというクラスを宣言します.
class FizzBuzz:
    #コンストラクタの定義
    #FizzBazzインスタンスを生成する際に必要となるデータを定義する
    def __init__(self, number):
        #インスタンス属性
        self.number = number

    #メソッド eveluateの定義
    #FizzBuzzインスタンスは,evaluateすることで,`FizzBuzz`の
    #ゲームが実行されます
    def evaluate(self):
        if self.number % 3 == 0 and self.number % 5 == 0:
            return "FizzBuzz"
        elif self.number % 3 == 0:
            return "Fizz"
        elif self.number % 5 == 0:
            return "Buzz"
        else:
            return str(self.number)

# 使用例
# 単一の数値を評価
fb = FizzBuzz(15)
print(fb.evaluate())  # FizzBuzz

fb = FizzBuzz(9)
print(fb.evaluate())  # Fizz

fb = FizzBuzz(10)
print(fb.evaluate())  # Buzz

fb = FizzBuzz(7)
print(fb.evaluate())  # 7

~~~

クラスの重要な機能に**継承**があります. 継承は新しいクラス(**サブクラス**)を作成する際に,すでにあるクラス(**スーパークラス**)の属性やメソッドを引き継ぐことを意味します. これによって,コードの再利用や,拡張が用意になります.

以下では,先程定義した `FizzBuzz`クラスを継承して,`3`と`5`以外の場合にも特別な挙動をする`AdvancedFizzBuzz`クラスを定義してみましょう.

サブクラスを定義するには, `class サブクラス名(スーパークラス名)`と記述します.
インデントブロック内では, `super().属性`や`super().メソッド`と書くことで,スーパークラスの属性やメソッドを利用することができます.

~~~ py
# 派生クラス AdvancedFizzBuzz
# ()内にスーパークラスを書きます
class AdvancedFizzBuzz(FizzBuzz):
    def __init__(self, number, custom_message=None):
        # スーパークラスの__init__を呼び出す
        super().__init__(number)
        self.custom_message = custom_message

    def evaluate(self):
        # スーパークラスのevaluateメソッドを拡張
        # self.custom_message が定義されている場合のみ実行されます
        if self.custom_message and self.number % 7 == 0:
            return self.custom_message
        else:
            # スーパークラスのevaluateメソッドを呼び出す
            return super().evaluate()


# サブクラスの使用例
afb = AdvancedFizzBuzz(21, custom_message="Hozz")
print(afb.evaluate())  # Hozz (7で割り切れるためカスタムメッセージ)

afb = AdvancedFizzBuzz(10)
print(afb.evaluate())  # Buzz (スーパークラスのメソッドが呼ばれる)
~~~

::: note

- 演習

1. FizzBuzzクラスを拡張して, `3`かつ`5`かつ`7`の倍数のときに`Hozz`と表示されるようにしてください.

2. ジャンケンを行うためのクラスを定義してください.

:::


</details>

# データサイエンスを始めよう 研究計画の建て方

<details >
    <summary> 開く/閉じる </summary>

ライブラリの章で説明したように,データサイエンスの作業は基本的に以下の順序で進みます.

![データ分析の流れ](/images/data-science-flow.png)

皆さんは, ここまでの講義において, データを取得し,前処理を実施する方法を学びました. データサイエンスは, 文字通りデータを利用する学問ですが,データが使える,編集できる状態にやっとなったわけです.

ここから,順に可視化, 数値化, 分析に進みますが,それと並行して,皆さんの研究計画を立てていきましょう. できれば,研究計画に沿ったデータを取得し, その可視化,数値化,分析を学習しながら自分のデータに適用していくことを目指します.

# モデルとデータ(執筆中)

統計学やデータサイエンスはいずれもデータを利用しますが,データを利用して得たいものは何でしょうか. 統計学や,データサイエンスは,世界に何かしらの**構造(モデル)**が存在するという**仮定**をおいて,情報を利用してその構造を明らかにすることを目的としています.

- 星の動きは特定のルールに従っている

- コインを投げて表が出るか,裏が出るかは,コインの構造や投げ方で決まる

社会科学では,そのような構造がない,あるいは変化する,作るなどの立場もありますが,ここでは深くは扱いません.

しかし, 世の中の何かしらの対象の動きや現象を決める構造はどのようにすれば明らかになるのでしょうか. 私達は,情報を解釈することでお野中の何かしらの現象の背景にモデルを見出します. 単純な法則であれば,目で見て,耳で聞いて,触ってという五感によってモデルを発見しますが, そのようなものも,視覚,聴覚,触覚などの情報といえます. 自然の情報は複雑すぎるためにそのような情報全ては利用できません, 私達は**データ**と言う形で情報を抽出し,利用可能な形態にまとめます. 情報をそのまま眺めていても,まだ人間には複雑過ぎます,そこで,統計やデータサイエンスの技法を用いてそれらの情報を何らかの基準でまとめて,理解可能な形,すなわち**モデル**に変形します.

![情報の抽出](/images/info-to-model.png)

::: warn

統計におけるモデルの話(あとで載せる予定)

:::

この講義はデータサイエンスを扱うので, **データからデータを生み出した世界の背景にある構造を明らかにする**ことを目指します. なので, これから皆さんに立ててもらう研究テーマを決めるには,**自分が明らかにしたい構造**を決めて貰う必要があります.

::: note

過去のこの講義における研究テーマでは,

- **大学の学生の成績が決定する構造とは?**
---
    大学の学習データを用いたカテゴリーデータ解析によって, 授業形態別に良い成績を取る学生とそうでない学生の特徴を明らかにしました.

- **ヒット曲を生み出す構造は?**
---
    過去のヒット曲データや歌詞を利用したクラスタリングによって, 時代ごとのヒット曲の変遷と,ヒット曲になりやすい曲の特徴を明らかにしました.

- **大学の電力消費を決めている構造は?**
---
    大学の棟別の電力データを利用した時系列解析によって,大学の電力消費に影響の大きい要素を明らかにしました.

- **国ごとのLGBTQに関する言説を決める要因は?**
---
    言語毎のLGBTQに関するWikipediaの記事の自然言語解析結果と,世界LGBTQ需要度ランキングの関係性を明らかにしました.

- **大学生のメンタルヘルスを決める構造は?**
---
    アンケート調査を利用した共分散構造分析によって,大学生の大学生活の様子とメンタルヘルスの関係を明らかにしました.

などなど,それぞれが知りたい,何かの現象を生み出す構造を明らかにする研究を実施しました.

皆さんが研究テーマを決めるにあたって,必要となる第一歩は,**何の**,**何を決める**構造が知りたいのかを決めることです.
:::

ところで,**構造を明らかにする**と言ってもどのようにするのでしょうか.

これから学ぶ様々な統計,データサイエンスの手法によって,明らかにすることができる構造は異なります.


::: note

例えば,

- **関係がある → 相関**
---
    SDGsに取り組む企業は, 生産性が高い傾向がある
    成績が良い学生は, 勉強時間が長い傾向がある
    ただし, 傾向があるだけで, 勉強を沢山すれば,成績が伸びるという説明(因果関係)はできない

- **違いがある → 検定**
---
    オンデマンド授業の方が, 対面授業よりも学生の成績が高い

- **モデルで説明できる(予測できる) → 多変量解析, 統計モデリング(回帰など)**
---
    生産性がX高まると,SDGsへの取り組みの量がY増える

- **区別/要約できる → クラスタリング,次元削減**
---
    流行りの音楽は,5つのグループに区別できる
    流行りの音楽の特徴をまとめると,XやYである.

といった手法があります. 自分が,対象の何を知りたいのかを明らかにすることで,使用する手法は異なります.

:::

この講義では, 分析手法を扱う章でこれらの手法を扱います. 時間の都合上すべてを詳細に扱うことはできませんが,自分の知りたい対象に使える手法1つか2つだけを選んで,それを利用できるようにしていきましょう.

## 手法の決め方

統計やデータサイエンスは非常に広い学問なので,その学習では様々な手法を断片的に学習し, その後必要となる特定の手法を深く学ぶというのが一般的です. では,必要となる手法はどのように決めるのでしょうか.


手法は,目的とデータによって決まります. まず,目的(何が知りたいか)が決まり,利用できるデータが決まったあとに,使える手法(何が言えるか)が決まります. これは目的が先にあるパターンですが,データが先,使いたい手法が先などで研究計画が決まる場合もあります.

![目的,手法,データ](/images/purpose_method_data.png)

::: warn

- **HARKing**について
---

この講義はあくまで,研究を通じてデータサイエンスの手法を学習することを目指しているため, 以下の様に,手法やデータ優先で研究計画を立てることも認めていますが,研究の世界では, データが有り,何かしらの手法で分析した後に,その結果を元々知りたかったかのように報告することは[**HARKing**](http://journals.sagepub.com/doi/10.1207/s15327957pspr0203_4)と呼ばれる研究不正の一種とみなされる場合があります.

データ優位の分析は,データサイエンス分野や,観察研究と呼ばれる分野でしばしば行われていますし,特定の手法が利用可能な分野やデータを選択することは特定の手法の学習のためには有用です.

しかし,**統計的仮説検定**などの統計的な仮説を前提とする分野では,望ましい態度ではないとされています.

卒業研究などで,研究をする場合には,指導教官の指導方針に従って,研究計画を立てるようにしましょう.

:::

::: note

- **目的から**
---
    - この構造を明らかにしたい.

    - → この構造から生じたデータを集める → このデータを使って構造を表す手法を選ぶ

    - → この手法で明らかになる →この手法が使えるデータを集める

- **データから**
---
    - このデータを分析したい

    - → このデータを分析できる手法を選ぶ → この手法とデータから明らかになることを考える

    - → このデータから明らかにしたい構造を考える → データから構造を明らかにする手法を考える

- **手法から**
---
    - この手法を使いたい,作りたい

    - → この手法で何が明らかになるのかを決める → この手法が使えるデータを集める

    - →この手法が使えるデータを集める → この手法とデータから明らかになることを考える

:::

## 研究計画を建てよう

**目的,データ,手法**いずれかを決めましょう.

::: note

- **目的の決め方**
---

    - 自分の知りたい構造を沢山挙げる

    - その構造の仮説を立てる

        例: Twitterでバズる方法には規則があるかもしれない?, 少子高齢化には背景に構造があるかもしれない?

    - → すでに明らかになっていないか,本,論文を探す(先行研究).

        何が分かっていて何が分かっていないのか?

    - 大変なところ: アイデア勝負(創造性), 先行研究を調べるのが大変

- **手法の決め方**
---

    - 沢山の手法の原理/使い方を学ぶ

    - 興味のある,自分で使えるようになりたい手法を選ぶ

        例:兎に角テキスト解析してみたいぜ!,  取り敢えず統計モデルっていうものを作ってみたいぜ!

    - → その手法について学習して, 使えるようにする.

        どんな原理で,何が明らかになるもので,どのように使うのか?

    - 大変なところ: 手法は難しい,沢山ありすぎる,数学とかプログラミングが必要

- **データの決め方**
---

    - 様々なデータを探す/自分でデータを作ってみる(実験,調査)

    - → データの特徴を分析(記述統計学),設計する(実験計画)

         例: 取り敢えずYoutubeの視聴データが面白そう!, 大学の教育のデータから何が言えるかな?, アンケート調査とかしてみたい!

    - 大変なところ: 実験や調査は手間がかかる. データの特徴を調べないといけない.

この3つをそれぞれ,自分の興味のあるものを考えて見よう.
最低3アイデアを考えてみましょう.(目的1つで3つでも, それぞれ1つずつでもOK)

:::

どれか1つ(目的/手法/データ)が決まったら,残り2つを順番に探してみましょう.
しかし, 実際に行ってみると,途中でうまくいかないことが大半です.

::: note

- **目的も手法も決まったけどデータがない!**
---
    - 千葉商科大学生の親の所得と成績について,回帰分析したい!

    → けどそんなデータはない

    → アンケート調査する?/変更する

- **データも手法も決まったけど,出てくる構造がすでに知られている/つまらない**
---
    天気と気温のデータがあるので検定をすれば,晴れの日は雨の日より気温が高いことが分かる

    → そんな当たり前のことが分かっても…..

- **目的もデータもあるけど,手法が難しすぎる/存在しない**
---
    企業の会計データと取引経路のデータがあるので,これで経済構造を明らかにできる!

    → エージェントベースモデリングをやるには,数百万円の計算機と,数万行のコードを書く必要があるし,モデル作成も複雑すぎる….

ので行ったり来たりするのが通常です.

:::

卒業研究などではこの過程に,1年以上かけますが,この講義では数ヶ月しか時間がとれません. そこで,**少し妥協する必要があります**.

本当に知りたいこと,本当にやりたいこと,本当に分析したいデータなどが,上手く使えればそれに越したことはありませんが,なかなか上手くは行きません.
一生物の仕事になる場合もあります(その場合は是非,大学院に進んで研究者になりましょう.)

この講義では,1年で0から行うので,

::: note

- **すぐに結果が出そうな目的**
---
    - おおよそ明らかになっていることを少し変える

    - 既に知られていることでも取り敢えずやってみる

    - 本当に知りたいことのほんの一部だけに限定する

- **手頃なデータ**
---
    - 昨年の講義で利用されているデータ

    - 簡単に手に入るデータ

- **簡単な手法**
---
    - 授業で扱っている基礎的な手法に限定する

    - やりたいことの簡単なバージョンを利用する

:::

くらいに,妥協する必要があります(それでも半年で行うのはかなり大変です.)

これから,具体的な統計やデータサイエンスの手法を学習するので, 並行してできるだけ早くに,自分がこの講義で何をやるのかを決めていきましょう.

::: note

- **演習**

- 目的,手法,データ,どれでも良いので,3つ思いついたものをスライドにまとめてください.

    - デザインを入れるなど,きれいにまとめる必要はありません.議論のためのメモ帳として作成してください.

    - あとで補足,追加できるように1つのテーマにつき,1ページ使用してください.

    - 何かを調べた,読んだ,参考にした場合はその出典を書いておいてください.

        URL直張りでも構わないので,必ずあとで参照できる形でメモを残しておいてください.

    - 途中経過で構わないのでグループウェアの自分のチャネルにアップロードしてください.

    現時点では,実現が困難なものでも,不可能なものでも,つまらないものでも構いません,取り敢えず沢山やってみたいこと,興味のあることをまとめましょう.後ほど,その資料を元に,説明してもらい,ディスカッションします.

:::

</details>

# データの可視化

<details >
    <summary> 開く/閉じる </summary>

ここまでで,プログラミングを利用してデータを読み込み,編集ができるようになりました. これから,データを分析する手法を学習しましょう. データ分析の第一歩は,データの**可視化**です.

統計データはテーブル形式のまま,眺めても理解できません. データを可視化することで,データの特徴や意味を理解する助けになります. ここでは,データを可視化するいくつかの方法を学びましょう.

::: note

- ライブラリのインストール
---

Pythonにおけるグラフ作成の,代表的なライブラリには,[`matplotlib`](https://matplotlib.org)や[`seaborn`](https://seaborn.pydata.org)があります.
それぞれ, `pip install`しておきましょう.

`matplotlib`,`seaborn`はそれぞれ以下のようにインポートするのが一般的です.

- `import matplotlib.pyplot as plt`

- `import searborn as sns`

:::

::: warn

- `matplotlib`における日本語表示
---
`matplotlib`は日本語などのマルチバイト文字に対応しておらず,日本語を使用すると日本語部分が日本語部分が, □(通称豆腐)に変わります.

 例: ｢Hello こんにちは｣ → ｢Hello □ □ □ □ □ ｣

`matplotlib`で日本語を利用する方法として一番簡単なものに, `japanize-matplotlib`の利用があります.
`pip install` したあとに, `import japanize_matplotlib` をしましょう(`pip` では `'-'`(ハイフン)ですが,`import`文では,`'_'`(アンダーバー)なので注意してください.)

:::

::: warn

- Python 3.12 における`japanize_matplotlib`

Python3.12では,`japanize_matplotlib`を読み込もうとすると,`ModuleNotFoundError: No module named 'distutils'` と表示されます.

これは,`japanize_matplotlib`内で利用されている`distutils`というモジュールが廃止されたことによります. こちらのエラーは, `distutils`の代替である`setuptools`を `pip install`することで消えます.

参考: [【inshellisense】ModuleNotFoundError: No module named 'distutils'の対処法](
https://qiita.com/pitao/items/1740a62ddee797aed807)

:::

本章におけるコードは以下, 先頭に以下のような記述があることが前提となります.

~~~ py
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import japanize_matplotlib
import seaborn as sns
~~~


## 基礎的なグラフ(棒グラフ,円グラフ,折れ線グラフ)

`matplotlib`を利用して,グラフを作成する大まかな手順は以下のようになります.

::: note
1. データを準備する.
    BOMなしのCSVを作りましょう

2. `pandas`でプログラムでCSVを読み込みましょう

3. グラフにしたい部分を抽出します.

    グラフを作るために何が必要かはグラフの種類によります.

5. `matplotlib`のメソッドを利用してグラフを生成します.

    基本的なメソッドは以下のとおりです.

    | グラフの種類 | メソッド |
    | :---:        | :---:    |
    |              |          |
    | 棒グラフ     | `plt.bar(x軸のリスト,y軸のリスト)`                      |
    | 円グラフ     | `plt.pie(カテゴリー別のデータのリスト,label=ラベル)`    |
    | 折れ線グラフ | `plt.plot(x軸のリスト,y軸のリスト)`                     |
    | 散布図       | `plt.scatter(x軸のリスト,y軸のリスト)`                  |

    これら以外の,グラフに関しては個別に扱います.

6. グラフの見た目を整えます.

   ある程度自動で設定されますが,色,メモリ,形,軸などを個別に設定できます.

   | グラフの要素 | メソッド                        |
   | :---:        | :---:                           |
   |              |                                 |
   | タイトル     | `plt.title('タイトル')`         |
   | 判例         | `plt.legend()`                  |
   | y軸          | `plt.yticks(軸の目盛のリスト)`  |
   | x軸          | `plt.xticks(軸の目盛のリスト)`  |
   | y軸ラベル    | `plt.ylabel('ラベル')`          |
   | x軸ラベル    | `plt.xlabel('ラベル')`          |
   | 補助線の追加 | `plt.minorticks_on()`           |
   |              | `plt.grid(which='both')`        |
   | 表示領域     | `plt.ylim(min,max)`             |
   |              | `plt.xlim(min,max)`             |


7. `plt.show()`でグラフを出力.

ポップアップウィンドウでグラフが表示されます.


8. グラフの保存

ポップアップウィンドウの保存ボタンを押すことで画像に名前をつけて保存することが可能です.

![画像の表示](/images/graph_size_setting.png)


:::

例えば,以下のコードで次のグラフが表示されます.
`np.arange(0,100,1)`は,`0`から`100`までの数値を`1`ずつ増える`numpy`の配列を生成しています.
`[x for x in range(0,101,1)]` でも同様の結果となります.

~~~ py
plt.plot(np.arange(0,100,1)
        ,np.arange(0,100,1)
        ,color='red'
        ,label='sample')
plt.title('title')
plt.ylim(0,100)
plt.xlim(0,100)
plt.xticks(np.arange(0,100,10))
plt.yticks(np.arange(0,100,10))
plt.minorticks_on()
plt.grid(which='both')
plt.xlabel('x_label')
plt.ylabel('y_label')
plt.legend()
plt.show()
~~~

![グラフの要素](/images/python_graph_elem.png)


::: warn

画像は`plt.show()`で表示されるポップアップウィンドウを立ち上げなくても,`plt.savefig('figure.png')`と記述することで保存することができます.

ただし,`plt.show()`をして自分でポップアップウィンドウを閉じないと,`plt.`による設定がメモリに保存されたままになります.

`plt.show()`を実行しない場合は必ず最後に`plt.close()`を追加してメモリを開放しましょう.


~~~ py
plt.plot(np.arange(0,100,1)
        ,np.arange(0,100,1)
        ,color='red'
        ,label='sample')
plt.savefig('figure.png')
plt.close()
~~~


:::



### 棒グラフの作成

[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/bar_pie.csv)からデータをダウンロードし,棒グラフを作成してみましょう.

まずはデータを確認します.

~~~ py
# import 部分は省略されています
# dataフォルダにあるbar_pie.csvというファイルを読み込みます.
df = pd.read_csv("data/bar_pie.csv")

# 読み込んだファイルの中身を見てみます.
print(df)
"""
    cat dog bird
0    10  12   20
"""
~~~

プログラムは読み込むデータと,作りたいグラフの形によって変化します.
どのようなデータが必要かを意識して,データを抽出しましょう.

::: note

棒グラフの作成に必要なデータは,

- `棒毎のラベル`

それぞれの棒の名前です.文字列で指定します.
今回は`'cat'`,`'dog'`,`'bird'`とします.

- `棒毎のx軸の位置`

棒グラフの位置は,`0`から始まる整数の列として指定します.
今回は,`0`,`1`,`2`を指定します.

- `棒毎の高さ`になります.

高さに`10`,`12`,`20`を指定します.

:::


~~~ py
# 棒グラフのラベルを抽出します
# df.columns でDataFrameからHeaderを抽出し,
# list()関数でDataFrameからlist型に変換しています
labels = list(df.columns)

# 棒グラフのx軸のどこに置くのかを指定しています.
# np.arange(x) は [0,1,..,x]というarrayを作ります.
x_position = np.arange(len(labels))

#棒グラフの高さを抽出します
# メソッド icloc[x]はDataFrameのx行目を抽出します.
values = list(df.iloc[0])
~~~

抽出した情報を元に,棒グラフを作成します.
`plt.bar(X軸のリスト,Y軸のリスト)`の形で,データを指定します.

~~~ py
# 棒グラフを作成します.
# x 軸 にx_position
# それぞれの棒グラフの高さにvaluesを与えています.
# その他にも沢山の引数があります.
plt.bar(x=x_position, height=values)
~~~

このままだと,X軸のラベルに何も記述されないので,
`plt.xticks(ラベルの位置のリスト,ラベルのリスト)`で,ラベルを指定します.

~~~ py
#ラベルの位置を指定します.
plt.xticks(ticks=x_position,labels=labels)
# Y軸ラベルを指定します
plt.ylabel("number")
# タイトルを指名します
plt.title("kind")
# グラフの表示
plt.show()
~~~

![棒グラフの表示](/images/bar_graph.png)

表示されたグラフは, 保存ボタンで画像として保存できます.

グラフ作成の基本は,これで終わりです.あとは,それぞれのグラフ毎に`plt.bar()`の部分を使い分け,グラフのデザインを変更することで,様々なグラフが作成できます.


グラフのデザインに関する要素は,無数にあるためこの講義ですべてを扱うことはできませんが,いくつかの要素を実際に変更してみましょう.

### 色の変更

matplotlibではグラフの各部に以下の色を指定できます.
これ以外の指定の仕方もあります.色の変え方は,それぞれのグラフで異なります.

![グラフの色](/images/graph_color.png)

棒グラフは`plt.bar(color=棒毎の色のリスト)`の形で棒ごとに色を指定することができます.

~~~ py
# 棒グラフの色を指定します.
# 1つ目 red
# 2つ目 blue
# 3つ目 yellow
color_list = ["red", "blue", "yellow"]
# 引数に色の指定をします
plt.bar(color=color_list, x=x_position, height=values)
#ラベルの位置を指定します.
plt.xticks(ticks=x_position,labels=labels)
# yラベルを指定します
plt.ylabel("number")
# タイトルを指名します
plt.title("kind")
# グラフの表示
plt.show()
~~~

![棒グラフの色の変更](/images/bar_graph_color.png)

### スタイルの変更

毎回細かなデザインを自分で調整すると手間なので,デフォルトで準備されているスタイルを利用すると楽です.

matplotlibではいくつかのデフォルトのスタイルが準備されています. 使用可能なスタイルは,plt.style.available で確認できます.

~~~ sh
❯ python
Python 3.12.3 (main, Jun  3 2024, 08:31:31) [Clang 15.0.0 (clang-1500.3.9.4)] on darwin
Type "help", "copyright", "credits" or "license" for more information.
>>> import matplotlib.pyplot as plt
>>> plt.style.available
['Solarize_Light2', '_classic_test_patch', '_mpl-gallery', '_mpl-gallery-nogrid', 'bmh', 'classic', 'dark_background', 'fast', 'fivethirtyeight', 'ggplot', 'grayscale', 'seaborn-v0_8', 'seaborn-v0_8-bright', 'seaborn-v0_8-colorblind', 'seaborn-v0_8-dark', 'seaborn-v0_8-dark-palette', 'seaborn-v0_8-darkgrid', 'seaborn-v0_8-deep', 'seaborn-v0_8-muted', 'seaborn-v0_8-notebook', 'seaborn-v0_8-paper', 'seaborn-v0_8-pastel', 'seaborn-v0_8-poster', 'seaborn-v0_8-talk', 'seaborn-v0_8-ticks', 'seaborn-v0_8-white', 'seaborn-v0_8-whitegrid', 'tableau-colorblind10']
~~~

それぞれのスタイルのイメージは[こちら](https://matplotlib.org/stable/gallery/style_sheets/style_sheets_reference.html)で確認できます.

スタイルは`plt.style.use('スタイル名')`で指定し,以降のコード全てに適用されます.

~~~ py
# スタイル 'ggplot' を使ってみます
plt.style.use('ggplot')
# 引数に色の指定をします
plt.bar(x=x_position, height=values)
#ラベルの位置を指定します.
plt.xticks(ticks=x_position,labels=labels)
# yラベルを指定します
plt.ylabel("number")
# タイトルを指名します
plt.title("kind")
# グラフの表示
plt.show()
~~~

![スタイルの適用](/images/bar_graph_style.png)

### 要素の追加

グラフに新しい要素を付け加えるのも簡単です.

`plt.show()`までの間に,グラフを宣言することで,複数のグラフを重ねることが可能です.
ここでは, 異なる色の棒グラフを追加しています.

また, `plt.legend()`によって凡例を追加しています.

~~~ py
# スタイル seaborn を使ってみます
plt.style.use('seaborn')
# 棒グラフを2つ並べます
plt.bar(color='red', x=x_position, width=0.3,height=values)
plt.bar(color='blue',x=x_position+0.3, width=0.3,height=[11,15,14])
#凡例を追加します
# loc で位置を指定します
# 上下 upper center lower
# 左右 left center right
plt.legend(['2020','2000'], loc='upper left')
#ラベルの位置を指定します.
plt.xticks(ticks=x_position+0.15,labels=labels)
# yラベルを指定します
plt.ylabel("number")
# タイトルを指名します
plt.title("kind")
# グラフの表示
plt.show()
~~~

![凡例の追加](/images/bar_graph_style2.png)

デザインのすべてのパターンをここで扱うことは出来ないので,
やりたいことに応じて,
[matplotlubの公式ドキュメント](https://matplotlib.org/stable/users/index)を確認しましょう.

## 円グラフ

先ほどと同じデータ`bar_pie.csv`を利用して円グラフを作成してみます.

~~~ py
"""
header1 header2 header3 ...
value1  value2  value3  ...
の形のcsvから棒グラフを作成する
"""

# dataフォルダにあるbar_pie.csvというファイルを読み込みます.
df = pd.read_csv("data/bar_pie.csv")

# 読み込んだファイルの中身を見てみます.
print(df)

# 棒グラフのラベルを抽出します
# df.columns でDataFrameからHeaderを抽出し,
# list()関数でDataFrameからlist型に変換しています
labels = list(df.columns)

# メソッド icloc[x]はDataFrameのx行目を抽出します.
values = list(df.iloc[0])

# 円グラフの作成
plt.pie(values, labels=labels, autopct='%1.1f%%')
# タイトルを指名します
plt.title("kind")
# グラフの表示
plt.show()
~~~

円グラフは, `plt.pie()`にそれぞれの領域の割合と,ラベルを与えることで,作成できます.
グラフに数字を表示するには,`autopct=`に`フォーマット文字列`で表示内容を指定します.
上記の例では`'%1.1f%%'`と書くことで,小数点1桁まで数値を表示しています.

![円グラフ](/images/pie_graph1.png)

::: note

- 演習

[円グラフデータ](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/pie_chart_practice.csv),[折れ線グラフデータ2](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/line_chart_practice.csv),[棒グラフデータ3](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/bar_chart_practice.csv)を利用し,それぞれのグラフを作成してください.
表示が必要だと思われるデザインを設定してください.

:::


## for文を利用したグラフ

これまでのように単純な一つのグラフを作成するだけであれば,恐らくExcelなどのほうが手軽ですが,多数のグラフを作成したり, 複数のデータを組み合わせた複雑なグラフを作成する場合にはプログラミングの方が便利になります.


例えば[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/temperature_10location.csv)のデータを利用して棒グラフを作成することを考えてみましょう.このデータは10箇所の気温が記録された時系列データです.

~~~ sh
          Date  Location_1  Location_2  ...  Location_8  Location_9  Location_10
0   2023-01-01   18.211700   21.553371  ...   18.665620   29.108637    21.634069
1   2023-01-02   15.108630   16.172744  ...   20.135756    9.212193    24.209877
2   2023-01-03   14.938279   24.593193  ...   16.778431   22.291269    19.815977
3   2023-01-04   20.640249   18.862405  ...   13.282269   16.040627    15.993132
4   2023-01-05   14.218562   16.281882  ...   15.729466   24.951213    18.053277
..         ...         ...         ...  ...         ...         ...          ...
95  2023-04-06   21.312739   19.613363  ...   16.497553   20.446656    13.644605
96  2023-04-07   26.388210   31.533854  ...   18.999736   15.603714    19.215097
97  2023-04-08   16.914329   20.479892  ...   21.698464   17.705697    16.867517
98  2023-04-09   14.316258   17.841650  ...   31.885071   20.816917    16.895196
99  2023-04-10   23.595042   19.896247  ...   17.534881   15.180066    15.104460
~~~

このデータの`Location_1`から`Location_10`までの折れ線グラフを一つのグラフに表示することを考えてみます.

`matplotlib`では, `plt.show()`までに要素を重ねることで複数のグラフを重ねることができます.

例えば,10本の折れ線グラフを表示する場合,一つ一つ手書きすると以下のようになります.

~~~ py
df = pd.read_csv('data/temperature_10location.csv')
print(df)

#'Date'列を日付型に変更しています.
df['Date'] = pd.to_datetime(df['Date'])

#一つ一つ手書きする方法
plt.plot(df['Date'],df['Location_1'],label='Location_1')
plt.plot(df['Date'],df['Location_2'],label='Location_2')
plt.plot(df['Date'],df['Location_3'],label='Location_3')
plt.plot(df['Date'],df['Location_4'],label='Location_4')
plt.plot(df['Date'],df['Location_5'],label='Location_5')
plt.plot(df['Date'],df['Location_6'],label='Location_6')
plt.plot(df['Date'],df['Location_7'],label='Location_7')
plt.plot(df['Date'],df['Location_8'],label='Location_8')
plt.plot(df['Date'],df['Location_9'],label='Location_9')
plt.plot(df['Date'],df['Location_10'],label='Location_10')

plt.legend()
plt.xticks(rotation=15) #x軸を15度傾かせています
plt.show()

~~~

![10本の折れ線グラフ](/images/temperature_10location.png)

10本程度であれば,まだ書けなくもありませんが,それでも手間がかかります.こういった繰り返しの作業は`for文`を利用しましょう.

`for文`を利用した場合には以下のようになります.

~~~ py
for x in df.columns[1:]:
    plt.plot(df['Date'],df[x],label=x)
plt.legend()
plt.xticks(rotation=15)
plt.show()
~~~

グラフの内容は同じですが,こちらのほうが労力が少なく,コードもスッキリしており,何か修正を加える場合でも修正箇所が少なくて済みます.
繰り返し作業は積極的に`for文`や`while文`を利用するようにしましょう.

## グラフの分割

先程は一つのグラフ内に複数の折れ線グラフを表示しましたが,個別に表示する場合にはどのようになるでしょうか.
一つの方法として,以下の用に複数のグラフを個別に作成することも可能です.
(先に保存先のディレクトリ `result/multi_plot` を作成しておきましょう.)

~~~ py
for x in df.columns[1:]:
    plt.plot(df['Date'],df[x])
    plt.title(x)
    plt.xticks(rotation=15)
    plt.savefig('result/multi_plot/' + x + '.png')
    plt.close()
~~~

![保存された10個のグラフ](/images/temperature_10location2.png)

しかし,レポートなどに10枚の画像を貼り付けるのは手間がかかりますし,余白など無駄も多いです.

- subplots()

`matplotlib`には1枚の画像を分割して複数のグラフを載せるためのメソッド`.subplots()`があるので,関連するグラフや比較のためのグラフなどはできるだけ1枚の画像に集約しましょう.

`.subplots()`は1枚の画像を`n行`,`n列`に分割し,それぞれの領域にグラフを描画します.

各領域は `axes`などと呼ばれ,画像全体を`figure`などと呼びます.
利用するためには,まず `fig, axes = plt.subplot()`の形で宣言します. 引数として,行数は`nrows=`,列数は`ncols=`にそれぞれ`int`で指定します.

![FigureとAxes](/images/figure_axes.png)

~~~ py

fig, axes = plt.subplot(nrows= 5 #行数の指定
                       ,ncols= 2 #列数の指定
                        )
~~~

宣言のあと,各領域のグラフを `axes[行,列]`の形で指定していきます.行や列は`0`から始まるので注意してください.

~~~ py
axes[0,0].plot(df['Date'],df['Location_1'],label='Location_1')
axes[0,1].plot(df['Date'],df['Location_2'],label='Location_2')
axes[1,0].plot(df['Date'],df['Location_3'],label='Location_3')
axes[1,1].plot(df['Date'],df['Location_4'],label='Location_4')
axes[2,0].plot(df['Date'],df['Location_5'],label='Location_5')
axes[2,1].plot(df['Date'],df['Location_6'],label='Location_6')
axes[3,0].plot(df['Date'],df['Location_7'],label='Location_7')
axes[3,1].plot(df['Date'],df['Location_8'],label='Location_8')
axes[4,0].plot(df['Date'],df['Location_9'],label='Location_9')
axes[4,1].plot(df['Date'],df['Location_10'],label='Location_10')
plt.show()
~~~

以下のようなグラフが作成されます. しかし, 少し見にくいですね.

![subplots](/images/subplot1.png)

`.subplots(sharex=True)`とすると,x軸を共有することができます.今回のグラフはx軸がすべて同じなので,共有してみましょう.
また,それぞれのグラフにタイトルを付けてみます.
更に,一つ一つ手で入力するのは手間なので`for文`を利用してみましょう.

タイトルを付けるには今までの`plt.title()`ではなく`axes[r,c].set_title()`になります. `axes`毎の要素に関しては[公式サイト](https://matplotlib.org/stable/users/explain/axes/axes_intro.html)を参考にしてください.

![axesの要素(https://matplotlib.orgより)](https://matplotlib.org/stable/_images/anatomy.png)


~~~ py
fig, axes = plt.subplots(nrows= 5 #行数の指定
                        ,ncols= 2 #列数の指定
                        ,sharex=True)

count = 0
for i in range(5):
    for j in range(2):
        col = df.columns[1:]
        axes[i,j].plot(df['Date'],df[col[count]])
        axes[i,j].set_title(col[count])
        axes[i,j].tick_params(axis='x', rotation=15)
        count +=1
plt.show()
~~~

![subplots](/images/subplot2.png)

グラフ全体の要素は`fig.`の形で指定します.
タイトルを付ける場合は`fig.suptitle('title')`となります.

~~~ py
fig, axes = plt.subplots(nrows= 5 #行数の指定
                        ,ncols= 2 #列数の指定
                        ,sharex=True)

count = 0
for i in range(5):
    for j in range(2):
        col = df.columns[1:]
        axes[i,j].plot(df['Date'],df[col[count]])
        axes[i,j].set_title(col[count])
        axes[i,j].tick_params(axis='x', rotation=15)
        count +=1
fig.suptitle('subplots title')
plt.show()
~~~

![subplots](/images/subplot3.png)

::: note

- flatten()

`for文`を二重ループで記述するのは大変なので,しばしば`axes.flatten()`を利用して,連番に変換すると便利です.

![flatten](/images/figure_axes_flatten.png)

~~~ py
fig, axes = plt.subplots(nrows= 5 #行数の指定
                        ,ncols= 2 #列数の指定
                        ,sharex=True)

#連番に変換
axes = axes.flatten()
for i in range(10):
    col = df.columns[1:][i] #countをiで共通化
    axes[i].plot(df['Date'],df[col])
    axes[i].set_title(col)
    axes[i].tick_params(axis='x', rotation=15)
fig.suptitle('subplots title')
plt.show()
~~~

:::


## 度数分布表とヒストグラム

データを手に入れたら最初にデータを可視化してデータの特徴を掴む必要があります. データの特徴として重要なものに,データの**分布**があります.

::: warn

- 分布の意味
---

**分布**という語の詳細な意味に関しては,後ほど検定や回帰の章でも簡単に扱いますが,統計学入門において数理的に詳しく扱っています.

統計学入門を履修していない人はここでは,単に**データの散らばり具合**という意味として捉えておきましょう.

:::

分布を可視化する手法として代表的なものに,**度数分布表**と,**ヒストグラム**があります. 1次元のデータの可視化において,度数分布表とヒストグラムは,**データ分析のファーストステップ**とも称される,重要な手法です. データを手に入れたらまずは度数分布表と,ヒストグラムを作成してみましょう.


### 度数分布表

::: note

**度数分布表**とは,データの数を区切られた範囲ごとに数え上げた表のことです. 質的データの場合は,データのカテゴリー毎に,量的データの場合は分析者が定めた区間毎にデータがいくつかるのかを数え上げます.

普通ヒストグラムというと,量的データを対象としたものをいいますが,ここではわかりやすさのために,質的データから見ていきましょう.

以下の表は,何かしらの商品の美味しさに関するアンケート結果です. 商品を食べて,｢とても美味しい｣と回答した人の人数が9人,｢不味い｣と回答した人の人数が5人であることなどがわかります.

このように**データのカテゴリー別にその値が生じたケースの数**を**度数**といい,**度数を数え上げ表に整理したもの**を**度数分布表**といいます.

|アンケート区分            |度数   |
|  :---:                   | :---: |
|とても美味しい            |  9    |
|どちらかといえば美味しい  |  11   |
|普通                      |  34   |
|どちらかといえば不味い    |  5    |
|不味い                    |  5    |

度数の亜種には,以下のようなものがあり,それぞれによってデータの分布が把握できます.

- 累積度数:

    順序尺度データの度数を少ない方から足し上げた値

- 相対度数(構成比率):

    度数の総和を100%としたときの構成比率

- 累積相対度数(累積比率):

    相対度数を上から足し上げた値

|アンケート区分            |度数   | 累積度数 | 相対度数 | 累積相対度数|
|  :---:                   | :---: | :---:    | :---:    | :---:       |
|とても美味しい            |  9    | 9        | 14%      | 14%         |
|どちらかといえば美味しい  |  11   | 20       | 17%      | 31%         |
|普通                      |  34   | 54       | 53%      | 84%         |
|どちらかといえば不味い    |  5    | 59       | 8%       | 92%         |
|不味い                    |  5    | 64       | 8%       | 100%        |
|計                        |  64   |          | 100%     |             |

度数分布表を作成することで,データがどの分類に偏っているのかなど,データの分布の傾向がつかめます. 度数分布表を見る場合には,相対度数や累積度数から**データが一番多いカテゴリー**,**度数データの過半数が属するカテゴリー**,**データのほとんど(90%程度)が属するカテゴリー**などに注目してみましょう.
上のデータでは,最も回答が多いのは｢普通｣であること, ｢とても美味しい｣｢どちらかといえば美味しい｣の回答が,31%であるのに対して,｢どちらかといえば不味い｣｢不味い｣の回答が18%であり,全体的にこの商品の味は高評価側に集中していることなどがわかります.

:::

Pythonで度数分布表を作成するにはどのようにしたら良いのでしょうか. 質的データの場合は,ただそれぞれの値を数えればいいので,`for文`などを利用することも可能ですが,`pandas`の`value_counts()`メソッドを利用することで簡単に作成できます.

[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/diff_class.csv)のとある授業の難易度に関する質的データをダウンロードして度数分布表を作成してみましょう.

データは以下のように,｢難しすぎてついていけない｣,｢難しいが許容できる｣,｢ちょうどよい｣,｢簡単だが許容できる｣｢簡単すぎて退屈｣の5段階のカテゴリーが記述されています.

~~~ sh
         Diff
0   難しいが許容できる
1   簡単すぎて退屈
2   難しいが許容できる
3   簡単だが許容できる
4      ちょうどよい
..        ...
~~~


~~~ py
import pandas as pd
import numpy as np
from matplotlib import pyplot as plt
import japanize_matplotlib
import math
import module.sturges as st


#データの読み込み
df = pd.read_csv('Data/diff.csv')
print(df)

#ヒストグラムを作りたいデータの列名を指定
target_column = 'Diff'

#度数
freq = df[target_column].value_counts(sort=False)
#順番の入れ替え
freq = freq.reindex(['難しすぎてついていけない'
                    ,'難しいが許容できる'
                    ,'ちょうどよい'
                    ,'簡単だが許容できる'
                    ,'簡単すぎて退屈'],axis='index')
print('度数',freq)

rel_freq = freq / df[target_column].count()  # 相対度数
cum_freq = freq.cumsum()  # 累積度数
rel_cum_freq = rel_freq.cumsum()  # 相対累積度数

dist = pd.DataFrame(
    {   "Value": freq.index,        #階級値
        "Freq": freq,               #度数
        "Rel": rel_freq,            #相対度数
        "Cum": cum_freq,            #累積度数
        "RelCum": rel_cum_freq,     #相対累積度数
    },
    index=freq.index
)

print(dist)
dist.to_csv('frequency_table.csv',)

~~~

保存されたcsv(`frequency_table.csv`)を確認してみると,度数分布表が作成されていることが確認できます.


|Value                       |Freq    |Rel         |Cum   |RelCum      |
| :---:                      |:---:   |:---:       |:---: |:---:       |
|難しすぎてついていけない    |2       |0.02247191  |2     |0.02247191  |
|難しいが許容できる          |44      |0.494382022 |46    |0.516853933 |
|ちょうどよい                |38      |0.426966292 |84    |0.943820225 |
|簡単だが許容できる          |4       |0.04494382  |88    |0.988764045 |
|簡単すぎて退屈              |1       |0.011235955 |89    |1           |


質的データはあらかじめデータのカテゴリーが定められているのでその数を数えるだけで,度数分布表が作成できました.
それでは,カテゴリーが存在しない量的データではどのようにして度数分布表を作成するのでしょうか.

::: note

- **量的データと度数分布表**

量的データには当てはまるデータを数えるためのカテゴリーが存在しません. 年収のデータを考えた場合,年収1万円ごとに表を作ると1から数億まで,非常に細かくなります.
そこで,量的データで度数分布表を作成するには,100万円ごと,300万円ごとなどデータをいくつかの区間に分けて,区間別の度数を数える必要があります. ここで, 作られた区間を**階級**,階級の幅を**階級幅**,階級を代表する値を**階級値**といいます. 階級値は大抵の場合,区間の中間の値が用いられます(100万円~200万円の区間だとすると,150万円など).

以下の度数分布表は,ある情報クラスの成績(0-100点)を10点毎に区分したものです. それぞれの点数に当てはまる人数を数えて度数とします.

![度数分布表の例](/images/histogram1.png)


量的データの階級数や階級幅は,100万円ごとなどある程度人間の判断によって作成しても構いませんが,数理的に決定する方法もあります.

階級幅は階級数が決まることで自動的に決まるため,階級数を決める一般的な目安として,**スタージェスの公式**が利用されます.

観測数を$n$,階級数を$k$とすると.

$$ k = 1 + log_2 n = \frac{log_{10} n}{log_{10} 2}$$

程度の$k$が望ましいとされています.

計算結果はおおよそ以下のようになります.

![スタージェス数](/images/sturges_number.png)
:::

Pythonではスタージェス数は対数計算をするためのモジュール`math`を利用して以下のように求めることができます.

~~~ py
# 2進対数を返す math.log2() 関数などを利用するために必要なモジュール
import math

# 関数名を sturgesNumber として引数をnとします
def sturgesNumber(n):
    # 公式の通り k = 1 + log2 n
    # 階級数は整数が良いので,math.floor()で小数点以下を切り捨てます
    return (math.floor (1 + math.log2(n)))

print(sturges(2048)) #>>> 12
~~~

階級数が決まることで階級幅が
$$\frac{(データの最大値 - データの最小値)}{階級数}$$

として決まります.

それでは,[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/histogram_quantitative.csv)のデータを利用して,度数分布表を作成してみます. 量的データの度数分布表を作成するには,`value-counts()`の引数`bins=`に各階級の終点を表すリストを指定します.

~~~ py
#ヒストグラムを作りたいデータの列名を指定
target_column = 'Values'

# 関数名を sturgesNumber として引数をnとします
def sturgesNumber(n):
    # 公式の通り k = 1 + log2 n
    # 階級数は整数が良いので,math.floor()で小数点以下を切り捨てます
    return (math.floor (1 + math.log2(n)))

# dataフォルダにあるhistogram_quantitative.csvというファイルを読み込みます.
df = pd.read_csv("data/histogram_quantitative.csv")
# 読み込んだファイルの中身を見てみます.
print(df)

#階級数を決定
stnum = sturgesNumber(len(df[target_column]))
print('sturges number:',stnum)

#階級幅を決定
space = int((df[target_column].max() - df[target_column].min()) / stnum)
print('space:',space)

#階級幅の終端を指定
bins = np.arange(start = int(df[target_column].min()) - 1 #最小値
                ,stop  = int(df[target_column].max()) + 1 #最大値
                ,step  = space #階級幅
                )

print('bins:',bins)

#度数
freq = df[target_column].value_counts(bins =bins, sort=False)
print('度数',freq)

rel_freq = freq / df[target_column].count()  # 相対度数
cum_freq = freq.cumsum()  # 累積度数
rel_cum_freq = rel_freq.cumsum()  # 相対累積度数

dist = pd.DataFrame(
    {   "Value": freq.index,        #階級値
        "Freq": freq,               #度数
        "Rel": rel_freq,            #相対度数
        "Cum": cum_freq,            #累積度数
        "RelCum": rel_cum_freq,     #相対累積度数
    },
    index=freq.index
)

print(dist)

dist.to_csv('frequency_table_qualitative.csv'
           ,encoding='utf-8-sig'
           ,index=False)
~~~

結果作成された,`frequency_table_qualitative.csv`を開いてみると,以下のような度数分布表が作成されたことが確認できます.

|Value           |Freq    |Rel     |Cum   |RelCum|
|:---:           |:---:   |:---:   |:---: |:---: |
|(8.999, 18.0]   |1       |0.01    |1     |0.01  |
|(18.0, 27.0]    |5       |0.05    |6     |0.06  |
|(27.0, 36.0]    |11      |0.11    |17    |0.17  |
|(36.0, 45.0]    |21      |0.21    |38    |0.38  |
|(45.0, 54.0]    |27      |0.27    |65    |0.65  |
|(54.0, 63.0]    |20      |0.2     |85    |0.85  |
|(63.0, 72.0]    |9       |0.09    |94    |0.94  |

### 度数分布表の可視化:**ヒストグラム**

ここまでで,データの分布,偏りを把握する手段としての度数分布表を学びました. 度数分布表を眺めることである程度データの形は分かりますが,よりわかりやすく可視化する方法として**ヒストグラム**があります.

**ヒストグラム(Histogram)/柱状図**とは,量的データの度数分布を棒グラフで表現したものです.ただし,通常の棒グラフと異なり値が連続しているので,棒と棒の間にスペースを置きません. 統計における最も基本的なグラフです.

ヒストグラムはいくつかの代表的なパターンがあり,それぞれ注意するべきポイントがありますので,順に見ていきましょう.

::: note

- 単峰型(unimodal)で左右対称
---

ヒストグラムの盛り上がっている部分を**峰**といいます. 峰の数が一つのヒストグラムを**単峰(unimodal)**なヒストグラムといいます.

ヒストグラムの基本となる形は,単峰で左右対称な分布です. この分布は,**データが同質な集団から発生していること**を表しています.

    - 例:同じ人種の,同じ年代の,男性の集団の身長

    - 例:同じ種類,同じ時期,同じ地域のうさぎの集団のサイズ

峰からの左右のデータのばらつきは集団の個体差を表しています. 異なる,質を持つ集団が混じっている場合は峰が複数になることが多いです.

![単峰で左右対称なヒストグラム](/images/histogram_unimodal.png)

- 多峰型(bimodal)なヒストグラム
---

峰が2つ以上あるヒストグラムを**多峰(bimodal)**なヒストグラムといいます.
異質な集団が混ざっているデータでは,ヒストグラムが多峰になることがあります.

    - 例:男女の混ざった集団の身長や体重
    男性の峰と女性の峰が現れます.

    - 例:小学生と中学生に同じテストを受けた点数
    小学生の点数の峰と,中学生の点数の峰が現れます.

ヒストグラムを作成し,多峰性が現れたらデータを集団別・要因別に分割して分析するのが良いとされています.データを特定の属性で分割することを**層別**といいます.

![多峰なヒスグラム](/images/histogram_bimodal.png)

- 左右非対称なヒストグラム
---

ヒストグラムは峰を中心として左右対称な場合もありますが,どちらかの方向に歪んでいるものも良く見られます.

    - 例:社会人の年収,企業の売上など

ヒストグラムの細くなっている部分をヒストグラムの**尾**といいます.

    - **尾が**左に伸びている場合に **左に歪んだ分布**

    - **尾が**右に伸びている場合に **右に歪んだ分布**

といいます.

![左右に歪んだヒストグラム](/images/histogram_right_left.png)

左右に歪んだ分布では,後に扱う**中心を表す代表値**(平均値や中央値)が適切に集団を代表しない場合があるので,代表値の使い分けが必要になります.

また, 後に扱う**検定**手法のうち,**正規分布**を仮定する検定が利用できないなど,手法の選択において,分布の歪みは重要なポイントです.

- **外れ値(Outlier)**のあるヒストグラム
---

大多数のデータとは離れた位置にある少数のデータを**外れ値(Outlier)**といいます.

外れ値は,データ分析において非常に重要な意味を持ち,外れ値が現れた場合にはその原因を探ることが必要となります.
外れ値が発生する原因としては以下のようなものがあり,いずれも注目する必要があります.

    - データの取得におけるミス
        単純な入力ミスや計算ミスなど
        発見した場合は修正,除外する必要がある.

    - 異質な存在の発見
        新種や新しい現象の発見などにつながる可能性があります.

また,代表値の計算においては歪みが生じる可能性があるので,外れ値を除外する必要があります.

![外れ値のあるヒストグラム](/images/histogram_outlier.png)

:::

それでは,Pythonでヒストグラムを作成してみましょう. 度数分布表を作成している場合は,棒グラフを作成し,棒の幅を`0`にすることで,ヒストグラムが作成できます.

`plt.bar()`では,引数`width=1`を与えることで,棒の太さを`1`(棒の間を0)にすることができます.

まずは,以前扱った[質的データ](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/diff_class.csv)の例におけるヒストグラムを作成してみましょう.

~~~ py
#データの読み込み
df = pd.read_csv('Data/diff_class.csv')
print(df)

#ヒストグラムを作りたいデータの列名を指定
target_column = 'Diff'

#度数
freq = df[target_column].value_counts(sort=False)
#順番の入れ替え
freq = freq.reindex(['難しすぎてついていけない'
                    ,'難しいが許容できる'
                    ,'ちょうどよい'
                    ,'簡単だが許容できる'
                    ,'簡単すぎて退屈'],axis='index')
print('度数',freq)

rel_freq = freq / df[target_column].count()  # 相対度数
cum_freq = freq.cumsum()  # 累積度数
rel_cum_freq = rel_freq.cumsum()  # 相対累積度数

dist = pd.DataFrame(
    {   "Value": freq.index,        #階級値
        "Freq": freq,               #度数
        "Rel": rel_freq,            #相対度数
        "Cum": cum_freq,            #累積度数
        "RelCum": rel_cum_freq,     #相対累積度数
    },
    index=freq.index
)

print(dist)
dist.to_csv('frequency_table.csv',encoding='utf-8-sig',index=False)

# ヒストグラム
plt.bar(x=dist['Value'], height=dist["Freq"],width=1)
plt.xticks(np.arange(len(dist)),list(dist.index),rotation=15)
plt.show()
~~~

以下のように,単峰で右に歪んだグラフが作成されるはずです.

![質的データのヒストグラム](/images/histogram_qualitative.png)

同様に[量的データ](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/histogram_quantitative.csv)についても作成してみます.

~~~py
#ヒストグラムを作りたいデータの列名を指定
target_column = 'Values'

# 関数名を sturgesNumber として引数をnとします
def sturgesNumber(n):
    # 公式の通り k = 1 + log2 n
    # 階級数は整数が良いので,math.floor()で小数点以下を切り捨てます
    return (math.floor (1 + math.log2(n)))

# dataフォルダにあるhistogram_quantitative.csvというファイルを読み込みます.
df = pd.read_csv("data/histogram_quantitative.csv")
# 読み込んだファイルの中身を見てみます.
print(df)

#階級数を決定
stnum = sturgesNumber(len(df[target_column]))
print('sturges number:',stnum)

#階級幅を決定
space = int((df[target_column].max() - df[target_column].min()) / stnum)
print('space:',space)

#階級幅の終端を指定
bins = np.arange(start = int(df[target_column].min()) - 1 #最小値
                ,stop  = int(df[target_column].max()) + 1 #最大値
                ,step  = space #階級幅
                )

print('bins:',bins)

"""
階級が整数では大きすぎる場合は,
np.arrange()では上手く出来ません.
while文などで,以下のように自作しましょう.

space = (data.max() - data.min()) / step
current = data.min() -1
bins = [current]
while current <= data.max() +1:
    current += space
    bins.append(current)
"""


#度数
freq = df[target_column].value_counts(bins =bins, sort=False)
print('度数',freq)

rel_freq = freq / df[target_column].count()  # 相対度数
cum_freq = freq.cumsum()  # 累積度数
rel_cum_freq = rel_freq.cumsum()  # 相対累積度数

dist = pd.DataFrame(
    {   "Value": freq.index,        #階級値
        "Freq": freq,               #度数
        "Rel": rel_freq,            #相対度数
        "Cum": cum_freq,            #累積度数
        "RelCum": rel_cum_freq,     #相対累積度数
    },
    index=freq.index
)

print(dist)
dist.to_csv('frequency_table_qualitative.csv'
           ,encoding='utf-8-sig',index=False)

# ヒストグラム
## dist['Value']は文字列でないので,
## .astype(str) で文字列に変換しています.
plt.bar(x=dist['Value'].astype(str)
       ,height=dist["Freq"],width=1)
plt.xticks(np.arange(len(dist)),list(dist.index),rotation=15)
plt.show()
~~~

以下のように単峰で左に歪んだヒストグラムが作成されます.

![量的データのヒストグラム](/images/histogram_quantitative.png)

一方で,量的データに関しては,度数分布表を作成せずとも`matplotlib`の`plt.hist()`を利用して直接作成することも可能です.

`plt.hist()`では`bins=`に階級数を指定することで,その階級数で自然に分割したヒストグラムを作成してくれます.


~~~ py
# ヒストグラムの作成
label  = df.columns[0]
values = df[label]
plt.hist(values, bins=stnum)
plt.show()
~~~

![量的データのヒストグラム](/images/histogram_quantitative2.png)

階級幅の設定によって,見た目が変わることが分かります. 作成手法や階級の設定は目的に応じて,使い分けるようにしましょう.

::: note

- 演習

[データ1](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/qualitative_histogram_practice.csv),[データ2](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/quantitative_histogram_practice.csv)の度数分布表とヒストグラムを作成し,pptなどでグラフとその解釈をまとめてください.

:::


## 箱ひげ図

データの観測対象が複数のグループに層別可能な場合には, それぞれのヒストグラムを作成して比較することなどが必要です. グループの数が多い場合には, 何個もヒストグラムを作成することになりますし,比較には剥いていない場合があります.
そのような複数のグループの分布を比較する際に良く用いられるグラフが,箱ひげ図です.

[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/boxplot_data_300.csv)のデータは, 複数の種類の馬鈴薯のサイズがまとまっています.

試しに,ヒストグラム馬鈴薯の種類毎のヒストグラムを作成してみます.

~~~ py
df = pd.read_csv('data/boxplot_data_300.csv')

#馬鈴薯の種類を抜き出す
types = list(set(df['Type']))
print(types)

#馬鈴薯の種類別にヒストグラムを作成してみる
fig, ax = plt.subplots(ncols=2
                      ,nrows=int(len(types) / 2) + 1
                      ,sharex = True)
axes = ax.flatten()
fig.suptitle('Potato Weight')

for i in range(len(types)):
    t = types[i]
    df_temp = df[df['Type'] == t]
    axes[i].hist(df_temp['Weight'])
    axes[i].set_title(t)

plt.show()
~~~

![馬鈴薯の重さのヒストグラム](/images/box_plot1.png)

これでも比較はできますが1枚の画像にまとめる手段として,箱ひげ図が利用できます.
箱ひげ図は`plt.boxplot(データのリスト,labels=ラベルのリスト)`で生成できます.

~~~ py
data = [df[df['Type']==x]['Weight'] for x in types]
"""
#内包表記を利用しない場合は
data = []
for x in types:
    data.append(df[df['Type']==x]['Weight'])
"""
print(data)
plt.boxplot(data,labels=types)
plt.show()
~~~

![馬鈴薯の重さの箱ひげ図](/images/box_plot2.png)

箱ひげ図は,線がそれぞれ四分位数を表しており,それぞれ上から**最大値**,**75%点**,**中央値**,**25%点**,**最小値**となります.また,外れ値は丸で表されます.

ヒストグラムと比較して情報量は減りますが, 一覧性と比較においては優れています.それぞれ一長一短なので,用途に応じて使い分けるようにしましょう.


## 発展:密度プロット

データの分布を表現する手法としてヒストグラムは非常に便利ですが,階級数や階級幅を自分で定める必要があり,その設定によって見た目が変わってしまいます. また, データ数が少ないときには正確なデータの分布をつかめないという問題点もあります.

そこで, データを階級で区分せずに,度数ではなく確率分布を直接推定する手法に**カーネル密度推定(Karnel Density Estimation)**があります.

## 発展:sinaplot

## 発展:バイオリンプロット

## 散布図

これまではデータの各観測項目を独立に可視化してきました. 複数の観測項目の関係性を可視化する代表的な手法に散布図があります.

[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/scatter.csv)のデータはGoogle Trendにおける同時期の`AI`というワードのの検索量と,`Python`というワードの検索量を表しています.

~~~ sh
     AI  Python
0    34      27
1    40      26
2    59      28
3    46      29
4    36      29
..   ..     ...
255  58      83
256  69      87
257  59      82
258  62      84
259  59      87
~~~

この2つの検索量がどのような関係にあるのかを散布図を用いて確認してみましょう.
散布図は`plt.scatter(x軸の値のリスト,y軸の値のリスト)`の形で作成できます.

~~~ py
df = pd.read_csv('data/scatter.csv')
#散布図のx軸を指定
x_column = 'AI'
#散布図のy軸を指定
y_column = 'Python'

x_value = df[x_column]
y_value = df[y_column]
plt.scatter(x_value, y_value)
plt.ylabel(y_column)
plt.xlabel(x_column)
plt.show()
~~~

![散布図](/images/scatter.png)

散布図を見ると,`AI`に関する検索量が増えるにつれて,`Python`の検索量が増えるという関係が見えてきます.

この関係の度合いを数値化する**相関係数**や,関係の仕方を説明する**回帰分析**に関しては後ほど扱います.

### 観測項目が複数ある場合の散布図

データが3つある場合には,以下のように3Dで表現することも可能ですがこの講義では,3次元のグラフに関しては深く扱いません.興味のある方は調べてみましょう.

~~~ py
x = np.random.rand(100)
y = np.random.rand(100)
z = np.random.rand(100)

fig = plt.figure()
ax = fig.add_subplot(projection='3d')
ax.scatter(x, y, z, color='blue')
plt.show()
plt.close()
~~~

![3次元の散布図](/images/3dscatter.png)


観測項目が4つ以上ある場合に散布図のように関係を表す手法としては,複数の観測項目の値を合成して3次元以下にする**次元削減**が良く利用されます. **次元削減**に関しては後ほど扱うとして,ここではいくつかの観測項目を**色**や**大きさ**などの要素の変換して関係を見る方法を紹介します.

`plt.scatter()`では`s=`に数値を与えることでサイズ,`c=`に数値を与えることで色を指定できます.
また,色と数値の関係は`plt.colorbar()`で表示可能です.

複数の点が重なると見にくくなるために `alpha=`に`0-1`の間の数値を指定して透明度を指定することができます.

~~~ py
x = np.random.rand(100)
y = np.random.rand(100)
z = np.random.rand(100)

#色とサイズをzで指定する
#そのままだとサイズが小さすぎるので,100倍している
plt.scatter(x,y,alpha=0.5,s=z*100,c=z)
plt.colorbar() #カラーバーの表示
plt.show()
~~~

![色とサイズによる表現](/images/scatter_color_size.png)

### クラスタリングにおける散布図

散布図は複数の観測項目間の関係性を可視化するための手法ですが,データから特定の観測対象の集まり(**クラスター**)を発見する**クラスタリング**とも深い関わりがあります.
クラスタリングの手法は後ほど扱いますが,ここでは可視化手法としての散布図とクラスタ表現の関係に関して確認してみましょう.

[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/scatter_class.csv)のデータはクラスタリングによって得られたクラスタ毎のラベルがなされています.

~~~ sh
           x          y  Cluster
0  -8.149818  -9.152380        2
1   5.860155   0.126873        1
2  -3.213409   9.828126        0
3   6.744070  -0.129607        1
4  -6.342946  -6.038933        2
..       ...        ...      ...
95 -3.524581   9.931801        0
96 -0.962698  10.411206        0
97  4.397950   2.579246        1
98 -5.255050  -5.299407        2
99 -3.768024   8.550468        0
~~~

クラスタを表現は`.scatter(color=)`で色を指定することが一般的です.

~~~ py
#散布図をクラスタごとに色を変えて表示
for i in range(3):
    df_temp= df[df['Cluster'] == i]
    plt.scatter(df_temp['x']
               ,df_temp['y']
               ,c=color_list[i]
               ,label='Cluster:' + str(i))
plt.legend()
plt.show()

~~~

![クラスタリング](/images/scatter_class.png)


## 同時度数分布表

2つの観測項目の関係を調べる手法として散布図を学びましたが,散布図は量的データにしか使えません. 質的変数同士の関係性を調べるにはどのようにしたらいいのでしょうか.

質的変数同士の関係性を調べる手法として代表的なものに**同時度数分布表(クロス表)**があります.
例えば[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/cross_table_data.csv)のデータはある講義の成績情報ですが,観測項目として成績以外に1時限から5時限までの時限が記録されています. 時限が早い講義と遅い講義で成績が変わるのかという関係性を調べてみます.

~~~ sh
     Period Grade
0         2     B
1         5     A
2         4     A
3         3     C
4         1     C
..      ...   ...
195       2     C
196       4     C
197       5     C
198       5     F
199       4     C
~~~

試しに,散布図で`Period`と`Grade`の関係を表してみましょう.
成績をそのままでは,散布図の軸上に配置できないので数値に変換します.

~~~ py
df = pd.read_csv('data/cross_table_data.csv')
print(df)

#あえて散布図を作ってみる
grade_num_map = {'S':1
                ,'A':2
                ,'B':3
                ,'C':4
                ,'F':5}

df['Grade_num'] = df['Grade'].map(lambda x: grade_num_map[x])

plt.scatter(df['Period'],df['Grade'])
plt.xlabel('Period')
plt.ylabel('Grade')
plt.show()
~~~

![質的データの散布図](/images/cross_table1.png)

質的データを数値に変換したとしても,離散値となるため,散布図はこのように基本的にはすべての交点に点があるだけのなんの情報も得られないグラフとなります.

散布図としてプロットすると, 点がありうる場所が少なすぎるため情報がとれません. 元々知りたいことは,講義の時限と成績にどのような関係があるのかということでした.
そこで, 講義の時間ごとの成績の偏りが分かるように可視化することを考えてみます.

講義の時限ごとの成績の分布がわかり,それぞれに違いがあれば時限によって成績に偏りが出ていると言えそうです.
そこで,以下のように講義の時限毎の成績の度数分布表を作ってみましょう.

![同時度数分布表](/images/cross_table2.png)

この度数分布表では,時限毎にその成績を取った学生の度数が数えられています($n_{11}$は1時限にSを取った学生の度数,$n_{ij}$は`j時限`に上から`i番目`の成績をとった学生の度数.)

このような2観測項目の度数分布表を**同時度数分布表**あるいは,**クロス表(cross table)**といいます.

それでは,Pythonで同時度数分布表を作成してみましょう.
`pandas` では,`.crosstab(行,列)`メソッドを利用することで,クロス表が作成できます.

~~~ py
#クロス表の作成
cross = pd.crosstab(df['Grade'],df['Period'])

#表示順の設定
cross = cross.reindex([1,2,3,4,5],axis='columns')
cross = cross.reindex(['S','A','B','C','F'],axis='index')
print(cross)
~~~

~~~ sh
Period   1   2   3   4   5
Grade
S        9  10  12   4   2
A        7  14   7   5   4
B        9  10   7  11   7
C       14   6   3  15  18
F        7   4   3   1  11
~~~

このようにしてみることで,それぞれの時限毎にそれぞれの成績がどのような分布なのかが分かります.
しかし,各時限の人数が同じとは限らないため,各列の値をその列の和で割って,列相対度数に変更してみましょう.

~~~ py
#列相対度数に変更する
for c in cross.columns:
    cross[c] = cross[c] / cross[c].sum()

print(cross)
~~~

~~~ sh
Period         1         2        3         4         5
Grade
S       0.195652  0.227273  0.37500  0.111111  0.047619
A       0.152174  0.318182  0.21875  0.138889  0.095238
B       0.195652  0.227273  0.21875  0.305556  0.166667
C       0.304348  0.136364  0.09375  0.416667  0.428571
F       0.152174  0.090909  0.09375  0.027778  0.261905
~~~

このようにすると,時限毎にどの程度の割合がSやAなどの良い成績をとっているのかが分かります.
通常度数分布表を作成したあとには,**χ二乗検定**や,**標準化残渣**を利用した**残渣分析**によって,**偏り**が統計的に存在するかを判定します. しかし,それらは後の検定の章に譲るとして,次の節では更に,これを一目で判断しやすいように可視化することを考えてみます.

## ヒートマップ

一つ前の節では,同時度数分布表を利用して2つの質的変数からなる観測項目の関係性を見てみました. しかし, 同時度数分布表のままでは,可視化とは言えません. 同時度数分布表のような表形式の数値を可視化する方法として,ヒートマップがあります.

ヒートマップとは,表形式の数値を各セルの色によって表現する可視化手法です.

先ほど作成した,列相対度数をヒートマップを利用して可視化してみましょう.

ヒートマップは`seaborn`の`.heatmap()`を利用することで簡単に作成できます.

~~~ py
sns.heatmap( cross  #ヒートマップを作成したいテーブル
           , cmap=plt.get_cmap('Reds') #カラーマップ(省略可)
           , linewidths=.5 #線の太さを指定することでセルを囲う線を表示
           , annot=True  #セルに数値を表示
           )
plt.show()
~~~

![ヒートマップ](/images/heatmap.png)

このヒートマップでは,数値が大きいほど,色が濃くなっており2,3時限においてB以上の成績を取る学生の割合が大きいこと,4,5時限においてCやFなどの成績を取る人の割合が大きいことが視覚的に分かります.

ヒートマップは複数の数値間の相関係数や距離を可視化する際にも良く用いられるので,覚えておきましょう.

::: note

- 演習

1. GoogleTrendで4つのワードに関して同じ期間の推移を調べ以下の2通りの方法でCSVを作成してください.

    - 1つのグラフに表示
        for文を利用して1つのグラフに4つの折れ線グラフを色を変えて表示する.
        凡例も表示する.

    - グラフの分割
        グラフを分割して,それぞれのワードに関して4象限の折れ線グラフを作成する.


2. [こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/beetle_datal.csv)のカブトムシの種類別の体長と体重のデータを利用して散布図を作成してください.カブトムシの種類別に散布図の色や点の図形を変更してください.

3. [こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/forest_beetle_data.csv)の森の地点別に採取できたカブトムシの種類を記録したデータを可視化しどの森でどのカブトムシが取れやすいのかを分析してください.

:::


</details>

# データの数値化 (執筆中)

<details>
    <summary> 開く/閉じる </summary>

データを可視化することで,データの大まかな傾向はつかめます. しかし,グラフではデータの特徴を大まかにしか捉えることが出来ません.
実際に,データの特徴に関して言及するためにはそれを数値にする必要があります.

例えば,以下のヒストグラム([データ](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/histogram_A_B_data.csv))を見てみましょう.

![ヒストグラムの比較](/images/histogram_compare.png)

ヒストグラムAと比較して,Bはデータの**中心が右**にあり,データの**散らばりが大きい**ように見えます. しかし,それらが具体的にどの程度右にあり,どの程度散らばりが大きいのでしょうか.
このように,グラフによる比較では,抽象的な印象しか語ることができないため,データの中心や,散らばりを数値で表すことが必要です.

数値化の対象となる量は,データの種類や分析の目的によって様々ですが,大まかに以下のような分類が可能です.

::: note

- データの数値化
---

|データの数| データの種類 | 求める数値| 目的               |
| :---:    | :---:        | :---:     | :---:              |
| 1        | 量的変数     | 基本統計量              |データの特徴を知る  |
| 2        | 量的変数     | ピアソンの積率相関係数  | データの関係を知る |
| 2        | 質的変数     | ピアソンのΧ二乗統計量 <br> スピアマンの順位相関係数| データの関係を知る |
| 2        | 混在         | 寄与率,相関比           | データの関係を知る |
| 3以上    |              | 次元削減,多変量解析など多数| |

:::

## 基本統計量

量的データを客観的に評価するために,分布の特徴を数値で表したものを**基本統計量(代表値)**といいます.

::: note

- 基本統計量

|名称 | 概要 |
|:---:|:---|
| 平均 (Mean)            | データの平均値.量的データの分布の中心傾向を示す        |
| 中央値 (Median) | データの順位における中央.量的データの分布の中心傾向を示す|
| 最頻値 (Mode)  | 最も度数の多い値.量的データの分布の中心傾向を示す      |
| 標準偏差 (Standard Deviation) | データのばらつき具合を示す.                          |
| 分散 (Variance)     | データのばらつき具合を示す.                          |
| 尖度 (Kurtosis)     | 外れ値の度合い                                       |
| 歪度 (Skewness)     | 分布の歪み

:::

Pythonでは, pandasのDataFrameに対して, `.describe()`メソッドを適用すると, データ数(`count`),平均値(`mean`),中央値(`50%`),四分位数(`24%,75%`),標準偏差(`std`),最大値(`max`),最小値(`min`)などが求まります.

~~~ py
df = pd.read_csv('data/histogram_A_B_data.csv')
print(df.describe())
"""
python quantify.py
       Histogram_A  Histogram_B
count  1000.000000  1000.000000
mean     60.289981    50.708362
std      14.688239     9.974544
min      11.380990    20.596114
25%      50.286145    43.937583
50%      60.379509    50.630771
75%      69.719158    57.288822
max     117.790972    81.931076
"""
~~~

それぞれの統計量の意味を順番に見ていきましょう.

## 中心を表す基本統計量

ヒストグラムにおける峰のある位置,分布の中心がどこにあるかを表す統計量には,**平均値**,**中央値**,**最頻値**などがあります. この3つは,いずれも分布の中心を表す統計量ですが,分布の歪みによって意味が異なり,使い分けが必要となります.

-  **算術平均 (mean)**
---
分布の中心を表す統計量としてもっとも一般的なものに,平均値があります. しかし, 一言に平均といっても,いくつかの種類があるので注意しましょう. 分布の中心を表す場合に用いられる平均は基本的に算術平均ですが, ここでは異なる定義の平均として, **幾何平均**と**調和平均**も紹介します.

一般に「平均」といった時にイメージされる,すべてのデータの和をデータの個数で割った値を**算術平均**といいます.

算術平均は対象とするデータを足し合わせることによって基準となる値が算出される場合に使用します.

::: note
n個の観測値 $ x_1, x_2, ..., x_n $ の時,平均値 $ \bar{x} $ は

$$ \bar{x} = \frac{1}{n} (x_1 + ... + x_n) = \frac{1}{n} \sum_{i=1}^{n} x_i $$
:::


例： 165,171,189の算術平均は

$$ \bar{x} = \frac{1}{3} (165 + 171 + 189) = 175 $$

となります.

`pandas`で平均を求めるには, `.mean()`を利用します.

~~~ py
df = pd.DataFrame({'x':[165,171,189]})
print(df['x'].mean()) #>>>175
~~~


- **幾何平均(geometric mean)**
---

すべてのデータを乗じて,データの数で根を取った値を**幾何平均**といいます. 増加率,減少率など対象とするデータを相互に乗じることによって基準となる値が算出される場合に使用します.

::: note
$n$個の観測値 $x_1, x_2, ..., x_n$ の時,幾何平均 $x_G$ は
$$ x_G = \sqrt[n]{x_1 \cdot x_2 \cdot ... \cdot x_n} = (\prod_{i}^{n} x_i)^{\frac{1}{n}} $$
:::

例：各年の売上と,その増加率が以下のように表されるとき,平均何%売上が伸びているかを考える.

|年度 | 売上  | 増加率 |
|:---:|:---:  |:---:   |
|1    | 300   |        |
|2    | 350   | 117%   |
|3    | 600   | 171%   |
|4    | 1000  | 167%   |

このとき算術平均を用いると,
$$ 算術平均 = \frac{1.17 + 1.71 + 1.67}{3} \approx 1.52 $$ となり,

$$ 300 \cdot 1.52^3 \approx 1045 $$

平均的な伸び率を3回乗じても4年目の売上の値になりません. これは,毎年度の値に増加率を**掛ける**ことで次の年度の値が求まるのに対して,算術平均は毎年度足す操作をしたばあいの平均値を求めているからです.

そこで,幾何平均を求めてみると.

$$ 幾何平均 = \sqrt[3]{1.17 \cdot 1.71 \cdot 1.67} \approx 1.49, \\
300 \cdot 1.49^3 \approx 1000 $$
となり, 正確に4年目の値が計算できていることが分かります.

- **調和平均(harmonic mean)**
---
先程の幾何平均が掛け算の平均値だったのに対して,割り算の平均値を**調和平均**といい,速度などの定義に割り算が含まれている計算で用います.

調和平均は対象とするデータに別の値を除して足し合わせることによって基準となる値が算出される場合に使用します.

::: note

$n$個の観測値 $x_1, x_2, ..., x_n$ の時,調和平均 $x_H$ は
$$ \frac{1}{x_H} = \frac{1}{n} \left( \frac{1}{x_1} + ... + \frac{1}{x_n} \right) \\
 \iff \\
 x_H = \frac{n}{\frac{1}{x_1} + ... + \frac{1}{x_n}}  = \frac{n}{\sum_{i=1}^{n} \frac{1}{x_i}}$$
:::

例：平均速度

100kmの道のりを行きは車 (60km/h),帰りは自転車 (30km/h) で移動した場合,算術平均は 45km/hとなります. しかし,

かかった時間は $\frac{距離}{速度}$で求まるので,
車は $\frac{100}{60}$, 自転車 は $\frac{100}{30}$ となり,速度は $\frac{距離}{時間}$ で求まるので

平均速度は,
$$ \frac{200}{\frac{200}{60} + \frac{200}{30}} = \frac{2}{\frac{1}{30} + \frac{1}{60}} = 40 $$

となります. 算術平均では正確に計算できていないことが分かります.

この $$\frac{200}{\frac{200}{60} + \frac{200}{30}} $$
が調和平均です.



- **中央値(median)**
---

中心を表す統計量として,データを昇順に並び替えて,そのちょうど真ん中の数を表す**中央値**も良く利用されます.

::: note
$n$個の観測値を大きさの順に並べ替えて $x_1, x_2, ..., x_n$ とした時,
中央値 $\tilde{x}$ は

$$ \tilde{x} =
\begin{cases}
x_{\frac{n+1}{2}}, ~~& if ~~ n \mod 2 \neq 0 \\
\frac{x_{ \frac{n}{2}} + x_{\frac{n}{2} + 1 }}{2}, ~~ & if~~ n \mod 2 = 0
\end{cases}
$$


:::


例：観測値が $3, 4, 7, 9, 11, 12, 15$ の時 中央値は
$$ \tilde{x} = x_{\frac{n+1}{2}} = x_{\frac{8}{2}} = x_4 = 9 $$

となります.

また,観測値が $4, 5, 6, 10, 14, 17$ のように偶数個の場合は
$$ \tilde{x} = \frac{x_{ \frac{n}{2}} + x_{\frac{n}{2} + 1 }}{2} = \frac{6 + 10}{2} = 8 $$

となります.

`pandas`では,中央値は`.median()`で求めることができます.

~~~ py
df = pd.DataFrame({'x':[3,4,7,9,11,12,15]})
print(df['x'].median()) #9.0

df = pd.DataFrame({'x':[4,5,6,10,14,17]})
print(df['x'].median()) #8.0
~~~


- 四分位数 (quartiles)
---

**中央値:** データを小さい順に並べた時に観測数が50%となる点

**四分位数:**
データを小さい順に並べた時に,観測数が25%, 50%, 75%となる点.
第一四分位数 (25%),第二四分位数 (50%),第三四分位数 (75%)

**四分位範囲:**
四分位範囲 = 第一四分位数から第三四分位数の範囲


![四分位数](/images/quartiles.png)


`pandas`で四分位数を用いるには, `.quantile(q=%点の数値,interpolation='nearest')`で求めることができます.
`interpolation` (補間) は,値がインデックス`i`と`j`の間にある場合に補間する方法を指定する引数で以下のような設定が可能です.

`.quantile()`では, データの最大インデックス掛ける`q`で必要な値を求めます. なので例えば`xs=[2,5,8,9,11,13,15,16,19,22,24]`の25%点は,`10*0.25=2.5`となり`xs[2.5]`となるような点を求めます.

|引数        |効果       |25%点の場合の計算                      |
|:---        |:---       | :---                                  |
|`'linear'`   | 線形補間  | `xs[2] + (xs[2] - xs[3]) * 0.5 = 8.5` |
|`'lower'`   | 小さい方  | `xs[2] = 8`                           |
|`'higher'`  | 大きい方  | `xs[3] = 9`                           |
|`'midpoint'`| 中間      | `(xs[2] + xs[3])/2 = 8.5`             |
|`'nearest'` | 近い方    | `xs[2] = 8`                           |

~~~ py
xs = [2,5,8,9,11,13,15,16,19,22,24]
df = pd.DataFrame({'x':xs})
print('25%点:',df['x'].quantile(q=0.25, interpolation='linear'))
print('25%点:',df['x'].quantile(q=0.25, interpolation='nearest'))
print('50%点:',df['x'].quantile(q=0.5, interpolation='nearest'))
print('75%点:',df['x'].quantile(q=0.75, interpolation='nearest'))

"""
25%点: 8.5
25%点: 8
50%点: 13
75%点: 19
"""
~~~


- 最頻値 (mode)
---

中心を表す統計量の最後は,最も頻繁にあらわれる値,すなわち度数分布表において最も度数の高い階級を表す**最頻値**です.

::: note
最頻値はデータの種類に応じて,意味が異なるので注意が必要です.
(※ 質的データの平均値などは定義できません)

- 質的データの場合

    - 最大度数のカテゴリー

- 量的データの場合

    - 最大度数の階級 = 最頻階級 (modal class)

    - 長さや気温など同じ間隔で値が存在するもの

:::

`pandas`で最頻値を求める方法は色々ありますが, 単純に同じデータが最も多い値を探す場合には`.mode()`が利用できます. `.mode()`は最頻値が複数ある場合に対応するために`DataSeries`Objectを返すので,`[0]`で最初の値を取っています.


~~~ py
df = pd.DataFrame({'x':['A','A','B','C']})
print(df['x'].mode()[0]) # A
df = pd.DataFrame({'x':[1,3,2,4,5,3]})
print(df['x'].mode()[0]) # 3
~~~

しかし,この方法では連続値の数値などにおける定義での最頻値は求められません. 特定の区間の最頻値を求めたい場合は`value_count()`などを利用して求めましょう.

~~~ py
# ランダムな1から100までのデータの生成
np.random.seed(0)  # 再現性のためにシードを設定
df = pd.DataFrame({'x':np.random.randint(0, 101, size=1000)})
print(df)

#10区切りで度数を求める
bins = [0,10,20,30,40,50,60,70,80,90,100]
freq = df['x'].value_counts(bins =bins, sort=False)
print(freq)
#最大の度数のindexを取得
print(freq.idxmax())

"""
      x
0    44
1    47
2    64
3    67
4    67
..   ..
995  79
996  41
997  17
998  80
999  43

[1000 rows x 1 columns]
(-0.001, 10.0]    121
(10.0, 20.0]       87
(20.0, 30.0]       96
(30.0, 40.0]      107
(40.0, 50.0]       94
(50.0, 60.0]       92
(60.0, 70.0]       97
(70.0, 80.0]      103
(80.0, 90.0]      103
(90.0, 100.0]     100
Name: count, dtype: int64
(-0.001, 10.0]
"""
~~~

::: note
- 中心を表す代表値の使い分け
---

中心を表す基本統計量である,**算術平均**,**中央値**,**最頻値**は,ヒストグラムが**単峰で左右対称**である場合一致します. したがってヒストグラムを作成して単峰で左右対称である場合には,どの値を利用しても大きな違いは生まれません.


一方で,分布が歪んでいる場合には,それぞれの統計量の値が変わります. 中心を表す統計量として,何も考えずに算術平均を利用する人がいますが,分布が歪んでいる場合には目的に応じて最頻値や中央値の方が適当である場合があります.

![分布の歪みと中心](/images/mean_median_mode.png)

例えば,以下の図は,日本人の平均所得を表したヒストグラムです.

![[https://www.mhlw.go.jp/toukei/saikin/hw/k-tyosa/k-tyosa09/2-2.html
](https://www.mhlw.go.jp/toukei/saikin/hw/k-tyosa/k-tyosa09/2-2.html
)](/images/mean_median_mode2.png)

この図では,分布が大きく右に歪んでいるため平均値,中央値,最頻値の値が異なっています.それぞれの値が何を意味するのかを考えてみましょう.

- 平均値: 547万円
    - 全員の所得を足して一人あたりで割る(仮に全員平等にもらえるならこの金額)

- 中央値: 427万円
    - 下から所得の低い順に並んだときに真ん中の人(これ以上なら真ん中より上の所得)

- 最頻値: 250万円
    - 一番多い(街で適当に声をかけるとコレくらいである可能性が高い)

このようなときに, 世間一般の人の感覚を表す値として平均値を利用することは適当ではないでしょう. 平均値は,分布が歪んでいる場合には少数のデータに大きく引っ張られるため全体の傾向を表せない場合があります.

例えば,年収300万円の人が100人いる村に年収50億円の野球選手が引っ越してくると,平均年収は5000万円,中央値,最頻値は300万円になります.

統計量はそれぞれの意味を把握したうえで, 目的に応じて使い分けるようにしましょう.

:::


## データの広がりを表す統計量

分布の中心がどこかという点の他に,データがどのように広がっているかもデータの特徴を記述するうえでは重要です.

![ヒストグラムの比較](/images/histogram_compare.png)

データの広がり具合を**散布度**といいますが,散布度を表す統計量として代表的なものに**分散**と**標準偏差**があります.

データの散らばり具合を数値化するために,どのように考えるかを順におって見ましょう.
データがどの程度散らばっているかを考える際の基準の一つが算術平均 $\bar{x}$ です.
各データ $x_i$ が,データの算術平均からどの程度離れているのかを考えてみましょう.

![偏差](/images/deviation.png)

::: note
- **偏差(deviation)**

$$ \text{観測値} x_i  \text{と平均} \bar{x} \text{の差} = x_i - \bar{x} $$

:::

この偏差がデータ全体でどのくらい大きいのかを考えるために**偏差の平均**を取ります.
ただし,平均からの差は,すべて足し合わせると0になるため,絶対値を取ります.これを**平均偏差**と呼びます.

::: note
- **平均偏差(mean deviation)**

各観測値が平均からどれだけ離れているかの絶対値平均

$$ d = \frac{1}{n} \sum_{i=1}^{n} |x_i - \bar{x}| $$

:::

絶対値の計算は面倒なので,値が大きくはなるけれど2乗してみることにします. これが**分散**です.

::: note
- **分散(variance)**

各観測値の偏差の2乗の平均

$$ S^2 = \frac{1}{n} \sum_{i=1}^{n} (x_i - \bar{x})^2 $$

:::


2乗すると値が大きくなるので,単位をもとのデータに合わせるために分散の平方根をとります(例えば, $x_i$ の単位がKgのとき, $S^2$ の単位は $Kg^2$ となってしまう).これを標準偏差といいます.

::: note
- **標準偏差(standard deviation)**

$$ S = \sqrt{S^2} $$

:::

例えば, データが`[9,6,12,18,10]`の場合を考えてみましょう. 何を計算しているのか,イメージしながら順番に計算していきましょう.

~~~ py
data = [9,6,12,18,10]
df = pd.DataFrame({'data':data})
print(df)
"""
  data
0     9
1     6
2    12
3    18
4    10
"""
~~~

まずは平均からの差(偏差)を求めてみます.

~~~py
#平均
barx = df['data'].mean()
print(barx) #11.0

# 偏差
df['dev'] = df['data'] - barx
print(df)
"""
   data  dev
0     9 -2.0
1     6 -5.0
2    12  1.0
3    18  7.0
4    10 -1.0
"""
~~~

次に偏差の2乗とその平均(分散)を求めます.

~~~py
#偏差の2乗
df['dev2'] = df['dev'] * df['dev']
print(df)
"""
   data  dev  dev2
0     9 -2.0   4.0
1     6 -5.0  25.0
2    12  1.0   1.0
3    18  7.0  49.0
4    10 -1.0   1.0
"""

#偏差の2乗の平均(分散)
print(df['dev2'].mean()) #16.0
~~~

最後に分散の累乗根をとって,標準偏差を求めます.

~~~ py
#偏差の2乗の平均の累乗根(標準偏差)
print(np.sqrt(df['dev2'].mean())) #4.0
~~~

計算から,このデータの分散は`16`,標準偏差は`4`であることが分かりました.

`pandas`では,分散は`.var(ddof=0)`,標準偏差は`.std(ddof=0)`で求めることができます.

~~~ py
print('分散:',df['data'].var(ddof=0)) #16.0
print('標準偏差:',df['data'].std(ddof=0)) #4.0
~~~

::: warn

- 不偏分散,不偏標準偏差

`.var()`と`.std()`における引数`ddof=0`とはなんでしょうか.これは,分散を求める際の分母の値から引く数を表しています(引いた後の値を自由度といいます).

デフォルトの値は,`ddof=1`となっており,

$$\frac{1}{n-1} \sum_{i=1}^{n} (x_i - \bar{x})^2$$ を求めています.

このような分母が $n-1$となっている分散を**不偏分散**といいます. 一方で,これまで計算してきた値を**標本分散**といいます.

不偏分散の意味に関しては, 統計学入門で学習していただくとして,ここでは一言に分散や標準偏差といっても,
細かくは**標本分散**,**不偏分散**,**母分散**などの異なる概念があることに注意しましょう.
特にプログラムにおいて, 分散や標準偏差を求める際には,それがなんの値なのかに注意が必要です.
ネット上の記事などにおいても混同していることが多いので, 実際に自分で計算して確かめることをおすすめします.

例えば,`pandas`における`describe()`で表示される`std`は不偏標準偏差であり,`std(ddof=0)`の値とは異なります.

~~~ py
print(df['data'].describe())
"""
count     5.000000
mean     11.000000
std       4.472136
min       6.000000
25%       9.000000
50%      10.000000
75%      12.000000
max      18.000000
Name: data, dtype: float64
"""
~~~

:::

::: note

- 演習

- 算術平均,幾何平均, 調和平均,標本標準偏差を計算する関数をそれぞれ作成してください.

- [こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/histogram_A_B_data.csv)のデータの列ごとの算術平均,中央値,最頻値,標本分散,標本標準偏差を求めよ

:::


## 相関

基本統計量は,一つの観測項目に対する数値化の手法でしたが,可視化における散布図のように,2つの観測項目間の関係を数値で表すことが可能です.

散布図の節で扱った[事例](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/scatter.csv)についてもう一度考えてみましょう.

~~~ sh
     AI  Python
0    34      27
1    40      26
2    59      28
3    46      29
4    36      29
..   ..     ...
255  58      83
256  69      87
257  59      82
258  62      84
259  59      87
~~~

~~~ py
df = pd.read_csv('data/scatter.csv')
#散布図のx軸を指定
x_column = 'AI'
#散布図のy軸を指定
y_column = 'Python'

x_value = df[x_column]
y_value = df[y_column]
plt.scatter(x_value, y_value)
plt.ylabel(y_column)
plt.xlabel(x_column)
plt.show()
~~~

![散布図](/images/scatter.png)

Google Trendの `AI`と`Python`の検索数から散布図を作成すると, AIの検索数が増えるにつれて`Python`の検索数が増えていることが分かります.

::: note
- 相関関係
---

このような関係を**相関関係(correlation)**といい,2つの変数の間に直線関係に近い傾向が見られるときに｢**相関関係がある**｣といいます.

直線的であるほど**強い相関**,逆を**弱い相関**といいます.

また,

- 一方が増加したとき,他方が増加する関係を **正の相関関係**
- 一方が増加したとき,他方が減少する関係を **負の相関関係**

といいます.

このような相関関係があるかないかは,散布図を見ただけである程度判断が可能ですが, 相関が**ある/ない**,**強い/弱い**というのは抽象的な表現なので, 厳密に判断する場合にはそれらを数値として表す必要があります.

![相関関係](/images/corre1.png)


相関関係を数値化したものを**相関係数(correlation coefficient)**といい,データの尺度に応じて,以下のような種類が存在します.

|尺度                       |係数                        |
|:---:                      |:---:                       |
|量的変数 $\times$ 量的変数 | ピアソンの積率相関係数     |
|順位尺度 $\times$ 順位尺度 | スピアマンの順位相関係数   |
|名義尺度 $\times$ 質的変数 | ピアソンの $\Chi^2$ 統計量 |

:::

### ピアソンの積率相関係数

2つの量的変数に利用される相関係数を**ピアソンの積率相関係数(product moment correlation coefficient)**といいます.

データが $(x_1, y_1), (x_2, y_2), ..., (x_n, y_n)$ の時,

$$
\begin{align*}
r_{xy} &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y}) / n}{\sqrt{\sum (x_i - \bar{x})^2 / n} \sqrt{\sum (y_i - \bar{y})^2 / n}} \\
& = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2} \sqrt{\sum (y_i - \bar{y})^2}} \\
&= \frac{s_{xy}}{s_x s_y}
\end{align*}
$$

なお,
$$
\quad s_{xy} = \frac{1}{n} \sum (x_i - \bar{x})(y_i - \bar{y})
$$
を $x$ と $y$ の共分散といい,相関係数は $\frac{xとyの共分散}{xの標準偏差 \times yの標準偏差}$の形で表されます.

![ピアソンの積率相関係数のイメージ](/images/corre2.png)
![ピアソンの積率相関係数のイメージ](/images/corre3.png)
![ピアソンの積率相関係数のイメージ](/images/corre4.png)


$x_i, y_i$ を標準化し $z_i = \frac{x_i - \bar{x}}{s_x}, w_i = \frac{y_i - \bar{y}}{s_y}$ とすると,

$$
\begin{align*}
r_{zw} &= \frac{1}{n} \sum z_i w_i \\
&= \frac{1}{n S_z S_w} \sum (x_i - \bar{x})(y_i - \bar{y}) \\
&= \frac{1}{n} \sum \left( \frac{x_i - \bar{x}}{S_x} \right) \left( \frac{y_i - \bar{y}}{S_y} \right) \\
&= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n S_x S_y} \\
&= r_{xy}
\end{align*}
$$

このとき、証明の為に $\frac{1}{n} \sum (z_i \pm w_i)^2$ を考える。

$$
\begin{align*}
\frac{1}{n} \sum (z_i \pm w_i)^2 &\geq 0 \\
\frac{1}{n} \sum (z_i^2 \pm 2z_i w_i + w_i^2) &\geq 0 \\
\frac{1}{n} \sum z_i^2 \pm \frac{2}{n} \sum z_i w_i + \frac{1}{n} \sum w_i^2 &\geq 0 \\
\frac{1}{n S_x^2} \sum (x_i - \bar{x})^2 \pm \frac{2}{n} \sum z_i w_i + \frac{1}{n S_y^2} \sum (y_i - \bar{y})^2 &\geq 0  \\
\frac{S_x^2}{S_x^2} + \frac{2}{n} \sum z_i w_i + \frac{S_y^2}{S_y^2}  &\geq 0 \\
1 \pm \frac{2}{n} \sum z_i w_i + 1 &\geq 0 \\
2 (1 \pm r_{xy}) &\geq 0 \\
-1 \leq r_{xy} \leq 1
\end{align*}
$$

このように相関係数は常に $-1 \leq r_{xy} \leq 1$ を取ります.

また, $c$を$c > 0$の定数として$すべての点で $x_i = c y_i$ が成り立つとき, $\bar{x} = c \bar{y_i}$ が成り立ち,

$$
\begin{align*}
S_y &= \sqrt{\frac{1}{n} \sum (y_i - \bar{y})^2} \\
    &= \sqrt{\frac{1}{n} \sum (c x_i  - c \bar{y})^2} \\
    &= \sqrt{\frac{c^2}{n} \sum (x_i - \bar{x})}
\end{align*}
$$


となります.
したがって,

$$
\begin{align*}
r &= \frac{\frac{1}{n} \sum (x_i - \bar{x})(y_i - \bar{y})}{S_x \times S_y} \\
  &= \frac{\frac{c}{n} \sum (x_i - \bar{x})(x_i - \bar{x})}{S_x \times S_x} \\
  &= \frac{c S_x^2}{c S_x^2} = 1
\end{align*}
$$

となり,`1`となります. また, $c < 0$ の場合は, $S_x \times S_y = -c S_x^2$となるので,`-1`になります.

このように$x_i$と$y_i$が同じ比率で増減するとき,

- $r_{xy} = 1$ となり、正の完全相関
- $r_{xy} = -1$ となり、負の完全相関

といいます.

なお, 相関が「ある/ない」の目安は以下のようになっています.

| 相関係数            | 関連性の程度               |
|---------------------|----------------------------|
| 0.0～0.4 、0.0～-0.4  | ほとんど相関がない       |
| 0.4～0.7 、-0.4～-0.7 | 弱い相関がある           |
| 0.7～0.9 、-0.7～-0.9 | 強い相関がある           |
| 0.9～1.0 、-0.9～-1.0 | きわめて強い相関がある   |


Pythonで積率相関係数を求めるには `numpy`の`np.corrcoef(xのデータ,yのデータ)`あるいは,`scipy.stats.pearsonr(xのデータ,yのデータ)`を利用します. `scipy`がインストールされていない人は `pip install scipy`をしておきましょう.

~~~ py
import pandas               as pd
import matplotlib.pyplot    as plt
import japanize_matplotlib
import numpy as np
import scipy.stats as st

#データの読み込み
#データの位置を指定しよう
df = pd.read_csv('Data/scatter.csv')
print(df)

#散布図のx軸を指定
x_column = 'AI'
#散布図のy軸を指定
y_column = 'Python'

x_value = df[x_column]
y_value = df[y_column]
plt.scatter(x_value, y_value)
plt.ylabel(y_column)
plt.xlabel(x_column)
plt.show()

# numpyで相関係数を求める
# 返り値が [[xとxの相関係数=1, xとyの相関係数]
# ,[yとxの相関係数, yとyの相関係数=1]]
# となっている
print(np.corrcoef(df[x_column],df[y_column]))
"""
[[1.         0.83281294]
 [0.83281294 1.        ]]
"""
print(np.corrcoef(df[x_column],df[y_column])[0][1])
# 0.83281294

#scipy.stats.pearsonr でも計算可能
# 返り値が (相関係数, p値)の形に成っている
# p値に関しては, 検定の章で扱います.
r, p = st.pearsonr(df[x_column],df[y_column])
print(r) #0.8328129378961621
~~~

### スピアマンの順位相関係数

積率相関係数は量的変数にしか利用できませんが,質的変数のうち順序尺度データに関しては,**スピアマンの順位相関係数(rank correlation coefficient)**が利用できます.

スピアマンの順位相関係数は, 順序尺度データを順位に変換して,順位の間の相関係数を求めたものになります.

![ピアソンの積率相関係数のイメージ](/images/corre5.png)

データを小さい順に並べ替えた順位 $x_i, ..., x_n$ がある時,

$$
r_{xy} = \frac{\frac{1}{n} \sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\frac{1}{n} \sum (x_i - \bar{x})^2} \sqrt{\frac{1}{n} \sum (y_i - \bar{y})^2}}
$$
がどの様になるかを考える.

順位相関係数は,

$$
\sum x_i = \sum y_i = \frac{n(n+1)}{2}
$$

$$
\sum x_i^2 = \sum y_i^2 = \frac{1}{6} n(n+1)(2n+1)
$$

$$
\bar{x} = \bar{y} = \frac{\sum x_i}{n} = \frac{n(n+1)}{2n} = \frac{n+1}{2}
$$

なので, 分子に関して,

$$
\begin{align*}
& \frac{1}{n}\sum(x_i - \bar{x})(y_i - \bar{y}) \\
&= \frac{1}{n}\sum \{x_iy_i - x_i \bar{y} - \bar{x}y_i + \bar{x}\bar{y}\} \\
&= \frac{1}{n}\sum x_i y_i - \frac{1}{n}\sum x_i\bar{y} - \frac{1}{n}\sum \bar{x}y_i + \frac{1}{n}\sum  \bar{x}\bar{y} \\
&= \frac{1}{n}\sum x_i y_i - \frac{\bar{y}}{n}\sum x_i - \frac{\bar{x}}{n}\sum y_i + \bar{x}\bar{y} \\
&= \frac{1}{n}\sum x_i y_i - \bar{x}\bar{y} \\
&= \frac{1}{2n}\sum \{x_i^2 + y_i^2 - (x_i - y_i)^2\} - \bar{x}\bar{y} \\
\end{align*}
$$

$$
\begin{align*}
\because (x_i - y_i)^2 = x_i^2 -2x_i y_i + y_i^2  \\
x_i y_i = \frac{1}{2} \{ x_i^2 + y_i^2 - (x_i - y_i)^2 \}
\end{align*}
$$

$$
\begin{align*}
& \frac{1}{2n}\sum \{x_i^2 + y_i^2 - (x_i - y_i)^2\} - \bar{x}\bar{y} \\
&= \frac{1}{2n}\sum x_i^2 + \frac{1}{2n}\sum y_i^2 - \frac{1}{2n}\sum (x_i - y_i)^2 - \bar{x}\bar{y} \\
& = \frac{1}{6} (n+1)(2n+1) - \frac{(n+1)^2}{4} - \frac{1}{2n}\sum (x_i - y_i)^2 \\
& = \frac{1}{12}(n+1)(n-1) - \frac{1}{2n}\sum (x_i - y_i)^2
\end{align*}
$$

また, 分母に関して,
$$
\begin{align*}
& \frac{1}{n}\sum (x_i - \bar{x})^2 \\
&= \frac{1}{n}\sum x_i^2 - n \bar{x}^2 \\
&= \frac{1}{n} \{ \sum x_i^2 -2n \bar{x}^2 + \bar{x}^2 \} \\
&= \frac{1}{n} \{\sum x_i^2 - n\bar{x}^2\} \\
&= \frac{1}{6}(n+1)(2n+1) - \frac{1}{4}(n+1)^2 \\
&= \frac{1}{12}(n+1)(n-1)
\end{align*}
$$


なので

$$
r_{xy} = \frac{\frac{1}{n} \sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\frac{1}{n} \sum (x_i - \bar{x})^2} \sqrt{\frac{1}{n} \sum (y_i - \bar{y})^2}}
$$

に代入して,
$$
\begin{align*}
r_{xy} = \frac{\frac{1}{n} \sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\frac{1}{n} \sum (x_i - \bar{x})^2} \sqrt{\frac{1}{n} \sum (y_i - \bar{y})^2}} &= \frac{\frac{1}{12}(n+1)(n-1) - \frac{1}{2} \sum (x_i - y_i)^2}{\frac{1}{12}(n+1)(n-1)}\\
&=  1 - \frac{6}{n^3 - n} \sum (x_i - y_i)^2
\end{align*}
$$

となる.

スピアマンの順位相関係数は,Pythonでは, `scipy.stats.spearmanr(xのデータ,yのデータ)`で求めることができます.

[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/spearman.csv)のデータは,国別(A~J)のサッカー(FIFA)と野球(WBSC)のランキングのダミーデータです.

~~~ sh
    rank FIFA WBSC
0     1    A    A
1     2    B    E
2     3    C    G
3     4    D    I
4     5    E    D
5     6    F    C
6     7    G    B
7     8    H    F
8     9    I    H
9    10    J    J
~~~

こちらの順位の相関係数を求めてみましょう.

~~~ py
df = pd.read_csv('Data/spearman.csv')
# sciypyで相関係数を求める
correlation, pvalue = st.spearmanr(df["FIFA"], df["WBSC"])
print("相関係数:",correlation) #0.4545
~~~

`0.45`なので弱い正の相関があることが分かります.

### 相関係数のヒートマップ

相関係数はデータの関係を探るために非常に便利な数値であり, 複数の観測項目からなるデータを扱う場合には,最初に相関係数をとってそれぞれにどのような関係があるのかを確認するようにしましょう.

[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/coeff_multi.csv)のデータは,e-statから取得した,県別の身長,体重,食費,睡眠の平均時間,スポーツの平均時間に関するデータです.

~~~ sh
python coeff.py
    pref  height  weight   food  sleep  sports
0    北海道   170.4    63.7  65739    477      15
1    青森県   169.8    62.8  64889    490      13
2    岩手県   170.6    63.7  70156    489      13
3    宮城県   169.8    63.4  73337    482      15
4    秋田県   170.6    66.1  74560    493      14
5    山形県   170.9    63.9  76000    497      12
6    福島県   170.2    63.9  71074    480      13
7    茨城県   169.7    62.4  74341    467      17
8    栃木県   169.8    63.3  74387    472      16
9    群馬県   170.5    62.7  71701    475      15
10   埼玉県   170.4    61.1  76663    463      15
11   千葉県   170.3    62.4  77639    458      15
12   東京都   170.5    61.3  83506    461      16
13  神奈川県   170.9    62.5  77510    456      17
14   新潟県   170.9    62.0  75937    479      13
15   富山県   170.8    63.5  73589    471      15
16   石川県   170.8    62.9  76256    470      16
17   福井県   170.4    62.5  79478    476      14
18   山梨県   170.1    61.7  71294    481      16
19   長野県   169.5    61.0  72228    474      17
20   岐阜県   169.9    60.6  69527    469      13
21   静岡県   170.1    61.9  75833    474      19
22   愛知県   169.6    60.9  74694    463      14
23   三重県   170.4    62.1  75721    473      16
24   滋賀県   170.6    62.9  77978    470      18
25   京都府   170.5    62.0  76904    464      16
26   大阪府   170.2    62.2  74015    469      18
27   兵庫県   169.6    60.6  72847    466      15
28   奈良県   169.9    61.8  74888    461      18
29  和歌山県   170.0    62.6  69858    479      15
30   鳥取県   170.4    62.7  73321    475      14
31   島根県   169.7    60.7  72160    483      16
32   岡山県   169.6    61.7  69060    475      17
33   広島県   168.8    60.8  69061    473      15
34   山口県   169.2    60.2  69882    472      17
35   徳島県   169.9    64.4  67102    472      18
36   香川県   169.9    62.7  68400    469      18
37   愛媛県   168.8    61.8  67274    474      16
38   高知県   169.1    61.8  70188    484      16
39   福岡県   169.7    60.9  70135    471      14
40   佐賀県   169.2    62.2  68749    473      16
41   長崎県   170.1    63.3  66641    473      18
42   熊本県   169.5    61.8  66184    482      20
43   大分県   169.3    62.5  69255    479      18
44   宮崎県   168.7    61.8  65165    477      19
45  鹿児島県   169.7    61.4  65377    479      18
46   沖縄県   168.7    60.6  56298    482      20
~~~

データの取得過程に興味がある人は,以下の手順を自分で行い,データを作ってみましょう.

<details>
        <summary> e-statのデータ表示機能を使ってデータを自分で作る (開く/閉じる) </summary>
::: note

e-statで県別の身長,体重,睡眠時間等のデータを集めます.
地域別のデータは｢地域｣から選択できます.

![e-stat 地域](/images/coeff_multi1.png)

｢都道府県データ｣を選択し,｢データ表示｣をクリックします.

![e-stat 地域](/images/coeff_multi2.png)

データを集める,地域,表示項目,表示方法の順に選択します.
今回はすべての県を利用するので,｢全て選択｣をクリックしたあと｢確定｣をクリックします.

![e-stat 地域](/images/coeff_multi3.png)

次に項目をきめます.
分野をクリックして｢I健康･医療｣を選ぶと,項目候補に健康・医療に関わる項目が表示されます.
そこから,

    - I411301_身長（高校2年）（男）【ｃｍ】
    - I412301_体重（高校2年）（男）【ｋｇ】

を順番に選んで,｢項目を選択｣をクリックします.

![e-stat 地域](/images/coeff_multi4.png)

同様に
｢L 家計｣から

    - L421101_食料費（二人以上の世帯のうち勤労者世帯）（全国消費実態調査結果）【円】

｢M 生活時間｣から

    - M1101_睡眠の平均時間（10歳以上）（男）【分】
    - M360100_スポーツの平均時間（15歳以上）（男）【分】

を順番に選んで,｢項目を選択｣をクリックします.

![e-stat 地域](/images/coeff_multi5.png)

最後にどのデータを表示するかレイアウトを決めます.

    - 調査年を列に配置
    - 表示年度を2000から2010まで
    - 設定して表示を更新

![e-stat 地域](/images/coeff_multi6.png)

データをダウンロードします.


![e-stat 地域](/images/coeff_multi7.png)

ダウンロードしたデータを読み込めるcsvに編集します.

    - 2006年以外の列を削除
    - 数値として新しいシートにコピー
    - ヘッダー名をつける
        - 県名 pref (prefectureの略)
        - 身長 height
        - 体重 weight
        - 食費 food
        - 睡眠 sleep
        - スポーツ sports
    - utf-8のcsvで保存
    - ファイル名: coeff_multi.csv
    - 作業ディレクトリのDataフォルダに保存

![e-stat 地域](/images/coeff_multi8.png)

:::
</details>

データのどの観測項目間に関連があるのかを確かめるために,作成したデータのすべての組み合わせの相関係数を見てみましょう. 今までのように一つ一つ散布図を作成して,相関係数を求めていると,$\text{観測項目数} \times \text{観測項目数}$ のグラフを作成することになります.
そこで, 与えられた観測項目すべての組み合わせで図示する**ペアプロット**と**ヒートマップ**を活用してみます.

`DataFrame`に含まれるデータのペアプロットには, `pandas`の`.plotting`メソッドを利用します. `pd.plotting.scatter_matrix(ペアプロットを求めるDataFrame)`で,散布図のペアプロットが作成できます.

また, 各項目の相関係数も`pandas`の`.corr()`メソッドで取得することが出来ます.

~~~ py
# CSVファイルを読み込んでデータフレームに格納
# Dataフォルダを作成し,そこにデータを入れておきましょう
df = pd.read_csv('Data/coeff.csv')

# データの表示
print(df)

#分析するデータの選択
labels = ['height', 'weight', 'food', 'sports', 'sleep']
X = df[labels]

#散布図行列を作成してみる
pd.plotting.scatter_matrix(X, range_padding=0.2)
plt.show()

#相関係数の組み合わせを確認
print(X.corr())

#ヒートマップで確認
#ヒートマップで確認
sns.heatmap(X.corr()
           ,vmax=1     #ヒートマップの最大値
           ,vmin=-1    #最小値
           ,center =0  #中心
           ,annot=True)
plt.show()
~~~

![散布図のペアプロット](/images/pair_plot.png)
各項目の組み合わせごとに,散布図が作成されています. 自交点にはヒストグラムが作成されます.

![相関係数のヒートマップ](/images/coeff_multi9.png)

食費と体重,体重と身長などに正の相関があることが分かります.
このように複数の観測項目から関係があるデータを探したい場合には,ペアプロットや,相関係数のヒートマップを作成することで,関係性がわかりやすくなります.


### $\Chi^2$統計量

量的データには積率相関係数, 順位尺度データに対しては,順位相関係数を求めることで2つのデータの関連性を確かめることができました. では,名義尺度データの場合はどのようにすれば良いのでしょうか.

名義尺度を含めた質的変数の関係性を可視化するには,同時度数分布表が利用できました. 数値化においても,同時度数分布表を用いることができます.

![同時度数分布表](/images/cross_table2.png)


質的変数間の関連度合いは,同時度数分布表の数値を利用した **ピアソンの$\Chi^2$統計量(かいじじょうとうけいりょう)**で表すことができます. 可視化の節では, 同時度数分布表から列相対度数を求めましたが,ここでは相対度数ではなく,度数なので注意してください.

$$
\Chi_o^2 = \sum_{i=0}^{r} \sum_{j=0}^{c} \frac{(n_{ij} - E_ij)^2}{E_{ij}} (r:行数,c:列数)
$$

このとき,$E_{ij}$を期待度数といい, $\frac{行の合計 \times 列の合計}{総数}$

$$
E_{ij} = \frac{\sum_{i}^r n_{ij} \times \sum_{j}^c n_{ij}}{\sum_{i}^{r}\sum_{j}^c n_{ij}}
$$

で求められます.

この$\Chi_o^2$が大きいほど,2つの変数の間の関連が強いと言え,この値を利用して行と列のデータが独立であるかを検定する **$\Chi^2$検定(独立性の検定)** を行うことができます. ** $\Chi^2$ 検定** に関しては後ほど扱うとして,ここではこの値を利用して2つのデータの関連の度合いを判断する方法に関して見ていきましょう.

$\Chi_o^2$の値は, 同時度数分布表の行数や列数に依存して値が変わるため,相関係数のように,｢特定の値から関連があるといえる｣といった利用方には適しません.

そこで, 異なるデータを比較するためには, $0 \leq V \leq 1$の値を取る,**クラメールの連関係数V**に変換します.

$$
V = \sqrt{\frac{\Chi_o^2}{n \times min(r - 1,c - 1)}}
$$

クラメールの連関係数は,相関係数よりも高い値が出にくいので,以下のような基準で判断します.

|V          |判断            |
|:---       |:---            |
|0 ~ 0.1    | 関連なし       |
|0.1 ~ 0.25 | 弱い関連がある |
|0.25 ~ 0.5 | 関連がある     |
|0.5 ~ 1.0  | 強い関連がある |

$\Chi_o^2$は `scipy.stats`の`chi2_contingency(度数分布表,correction=False)`で求めることが出来ます.
返り値が, $\Chi_o^2$,p値,自由度,期待度数の4つあるので,注意しましょう.

同時度数分布表の節で扱った時限と成績の関係を記録した[データ](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/cross_table_data.csv)を利用して,クラメールの連関係数Vを求めてみましょう.


~~~ sh
     Period Grade
0         2     B
1         5     A
2         4     A
3         3     C
4         1     C
..      ...   ...
195       2     C
196       4     C
197       5     C
198       5     F
199       4     C
~~~

![ヒートマップ](/images/heatmap.png)


~~~ py
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import scipy.stats as st
import numpy as np

df = pd.read_csv('data/cross_table_data.csv')

#クロス表の作成
cross = pd.crosstab(df['Grade'],df['Period'])

#表示順の設定
cross = cross.reindex([1,2,3,4,5],axis='columns')
cross = cross.reindex(['S','A','B','C','F'],axis='index')
print(cross)

#列相対度数に変更する
for c in cross.columns:
    cross[c] = cross[c] / cross[c].sum()

print(cross)

sns.heatmap( cross  #ヒートマップを作成したいテーブル
           , cmap=plt.get_cmap('Reds') #カラーマップ(省略可)
           , linewidths=.5 #線の太さを指定することでセルを囲う線を表示
           , annot=True  #セルに数値を表示
           )
plt.show()

#χ二乗統計量を求める
x2, p, dof, e = st.chi2_contingency(cross,correction=True)
print(x2) #1.1342960955202308

#クラメールの連関係数Vを求める
v = np.sqrt(x2/(cross.sum().sum() * min(cross.shape[0]-1,cross.shape[1] -1)))
print(v) #0.2381487030743849
~~~

クラメールの連関係数Vの値は,0.28となり,弱い関連があることが分かりました.


### 因果関係と相関

相関関係は, 2つの観測項目間の関係を表していますが, 観測項目Aの変化によって,観測項目Bの変化が起きているという**因果関係(causality)**を示しているものではありません.

論理学における因果関係は, **AならばB $(A \Rightarrow B)$** という関係として示されます.

    - 例: 人間ならば死ぬ

このとき,Aを**十分条件**, Bを**必要条件** といいます.

しかし,統計学における因果関係は,このような関係では表せません.
例えば, **喫煙をすると肺がんになる**という関係は, 喫煙をしても肺がんにならない場合があるので,倫理学における因果関係ではありません. 統計学における因果関係は, **Aが,Bの一部を説明するための,あるいはBが起きる確率を高めるための十分条件となっている**ことを表します.

したがって,統計学における因果関係は, ** AによってBの一部が説明できる** あるいは, ** AによってBが起きる確率が高まる** という形で示され,これを**統計的因果関係**といいます.

::: note
統計的因果関係が認められる条件は,簡単には以下のように示されます.

1. AとBの間に明瞭な関係が認められる

2. Aが時間的に,あるいは意味的にBより選考している

3. AとBの共通要因となりうる要因を統制して(影響を取り除いて)も,両者に関係が見出される.
:::

これらの因果関係を示すには, 特に3.に関して, A以外の条件を揃えてBの発生確率を確かめる**対照実験**などの手法によって明らかにされますが,本資料では実験に関しては扱いません. AによってBを説明する,という関係に関しては,後の回帰の章で少し扱います.

このように, 相関関係と因果関係は異なる概念として理解する必要があります.

例えば, 因果関係があっても相関関係がない有名な例として, **チーズの消費量と,ベッドシーツに絡まって死ぬ人の数**や,**プールで溺れた人の数と,ニコラス･ケイジの映画出演数**などがあります(こちらのサイト([https://www.tylervigen.com/spurious-correlations ](https://www.tylervigen.com/spurious-correlations ))にこういった例が沢山まとめられているので興味のある人は見てみましょう.)



![[spurious correlations](https://www.tylervigen.com/spurious-correlations )](/images/cheese_cinsumption.png)

![[spurious correlations](https://www.tylervigen.com/spurious-correlations )](/images/nicolas_cage.png)

このように,全く因果関係のないものでも現れる相関関係を**偽相関(Spurious Correlation)**といいます.

また,反対に$ y = (x-8)^2 $ という関係においては, yの値は完全にxによって決まるため,xとyの間に因果関係は認められますが,相関係数は0となります.

![因果があっても相関がない例](/images/y_x_8.png)

### 発展:偏相関係数

因果関係を検証する方法に関しては後の章に譲るとして, ここでは,相関における類似概念である**偏相関**について見てみましょう.

**偏相関係数(partial correlation coefficient)** とは, 3つの変数がある場合に, **1つの変数の影響を除いた**残り2つの変数間の相関係数となります.

変数,$x,y,z$があるとき,$z$の影響を除いた, $x,y$の間の偏相関係数は以下のように求められます.

$$
r_{xy \dot z} = \frac{r_{xy} - r_{xz}r_yz}{\sqrt{1 - r_{xz}^2}\sqrt{1 - r_{yz}^2}}
$$

式を見てみると,分子では, xとyの相関係数から,zに関する相関係数を引いていることが分かります.

偏相関係数の具体例を見てみましょう. [こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/partial_coeff.csv)のデータは米国における`x:小麦の1日あたりの消費量`,`y:米の一日あたりの消費量`,`z:肥満度`を表しています.なお, いずれの列も最大を1,最小を0に変換してあります.



この3変数の相関係数を取ってみます.

~~~ py
df = pd.read_csv('data/partial_coeff.csv')
x = df['x']
y = df['y']
z = df['z']

#散布図行列を作成してみる
pd.plotting.scatter_matrix(df, range_padding=0.2)
plt.savefig('partial_coeff_scatter_matrix.png')
plt.close()

#ヒートマップで確認
sns.heatmap(df.corr()
           ,vmax=1     #ヒートマップの最大値
           ,vmin=-1    #最小値
           ,center =0  #中心
           ,annot=True)
plt.savefig('partial_coeff_heatmap.png')
plt.close()

rxy = np.corrcoef(x, y)[0, 1]
rxz = np.corrcoef(x,z)[0, 1]
ryz = np.corrcoef(y,z)[0, 1]
print('x-y:',rxy) #x-y: -0.6001681728315631
print('x-z:',rxz) #x-z: 0.8000777549807391
print('y-z:',ryz) #y-z: -0.4740072261555343
print('ryzx:',ryzx)
~~~

![partial_coeff_scatter_matrix.png](/images/partial_coeff_scatter_matrix.png)

![partial_coeff_heatmap.png](/images/partial_coeff_heatmap.png)

相関係数を見ると,

- $r_{xy}\approx-0.60$: 小麦を食べる量が多いと米を食べる量が少ない

- $r_{xz}\approx0.80$:小麦を食べる量が多いほど太っている

- $r_{yz}\approx-0.47$:米を食べる量が多いほど痩せている

となっています.

小麦を食べるほど,米を食べる量が少ないというのは米国において,米を主食とする人が少ないことから,普段小麦粉を利用した食事をしているほど,米を食べる機会が少ないということで理解ができます. また,小麦を食べる量が多いほど太っているというのも,炭水化物をたくさん食べるほど太っているということで理解できます. 一方で, 米を食べる量が多いほど痩せているという関係は, あまり自然ではありません.

これは, 一般的に小麦を食べる文化圏の人のほうが,アジア系よりも太っていることに影響されていそうです. yとzの散布図に,xで色をつけることで,xの影響を確認してみましょう.

~~~ py
plt.scatter(y,z,c=x)
plt.xlabel('米の消費量')
plt.ylabel('肥満度')
plt.xlim(-0.1,1.1)
plt.ylim(-0.1,1.1)
plt.grid()
plt.colorbar()
plt.title('ryz='+str(ryz)[:5])
plt.savefig('partial_coeff1.png')
plt.close()
~~~

![$r_{yz}$に対するxの影響](/images/partial_coeff1.png)

左上に行くほど,xの値を表す色が明るくなっており, xの影響で$r_{yz}$が負の相関となっていることが分かります.

それでは, 小麦の影響を除いた米の肥満への影響 $r_{yz \dor x}$ を計算してみましょう.

$$
\begin{align*}
r_{yz \dot x} &= \frac{r_{yz} - r_{xy}r_xz}{\sqrt{1 - r_{xz}^2}\sqrt{1 - r_{xy}^2}}
&\approx \frac{-0.47 + 0.8 \times 0.6}{\sqrt{1 - 0.8^2}\sqrt{1 - 0.6^2}}
&\approx 0.02
\end{align*}
$$

Pythonでも計算してみます.

~~~ py
ryzx = (ryz - (rxy * rxz)) / (np.sqrt(1-rxy**2)*np.sqrt(1-rxz**2))
print('ryzx:',ryzx) #0.012866706738387962
~~~

実際にはほとんど,米の消費量と,肥満度に相関はないことが分かります.

最後に,xの影響を打ち消した,yとzの関係をプロットしてみましょう. これは,この後扱う回帰を利用していますので, コードは理解できなくても問題ありません.

~~~ py
#の影響を除いたyとzの散布図
from sklearn.linear_model import LinearRegression
#yとzのxによる回帰式をたてて,その残渣をプロットすることで,
#xの効果を打ち消したyとzの関係を表現
model_y = LinearRegression().fit(df[['x']], y)
residual_y = y - model_y.predict(df[['x']])
model_z = LinearRegression().fit(df[['x']], z)
residual_z = z - model_z.predict(df[['x']])
(-0.47 + 0.8 * 0.6) / (np.sqrt(1 - 0.8**2) * np.sqrt(1 - 0.6**2))

plt.scatter(residual_z,residual_y,c=x)
plt.xlabel('米の消費量')
plt.ylabel('肥満度')
plt.grid()
plt.colorbar()
plt.title('ryz='+str(ryzx)[:5])
plt.savefig('partial_coeff2.png')
plt.close()
~~~

![partial_coeff2.png](/images/partial_coeff2.png)

もとのyとzの散布図における,xの広がりの影響が打ち消されて,ほとんど相関がなくなっていることが分かります.


## 距離と類似度

ユークリッド距離,コサイン距離


</details>

# 検定(執筆中)

# (線形)回帰分析

<details>
    <summary> 開く/閉じる </summary>

相関分析では,ある変数間に関係があることを示すことができました. しかし,相関分析で示せるのは,変数Aによって変数Bが増加するか,減少するかということのみです. 具体的に,どの程度変数Aが動くことで,変数Bがどの程度変動するかを式によって**説明する**手法に**回帰分析(Regression Analysis)**があります.

また,回帰分析は検定の手法によって求められた式がどの程度信頼できるのかを検定によって確かめることも可能です.

::: note
回帰分析では,データ

$$
y = \beta_1 + \beta_2 x
$$

のような式で変数yとxの関係を説明し,この式を**回帰式,回帰方程式**と呼びます. このとき,

- 説明される変数yを **非説明変数**,**目的変数**などと呼びます.

- 説明する変数xを**説明変数**,**従属変数**などと呼びます.

- 回帰式における **$\beta_1$**のような変数に乗じられていない値を**切片**といいます.

    切片は, $x = 0$ のときのyの値を意味しています.

- 変数に乗じられている**$\beta_2$**のような値を**傾き**といい,$\beta_1,\beta_2$ などを併せて**回帰係数**といいます.

    傾きは,xが1変化した際のyの変化量を表しています.

- 説明変数が一つの回帰式を求める分析を**単回帰分析**, 2つ以上の説明変数を用いる場合を**重回帰分析**といいます.

- yがxの線形関数である場合を **線形回帰(Linear regression)**,それ以外のものを**非線形回帰(non-linear regression)**といいます.
:::

## 発展: 回帰分析は何を行っているのか

回帰分析が何を行っているのかについて,単回帰で行っている最小二乗法を事例に確認していきましょう.
重回帰に関しては, 線形計画問題など異なる学習が必要になるので,今回は扱いません. あくまで,回帰というものがどのような意味であるかに関して簡単に説明します.

こちらの詳細は統計学入門で扱っていますので, この講義ではあまり深く扱いません.興味のある方は読んでみてください.

### 母回帰方程式

体重$y$を慎重$x$によって説明する回帰方程式として, $y = \beta_1 + \beta_2 x$ を考えてみます.
しかし, 実際の体重は身長以外の要素によってばらつきます. そのような**ばらつき**を考慮して,データの$i$人目の体重,身長をそれぞれ,$Y_i,X_i$として,身長以外の要素によるばらつきを$\epsilon_i$とすると,母集団において,以下のような式が立てられます.

$$
Y_i = \beta_1 + \beta_2 X_i + \epsilon_i ~(i=1,2,...,n)
$$

これを**母回帰方程式(Population Regression Equation)**と呼びます.
また, $\beta_1, \beta_2$を**母(偏)回帰係数**といい,これを推定,検定することを回帰分析といいます.

### 誤差項,撹乱項

母回帰方程式

$$
Y_i = \beta_1 + \beta_2 X_i + \epsilon_i ~(i=1,2,...,n)
$$

における $\epsilon_i$は$X_i$で説明できない誤差を表す確率変数であり,誤差項,撹乱項といいます.

回帰分析において,誤差項は以下の仮定をおいています.

::: note
- 期待値0: $E(\epsilon_i) = 0 ~ (i=1,2,...,n)$
- 分散一定: $V(\epsilon_i) = \sigma^2 ~ (i=1,2,...,n)$
- 無相関: $i \neq j \Rightarrow Cov(\epsilon_i, \epsilon_j) = 0$
- 正規分布: $\epsilon_i \sim N(o,\sigma^2)$
:::

これによって,

$$
E(Y_i) = \beta_1 + \beta_2 X_i
$$

が得られます.

### 最小二乗法

母回帰方程式

$$
Y_i = \beta_1 + \beta_2 X_i + \epsilon_i ~(i=1,2,...,n)
$$
における母回帰係数$\beta_1, \beta_2$は観測できないので,**誤差項を最小化する**母回帰係数を統計的推測することで求めます.

誤差項を最小化する母回帰係数の推定方法を**最小二乗法(least squares method)**といいます.

母回帰方程式を変形して,

$$
\epsilon_i = Y_i - (\beta_1 + \beta_2 X_i)
$$

が少ないほど, $X_i$による$Y_i$の説明力が上がります(モデルによってよく関係が説明できている.)

なので,モデル全体で,$\epsilon_i$を最小化することを考えてみます.

![least squares method](/images/least-squares-method1.png)

誤差の正負を打ち消すために,モデル全体の誤差項の二乗の和


$$
S = \sum_{i=1} \epsilon_{1}^{2} = \sum \{Y_i - (\beta_1 + \beta_2 X_i)\}^2
$$

Sを最小化する**(最小二乗)推定量** $\hat{\beta_1},\hat{\beta_2}$を求める問題として整理できます.

$S$の偏微分を0とおいて,

$$
\frac{\partial S}{ \partial \beta_1} = -2 \sum (Y_i = \beta_1 - \beta_2 X_i) = 0 \\
\frac{\partial S}{ \partial \beta_2} = -2 \sum (Y_i = \beta_1 - \beta_2 X_i)X_i = 0 \\
$$

これを解いて,

$$
\hat{\beta_1} = \bar{Y} - \hat{\beta_2}\bar{X} \\
\hat{\beta_2} = \frac{\sum(X_i - \bar{X})(Y_i - \bar(Y))}{\sum(X_i - \bar{X})^2}
$$
が得られます.

### 回帰係数の検定

最小二乗推定量によって得られた方程式

$$
Y = \hat{\beta_1} + \hat{\beta_2}X
$$

を**標本回帰方程式**といいます.

求めた標本回帰方程式が,XとYの関係を説明できているのかを考えます. XがYを全く説明できていない場合, $\epsilon_i$だけで説明ができるため, $\beta_2 \neq 0$と言えれば,統計的にXがYを説明できていると言えます.

そこで, 帰無仮説 $H_0:\beta_2 = 0$として,偏回帰係数に関する統計的仮説検定を実施します.

母数$\beta_2$に関する仮説検定を行うために, $\beta_2$の確率分布を考えます.

誤差項 $\epsilon_i \sim N(o,\sigma^2)$として,

$$
\hat{\beta_1} = \bar{Y} - \hat{\beta_2}\bar{X} \\
\hat{\beta_2} = \frac{\sum(X_i - \bar{X})(Y_i - \bar(Y))}{\sum(X_i - \bar{X})^2}
$$
であるから,

$$
V(\hat{\beta_1}) = \frac{\sigma^2 \sum X_i^2}{n\sum(X_i - \bar{X})^2} \\
E(\hat{\beta_1}) = \beta_1 \\
V(\hat{\beta_2}) = \frac{\sigma^2 }{\sum(X_i - \bar{X})^2} \\
E(\hat{\beta_2}) = \beta_2
$$
なので,

$$
\hat{\beta_2} \sim N(\beta_2,\frac{\sigma^2}{\sum(X_i - \bar{X})^2})
$$

となる.

誤差項の母標準偏差 $\sigma$が含まれるので,推定する.

標本回帰方程式 $Y=\hat{\beta_1}+\hat{\beta_2}X$によって求められる各$i$の値(回帰値)

$$
\hat{Y_i} = \hat{\beta_1} + \hat{\beta_2}X_i
$$

と実際に観測された実測値 $Y_i$との差を

$$
\hat{e_i} = Y_i - \hat{Y_i} = Y_i - \hat{\beta_1} - \hat{\beta_2}X_i
$$

を**回帰残渣(residual)**といい,Xで説明されなかった残渣を表す.

回帰残渣を母回帰方程式 $Y_i = \beta_1 + \beta_2 X_i + \epsilon_i$における誤差項$\epsilon_i$の推定値として利用する.

求める必要があるのは,誤差項の分散の推定値としての分散

$$
V(\hat{e_i}) = E(\hat{e_i^2}) - (E(\hat{e_i}))^2
$$

であるが,

$$
\frac{\partial S}{\partial \beta_2} = -2 \sum (Y_i - \beta_1 - \beta_2 X_i)X_i = 0
$$
なので,

$$
\sum (Y_i - \beta_1 - \beta_2 X_i) = \sum \hat{e_i} = 0
$$

となり,

$$
\bar{e_i} = \frac{1}{n} \sum \hat{e_i} = 0
$$

なので,

$$
\begin{align*}
V(\hat{e_i}) &= E(\hat{eI^2}) \\
&= \frac{1}{n-2}\sum (\hat{e_i}^2 - \bar{e_i}^2) \\
&= \frac{\sum \hat{e_i}^2}{n-2}
\end{align*}
$$

となります.

これを誤差項の分散$\sigma^2$の推定値

$$
S^2 = \frac{\sum \hat{e_i}^2}{n-2}
$$
として利用します.

なお,この累乗根は回帰式がどの程度実測値に当てはまっているかを表す,**推定値の標準誤差(standard error of estimates)**と呼ばれます.

$$
s.e. = \sqrt{S^2} = \sqrt{\frac{\sum \hat{e_i}^2}{n-2}}
$$

これを誤差項の母標準偏差$\sigma$の推定値として利用して,$\hat{\beta_2}$の標準誤差の推定値は,

$$
V(\hat{\beta_2}) = \frac{\sigma^2 }{\sum(X_i - \bar{X})^2}
$$

から,

$$
s.e.(\hat{\beta_2}) = \frac{s.e.}{\sqrt{\sum(X_i - \bar{X})^2}}
$$

となり,$s.e.(\hat{\beta_2})$を用いて標準化した値は, $t(n-2)$に従うので,

$$
t_2 = \frac{\hat{\beta_2} - \beta_2}{s.e.(\hat{\beta_2})} \sim t(n-2)
$$



## 重回帰分析

説明変数も被説明変数も量的変数のときに,

$$
y_i = \beta_0 + \beta_1 x_{1i} + \beta_2 x_{2i} + ... + \beta_n x_{ni}
$$

のような回帰式を求めます.

それでは,Pythonで重回帰分析を行ってみましょう.

題材として以下の｢日本教育新聞｣の記事([https://www.kyoiku-press.com/post-223665/
](https://www.kyoiku-press.com/post-223665/))について考えてみます.

::: note

> **Wi-Fi電磁波で学力低下を懸念、市議ら意見交換会**
>
> 2020年12月7日
>電磁波が人体に影響を与え、学力の低下を招くことなどを懸念する市議会議員らは11月8日、無線LANにより生じる「電磁波過敏症」への対策などについて、意見交換会をオンラインで開催した。
>　GIGAスクール構想でICT環境を整備するに当たって、電磁波による問題点とそれへの対策を話し合った。
>　東京都新宿区議会のよだかれん議員は、学力と健康の2つの観点から、「大人でもICT機器を使用すると前頭前野の機能が低下するという様々な研究報告がある。小学1年生からの使用で脳の発達への影響は懸念されないのか」と指摘した。
>　よだ議員は、9月議会の質疑の一部で、令和元年の全国学力テストの結果に基づき、電子黒板やプロジェクターなどの大型電子機器の整備率が1位の佐賀県は正答率が全国で43位だった一方、整備率最下位の秋田県は正答率が1位だったことを紹介した。
>　意見交換会を主催した「いのち環境ネットワーク」の加藤やすこ代表によると、電磁波過敏症は短い時間でも発症の可能性があり、一度の発症が長期に及んで続くという。
>　埼玉県日高市議会の松尾まよか議員は、GIGAスクール構想を進める上で、Wi-Fiのアクセスポイントの位置を児童・生徒から遠ざけた場所に設置する、使用していない時は電源を落とすことを重要な点に位置付けた。
>　松尾議員は、「発症者が出てからでは遅い。発症後の対策に予算をかけるよりも、事前に対策しておく方がよい」と強調した。
>　今回の意見交換会に参加した市議らは、9月議会の発言内容なども報告した。

こちらの記事では,

> 令和元年の全国学力テストの結果に基づき、電子黒板やプロジェクターなどの大型電子機器の整備率が1位の佐賀県は正答率が全国で43位だった一方、整備率最下位の秋田県は正答率が1位だった

ことから,

｢学校教育におけるICT機器の導入が学力低下を招いている｣ということを主張しています.

:::

最下位と1位の2つの観測対象のデータだけでこのような主張が可能なのでしょうか. データを使ってこの主張を検証してみましょう. 必要なデータは[こちら](https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/math_correct.csv)からダウンロード可能ですが,データの取得手順に興味がある方は,以下から確認して自分でデータを作成してみましょう.

::: note
- 教育データの作成

<details open>
    <summary> 開く/ 閉じる </summary>
まずは全国の県別の学力データを探してみます.

 国立教育政策研究所の行っている全国学力・学習状況調査([https://www.nier.go.jp/18chousakekkahoukoku/index.html](https://www.nier.go.jp/19chousakekkahoukoku/index.html))から,令和元年の県別の小学生の学力データを取得します.

![全国学力･学習状況調査](/images/regression1.png)

ただし,こちらのデータはPDFでのみ公開されているためExcelやAIなどを利用して,CSVに変換する必要があります. 今回私はPDFからテキストエディタにコピー&ペーストして, 不要な記号を置換しましたが,好きな方法でやりましょう.

![全国学力･学習状況調査](/images/regression2.png)

続いて, 記事にある,電子黒板やプロジェクターなどの大型電子機器の整備率に関するデータを[e-stat](https://www.e-stat.go.jp )から取得します.

今回は県別データなので,｢地域｣から,データを取得します.

![e-stat 地域](/images/regression3.png)

｢都道府県データ｣にチェックを入れて｢データ表示｣に進みます.

![都道府県データ](/images/regression4.png)

｢地域選択｣において｢全て選択｣をクリックしたあと選択中地域から全国をクリックし,｢地域を削除｣
を押し,47都道府県のみを選び,確定します.

![地域選択](/images/regression5.png)

｢分野｣から｢教育｣を選び関連のありそうな
    - ｢教育用コンピュータ一台あたりの児童数(小学校)｣
    - ｢普通教室の電子黒板整備率(小学校)｣
    - ｢デジタル教科書の整備率(小学校)｣
などを選択し,｢項目を選択｣を押し,確定します.

![教育関連データの選択](/images/regression6.png)

｢調査年｣からデータが揃っている2017年度を選択し,｢再表示｣を押します.

![調査年の選択](/images/regression7.png)

｢ダウンロード｣から,右の図のように選択して,ダウンロードします.

![ダウンロード](/images/regression8.png)

学力のデータにダウンロードした黒板などのデータをコピーして,適切にヘッダーをつけ,Utf-8で作業フォルダの中のDataフォルダに保存しましょう.例ではmath_correct.csvと名前をつけています.
    - 数学正答率       → math
    - 一台あたりのPC   → pc
    - 黒板             → board
    - 電子教科書       → text

Excelで貼り付けた際に文字列情報になっている場合があり,!マークが表示されていたら数値に変換しておきましょう.

![データの編集](/images/regression9.png)

データが読み込めるか確認してみましょう.

~~~ py

df = pd.read_csv('Data/math_correct.csv')
print(df)

"""
   pref  math   pc  board  text
0   北海道    64  5.8   20.8  38.5
1   青森県    67  5.3   22.4  30.9
2   岩手県    66  5.4   19.6  35.9
3   宮城県    65  6.8   14.0  66.6
4   秋田県    70  5.6   22.4  36.8
5   山形県    65  5.7   16.6  44.0
6   福島県    65  5.3   24.6  47.2
7   茨城県    66  6.5   22.0  49.5
8   栃木県    65  5.9   42.6  70.8
9   群馬県    65  6.1   15.4  39.3
10  埼玉県    66  9.6   23.9  58.8
...
"""
~~~

</details>
:::

それでは,このデータを利用して重回帰分析を行っていきます.

::: note

Pythonで重回帰を行えるライブラリは多数ありますが, 今回は統計モデリングのためのライブラリ`statsmodels`を利用します. pip install し, プログラムの最初に, `import statsmodels.api as sm`と記述し`import`しておきましょう.
:::

ただし, 重回帰を実施する前に,いくつか必要な前処理があります. 順番に見ていきましょう.

### 正規化・標準化

重回帰分析では,複数の説明変数の目的変数に対する影響を比較します. 各説明変数の目的変数への影響力は,回帰係数として現れますが説明変数1の単位がmm,説明変数2の単位がm, 説明変数3の単位がKgなどとなると,それぞれの回帰係数は,それぞれ1mmの変化,1mの変化,1Kgの変化に対する目的変数の変化量を表しているために,同じ基準で比較できません.そこで,**正規化/標準化**というデータの単位などを揃える操作を行う必要があります.


::: note

- 正規化(Normalization)

最大値を1,最小値を0に揃えること.

データを $X = {x_1,x_2,...,x_n}$ とすると, 正規化後の値 $x_i'$は

$$
x_i' = \frac{x_i - min(X)}{max(X) - min(X)}
$$

`pandas`では`X`が対象のデータ列名のリストだとすると,以下のように正規化列`X_n`が求められる.

~~~ py
X_n = (df[X] - df[X].min()) \
    / (df[X].max() - df[X].min())
~~~


:::

::: note
- 標準化(Standarization)

標準得点を求めて,平均0,分散1に揃える.

$$
z_i = \frac{x_i - \bar{x}}{\sigma}
$$

`pandas`では`X`が対象のデータ列名のリストだとすると,以下のように標準化列`X_z`が求められる.

~~~ py
X_z = (df[X] - df[X].mean()) \
    / df[X].std()
~~~

それでは, 先ほど得られた教育データを標準化してみましょう.

~~~ py
Y_label = 'math'
#標準化
Y = (df[Y_label] - df[Y_label].mean()) \
    / df[Y_label].std()

plt.hist(Y)
plt.title(Y_label)
plt.show()

# 説明変数のヘッダーを指定
X_labels = ['pc','board','text']
#標準化
X  = (df[X_labels] - df[X_labels].mean()) \
    / df[X_labels].std()

fig, axes = plt.subplots(nrows= 3 #行数の指定
                        ,ncols= 1 #列数の指定
                        ,sharex=True)

#連番に変換
axes = axes.flatten()
for i in range(3):
    col = X_labels[i] #countをiで共通化
    axes[i].hist(X[col])
    axes[i].set_title(col)
plt.show()
~~~

![Yのヒストグラム](/images/regression10.png)
![Xのヒストグラム](/images/regression11.png)

::: warn
本来ならば,ここで`math`が左右対称ではないことから正規分布を仮定した回帰を行うべきではなく,ベータ分布などを仮定した一般化線形モデルにすることを検討します.

また, `text`に外れ値(長崎県)があることなども考慮するべきですが,今回は線形回帰の事例ですのでそのまま進めてみます.
:::


:::

### 多重共線性(multi-colinearlity)

::: note
説明変数 $x_1, x_2, ..., x_n$ において, 特定の変数$x_i$が他の変数によって

$$
x_i = \sum_{i \neq j} \alpha_j x_j
$$

として,少なくとも1つが0でない$\alpha_j$によって表すことができる場合に,すなわち各変数が一次独立でなくなる場合に,説明変数に**完全な多重共線性**が成り立っているといいます.

このとき, 最小二乗法では, $y = \beta_0 + \beta_1 x_1 + beta_2 x_2 + ... + \beta_n x_n$を解くことができなくなるため,解が得られなくなります.

:::

例えば, $x_1 = \alpha_2 x_2$であったとすると,

$$
y = \beta_0 + (\beta_1 + \alpha_2 \beta_2) x_1 + \beta_3 x_3 + ... + \beta_n x_n
$$

となり,推定値が $\hat{\beta_{12}} = \hat{\beta_1} + \alpha_2 \hat{\beta_2}$であったとすると, $\hat{\beta_{12}}$となる$\hat{\beta_1}$と$\hat{\beta_2}$の組み合わせは無数にあるため,$\hat{\beta_1}$と$\hat{\beta_2}$を特定することができなくなります.

このような**完全な多重共線性**は,主に特定の説明変数を,他の説明変数の変形によって作成している場合に生じるため,変形した変数を利用するならば,変形前の変数はモデルに利用しないようにしましょう.

例えば, これから使い方を学習する`statsmodels`を利用して,あえて**完全な多重共線性**が存在するような重回帰分析を行ってみます.
(`statsmodels`や重回帰の実施については,後述するのでこの時点では変数の生成以外は意味を理解する必要はありません.)

~~~ py
import pandas as pd
import statsmodels.api as sm
import numpy as np

#乱数で目的変数と説明変数を生成
y = np.random.rand(100)
x_1 = np.random.rand(100)
df = pd.DataFrame({'y':y,'x_1':x_1})

#説明変数x_2をx_1から生成
df['x_2'] = df['x_1'] * 2

X = ['x_1','x_2']

#予測モデルを作成(重回帰)
X = sm.add_constant(df[X])
model = sm.OLS(df['y'],X)
result = model.fit()
print(result.summary())

"""
                            OLS Regression Results
==============================================================================
Dep. Variable:                      y   R-squared:                       0.000
Model:                            OLS   Adj. R-squared:                 -0.010
Method:                 Least Squares   F-statistic:                   0.01870
Date:                Mon, 08 Jul 2024   Prob (F-statistic):              0.892
Time:                        18:24:01   Log-Likelihood:                -11.013
No. Observations:                 100   AIC:                             26.03
Df Residuals:                      98   BIC:                             31.24
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const          0.4896      0.060      8.121      0.000       0.370       0.609
x_1            0.0028      0.021      0.137      0.892      -0.038       0.044
x_2            0.0056      0.041      0.137      0.892      -0.076       0.087
==============================================================================
Omnibus:                       19.023   Durbin-Watson:                   2.250
Prob(Omnibus):                  0.000   Jarque-Bera (JB):                5.214
Skew:                           0.170   Prob(JB):                       0.0738
Kurtosis:                       1.934   Cond. No.                     8.65e+16
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
[2] The smallest eigenvalue is 3.44e-32. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.
"""
~~~

`statsmodels`では,一応重回帰自体は実施できるものの,

> The smallest eigenvalue is 3.44e-32. This might indicate that there are
strong multicollinearity problems or that the design matrix is singular.

のように,多重共線性の存在を教えてくれます. (乱数を利用していることもあり)当然モデルの精度も非常に悪くなっているため, このモデルは利用できません.


一方で,

::: note
$$
x_i \approx \sum_{i \neq j} \alpha_j x_j
$$

で成り立つ(完全ではない/弱い)**多重共線性**という概念もあります. これは簡単に言えば, 説明変数間に相関関係が成り立つような場合を指しています.

変数間に相関がある場合,

- サンプルサイズによって,推定値が大きく変わる

- データによって推定値が大きく変わる

など, 利用するデータに対して推定値が不安定になると言われています. これはサンプルサイズを増やすことで対処できますが,一般には変数間の相関が強い場合には, 片方の変数は説明変数から除外することが望ましいとされています.
:::

それでは,先程の教育データの多重共線性をチェックしてみましょう.

~~~ py
# 多重共線性のチェック
# 散布図行列を作成してみる
pd.plotting.scatter_matrix(df, range_padding=0.2)
plt.show()

#ヒートマップで確認
sns.heatmap(df.corr()
            ,vmax=1
            ,vmin=-1
            ,annot=True)
plt.show()
~~~

![クロスプロット](/images/regression12.png)
![相関係数のヒートマップ](/images/regression13.png)

相関係数を確認すると,`board`と`text`の間に`0.56`の相関があることが分かります.
`0.56`程度であればそれほど影響はないのでそのままにしても構いませんが,練習として片方を除外してみます. `text`のほうが`math`との相関が強いので,`board`を除外したいところですが,市議の主張では電子黒板の普及率が問題と成っていたので`text`を除外します.

### 回帰分析の精度と判断

それでは, データの標準化,多重共線性を考慮して,実際に回帰分析を行ってみましょう.
`statsmodels`では, `sm.add_constant(説明変数)`の形で,定数を追加し,切片をモデルに追加することができます.

`sm.OLS(Y,X)`で線形回帰モデルインスタンスを宣言し, `.fit()`で推定を行います.
推定結果は`.summary()`で確認できます.


~~~ py
#相関が見られるため,textを除外
X.drop('text',axis='columns',inplace=True)

#予測モデルを作成(重回帰)
model = sm.OLS(Y,X)
result = model.fit()

#結果の表示
print(result.summary())

"""
==============================================================================
Dep. Variable:                   math   R-squared:                       0.004
Model:                            OLS   Adj. R-squared:                 -0.042
Method:                 Least Squares   F-statistic:                   0.07867
Date:                Mon, 08 Jul 2024   Prob (F-statistic):              0.924
Time:                        23:55:07   Log-Likelihood:                -66.101
No. Observations:                  47   AIC:                             138.2
Df Residuals:                      44   BIC:                             143.8
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       3.995e-15      0.149   2.68e-14      1.000      -0.300       0.300
pc             0.0493      0.162      0.305      0.762      -0.276       0.375
board          0.0560      0.162      0.347      0.730      -0.270       0.382
==============================================================================
Omnibus:                       13.834   Durbin-Watson:                   1.557
Prob(Omnibus):                  0.001   Jarque-Bera (JB):               14.866
Skew:                           1.170   Prob(JB):                     0.000591
Kurtosis:                       4.454   Cond. No.                         1.46
==============================================================================
"""
~~~

さて,推定結果には様々な情報が記述されていますが,どこをどのように見ればいいのでしょうか.

`statsmodels`の`.summary()`では中央部分に各説明変数の評価が記載されています.

- 各説明変数の評価

~~~ sh
==============================================================================
                 coef    std err          t      P>|t|      [0.025      0.975]
------------------------------------------------------------------------------
const       3.995e-15      0.149   2.68e-14      1.000      -0.300       0.300
pc             0.0493      0.162      0.305      0.762      -0.276       0.375
board          0.0560      0.162      0.347      0.730      -0.270       0.382
==============================================================================
~~~

それぞれいくつか重要なポイントを順に確認していきましょう.

::: note
- **回帰係数(coef)**
---

回帰係数は,説明変数毎に目的変数にどの程度影響力があるかを示したものになります.
:::

今回の事例では, 切片(`const`)が`3.995e-15`であり,`pc`,`text`が0のとき`math`が`3.995e-15`となります. `pc`の回帰係数が`0.0493`,`text`の回帰係数が`0.0560`であり,それぞれの説明変数が1変化する毎に,回帰係数の分だけ目的変数が変化します.

したがって,回帰式は,

$$
math = 3.995*10^{-15} + 0.0493 pc + 0.0560 text
$$

となり,いずれの変数も正の影響を持っており,市議の主張である電子黒板が普及するほどに成績が下がるという主張と逆の結果が出ています.値が標準化されているため,それぞれの回帰係数は相対的な影響力を表しており,実際の変化ではありません. `pc`の一人あたりの台数よりも,`text`の影響が強いことが分かります.

しかし,回帰係数だけを見て,回帰の結果を判断することはできません. 出てきた値が,信頼に値するか,利用可能であるかを他の値を用いて判断する必要があります.

::: note

- **P値(`P>|t|`)と95%信頼区間(`[0.025 0.975]`)**
---

それぞれの説明変数のP値(`P>|t|`)は,その説明変数に対する回帰係数が0である(影響がない)という仮説に対する仮説検定のP値を表しています. また, `[0.025      0.975]`はそれぞれ上側と下側の95%信頼区間を表しています.

有意水準5%の場合, `P値 < 0.025`で有意となり, 信頼区間が0をまたぎません. したがって,これらの値はいずれも同じ判断の基準となりますが,最近は論文などには両方載せることが主流です.
(仮説検定や区間推定,P値の意味などに関しては,統計学入門で詳細を扱っています.分からない人はそちらを履修しましょう.)

:::

今回の値を見てみると,いずれの説明変数も有意ではなく,区間推定の結果も0をまたいでいます. したがって,これらの回帰係数の推定値の解釈は, **｢今回のデータと説明変数の組み合わせでは, 電子黒板や電子教科書が学業成績に影響を与えるかどうかは判断できない.｣**ということになります.

::: warn
有意でない説明変数は, 基本的にモデルから除外することが望ましいので,仮により良いモデルを構築する場合には,別の変数やモデルの組み合わせを探すことになります.

それらの手法は,モデル選択などと呼ばれますが,モデル同士の比較の方法に関してはここでは扱わず,後の一般化線形モデルの章で扱うことにします.

:::

これまで各説明変数の影響を見てみましたが, 変数毎ではなく,モデル全体の評価はどのように行うのでしょうか.
`.summary()`の前半部分にはモデル全体での評価が記載されています.

- モデル全体の評価

~~~ sh
                            OLS Regression Results
==============================================================================
Dep. Variable:                   math   R-squared:                       0.004
Model:                            OLS   Adj. R-squared:                 -0.042
Method:                 Least Squares   F-statistic:                   0.07867
Date:                Mon, 08 Jul 2024   Prob (F-statistic):              0.924
Time:                        23:55:07   Log-Likelihood:                -66.101
No. Observations:                  47   AIC:                             138.2
Df Residuals:                      44   BIC:                             143.8
Df Model:                           2
Covariance Type:            nonrobust
==============================================================================
~~~


::: note
- **調整済決定係数(Adj. R-squared)**
---

自由度調整済み決定係数$Adj R^2$は,**モデルがどの程度当てはまっているかの基準**です.
基本的に$0 \leq Adj R^2 \leq 1$の値をとり,目安として$0.5 \leq Adj 𝑅^2$であればある程度予測できていると考えられます.

どんな散布図になっても回帰式自体は作成可能ですが,以下の左右どちらの予測値の方が信頼できそうでしょうか.

![R2](/images/regression14.png)

直感的には左の方が**回帰直線が実際の値にフィットしており**信頼できそうな気がしますね. $R^2$はその感覚を数値化したものになります.

![R2](/images/regression15.png)

$R^2$は,式から作られた直線と,実際のデータの点の距離の和であり,$Adj R^2$は,0から1に収まるように変換してものになります.
$AdjR^2$が1に近いほど, 式が点によく当てはまっていることを表します.
:::

今回のモデルを見てみると,`Adj. R-squared:-0.042`であり,全く予測精度が高くないことが分かります.

::: note
- **有意F (Prob (F-statistic))**
---

検定にはそれぞれの係数ごとにt検定,全体にF検定を用います.
有意F(Prob (F-statistic))は,モデル全体にF検定を実施した際のp値を表し,値が小さいほど回帰式が有意であることを表します(0.025 > F で有意).
(検定に関しては,検定の章を参照してください.)

以下の回帰式とデータを見るとどちらも,当てはまり方は同じだとしても, 左の方が信頼性が高いように感じます.

![Prob F](/images/regression16.png)

点が少ないと,他のデータを持ってきたら全然違うところに点が行く可能性が高まります.
この感覚=式が偶然の産物ではないかを検定したP値が有意Fです.
0.025 以下で, 信頼できるといえます.

:::

今回のモデルを見てみると`Prob (F-statistic):0.924`であり,回帰式全体は有意ではなく,このモデルは信頼できないことが分かります.


### 結果の図示

これまでは数値によって,モデルの結果を確認してきましたが,結果を図示することでより直感的に説明が可能になります. 回帰分析の結果を図示する方法は様々ありますが,ここでは結果の散布図と,密度プロットによって確認してみましょう.

各説明変数が,どの程度目的変数を説明しているかを確認する方法として,一つの説明変数を選んで散布図を作成し,予測値を重ねるという手法がよく用いられます.

`statsmodels`では,そのための`plot_partregress_grid`というメソッドが準備されています.

~~~ py
#回帰グラフの作成
from statsmodels.graphics.regressionplots import plot_partregress_grid
fig = plt.figure(figsize=(16,8))
plot_partregress_grid(result, fig=fig)
plt.show()
~~~

![3変数 Partial Plot](/images/regression17.png)

どの変数にもあまりフィットしていない様子が視覚的に把握できます.

また,モデルによって予測される値と実際の値のヒストグラムや密度プロットを比較することも良く行われます.
ここでは`seaborn`の`kdeplot`を利用してカーネル密度プロットを行ってみます.

~~~py
#予測結果の作成
pred = result.predict(X)
plt.figure(figsize=(16,8))
sns.kdeplot(pred, label = 'Predicted')
sns.kdeplot(Y, label = 'Actual')
plt.title('Actual/Predicted')
plt.xlabel('math')
plt.ylabel('Density')
plt.legend()
plt.show()
~~~

![3変数 Density Plot](/images/regression18.png)

実測値と予測値が全く違う分布をしており,予測がうまく行っていないことが視覚的に把握できます.

結論として, 今回のモデルでは市議の主張に対しては,否定も肯定もできないが,市議の根拠とするデータでは,市議の主張が導かれないということになりました.

実際に,どのような影響があるかを調べるためには,データかモデルのいずれかを変えて,説明可能なモデルを構築する必要があります.


::: note

- 演習問題

数学の成績と関連のありそうなデータをe-statから複数探し,重回帰分析を行い,その結果を解釈してください.

:::


# 一般化線形モデル(執筆中)
<details>
    <summary> 開く/閉じる </summary>

</details>



</details>

# 教師あり/なし学習 (執筆中)

<details>
    <summary> 開く/閉じる </summary>


それぞれ様々な手法があり,使い分ける必要があります

教師あり学習
* 回帰
  * 重回帰ロジスティック回帰
* 決定木分析
* k-近傍法
* サポートベクターマシン
* ニューラルネットワーク
  *  パーセプトロン,畳込み,再起,etc

教師なし学習
* クラスタリング
  * 階層(ウォード法
  * 非階層(k-means法
* 主成分分析
* 確率密度推定

<br>

P3の図

### 主成分分析(Principle component analysis)
* 多数のデータが有る際に, それら全てを使うことは大変です.
  * 多次元になると可視化が困難
  * データの説明が困難
  * モデル化が困難
  * 計算が困難
* 主成分分析
  * 多数の変数の持つ情報を損なわずに圧縮する技術(次数削減)であり,予測モデル構築の前処理としてもよく使われる.
  * 主成分の分散が最大になる軸を探し,その軸に直行する軸の中で分散が最大になる軸を探していく.

<br>

P4の図

非線形SVMで天気を当ててるときに選ぶべき説明変数は?
* 先週は電力データで休日かどうかを当てることを考えたが,今度は天気を当ててみよう.
* Lightning,Lamp,Power,Airがある中でどのように説明変数を選ぶきだろうか.
* 休日と同じ変数だと,正解率はあまり良くない.
* 変数を2つ選ぶとして,今回は 4C2 = 4*3/2 = 6通りだが, 変数が増えればその組み合わせは莫⼤になる.

<br>

主成分を使って天気を当てよう.
電力データの次元削減をして, その主成分で天気を当てる非線形SVMを実行してみよう.
第1成分と第2成分で,情報の85%以上を説明できることが分かる.この2成分を利用して,SVMを実行してみる.

```python
# -*- coding: utf-8 -*-
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn.model_selection import train_test_split
# svmのパッケージ
from sklearn import svm
#主成分分析
from sklearn.decomposition import PCA
#4つある電力データを主成分分析で次元削減してみる
# データを読み込み
df = pd.read_csv("./data/energy_data.csv")
# 主成分分析のターゲットとなる電力データを抽出
target = df[['Lighting', 'Lamp', 'Power', 'Air']]
#データを標準化します
# apply メソッドは,DataFlame全体に与えた関数を適用
# axis = 1 行に対して
# axis = 0 列に対して
target = target.apply(lambda x: (x-x.mean())/x.std(), axis=0)
"""
ラムダ式を使わなければ
def standardization(x):
return (x-x.mean())/x.std()
target = target.apply(standardization(), axis=1)
"""
#主成分分析の実行
pca = PCA()
pca.fit(target)
# 寄与率
# その主成分で情報量のどの程度を説明できているか
print('第1主成分の寄与率', pca.explained_variance_ratio_[0])
print('第2主成分の寄与率', pca.explained_variance_ratio_[1])
#主成分ベクトル
feature = pca.transform(target)
# データを主成分空間に写像
# n列目が第n主成分空間への写像
feature = pca.transform(target)
plt.scatter(feature[:, 0], feature[:, 1], alpha=0.8, c=list(df['weather']))
plt.grid()
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()
```

```
第1主成分の寄与率 0.5903597077929026
第2主成分の寄与率 0.27288683569579475
```

![](19_weather.png)

<br>

P6の図

説明変数に第1主成分と第2主成分を利用して,非線形SVMを実行(図示のプログラムはほぼ同じなので省略) .
TestDataは,それほど良くないが, TrainDataに関しては,かなり良く予測できている.

```python
#------------------------------------------------------------------
# ここからSVMに適用
#------------------------------------------------------------------
# 主成分をDFに追加
df['PC1'] = feature[:,0]
df['PC2'] = feature[:,1]
#説明変数を主成分とする
X = df[['PC1','PC2']]
#被説明変数
y = df['weather']
#トレーニングデータの作成
(train_X, test_X ,train_y, test_y) = train_test_split( X
, y
, stratify= y
, random_state = 0)
# SVMオブジェクトを定義
clf = svm.SVC(kernel='rbf')
#学習
clf.fit(train_X, train_y)
#結果の表示
print('正解率(train):', clf.score(train_X,train_y))
print('正解率(test):', clf.score(test_X,test_y))
```

```
第1主成分の寄与率 0.5903597077929026
第2主成分の寄与率 0.27288683569579475
正解率(train): 0.9130434782608695
正解率(test): 0.625
```

<br>

P7の図

### k-means法
最も広く使われているクラスタリング手法(データを類似度の高いグループに分ける手法). データをk個のグループに分割する場合は以下の手順で行われる.事前にクラスター数を指定する必要がある.
1. 入力データをプロットする
2. ランダムにk個の点をプロットする
3. 各ランダム点を,クラスター1,クラスター2,…,クラスターkの重心点とみなす.
4. 入力データの各点について,k個の重心点の中で最も近いものを選び,そのクラスターに分類する.
5. クラスター毎に重心を計算する.
6. 5.で定めたk個の重心を新しいクラスターの重心とする.
7. 4-6を設定した上限回数か,重心の移動距離が十分に小さくなるまで繰り返す.

<br>

P8の図

 これまでのデータはクラスタリングにあまり適していないので新しく配られたデータenergy.csv をクラスタリングしてみる.
* energy.csvは, 5~6⽉の1号館,研究館,体育館のデータを結合したもの.
* 建物を適切にクラスタリングできるかやってみよう.

<br>

k-means法で電力データの建物をクラスタリング
本来の教師なし学習では,事前にクラスターが分からないが,ここでは練習用として,実際のクラスタと,k-means法による結果を比較してみる.

```python
# -*- coding:utf-8 -*-
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
# 1号館, 研究館, 体育館のデータをk-means法でクラスタリングしてみる
#データの読み込み
#単位が同じなので,標準化は必要ない
df = pd.read_csv('./data/energy.csv')
print(df)
# まずは図示してみる
# 通常は事前にクラスターが分からないことに注意
plt.scatter(df['Lighting'], df['Lamp'], alpha=0.8, c=list(df['Building']))
plt.grid()
plt.title('raw data')
plt.xlabel('Lighting')
plt.ylabel('Lamp')
plt.show()
#k-means法で分類
# init = 'random' とすると kmeans法になる
# init を指定しないとk-means++という手法になる
# k-means++ は 初期の重心を広範に取る手法でk-meansより安定した結果が得られる
# 分割するクラスタ数は3に設定
kmeans = KMeans(init='random', n_clusters=3)
#重心を計算
kmeans.fit(df[['Lighting','Lamp']])
prediction = kmeans.predict(df[['Lighting','Lamp']])
print(prediction)
# 結果を図示
# 通常は事前にクラスターが分からないことに注意
plt.scatter(df['Lighting'], df['Lamp'], alpha=0.8, c=prediction)
plt.grid()
plt.title('prediction')
plt.xlabel('Lighting')
plt.ylabel('Lamp')
plt.show()
```

P9の結果載せる

<br>

P10の図

### 階層クラスタリング
似たデータ点を集めて一つの点とみなすことを全体が一つの点になるまで繰り返すクラスタリング手法を階層クラスタリングといいます(k-means法は非階層クラスタリング).
｢似ている｣ことを表す概念に様々な距離を用います.
* ユークリッド距離
* マハラノビス距離
* コサイン類似度,etc…

階層クラスタリングは,全てのデータ点ごとの距離を計算するのであまり大きなデータには使えません(時間がかかる)
一方で,デンドログラムという結合順序を表す図を得ることで,直感的な理解が可能になります.

<br>

ユークリッド距離
* ユークリッド距離
一般的によく用いられる距離.ピタゴラスの定理で求められる.

数式

* 標準化ユークリッド距離
ユークリッド距離を標準化したもの. 各要素の重みをなくしたユークリッド距離.

数式

<br>

コサイン類似度
* n次元ベクトルの向きの類似性をcosθで表す.

数式

θ=0° → cosθ=1 ∶同じ向き
θ=90° → cosθ=0 ∶直行
θ=180° → cosθ=−1:逆向き

<br>

P13の図

クラスター同士の距離
階層クラスタリングなどでは,複数の点からなるクラスター間の距離
をどのように定義するかによっても,結果が異なります.
* 最短距離法
2つのクラスターのデータ間距離の最小のものをクラスター間の
距離とする.
計算量が少ないが,クラスターが帯状になりやすい.
* 群平均法
各クラスター同士の全てのデータの組み合わせの距離を測りそ
の平均をクラスター間距離とする.
* ウォード法
①各クラスターを結合した場合の重心(平均)と各クラスターの データとのユークリッド距離
②もともとのクラスターの重心と各クラスターのデータとのユークリッド距離
① - ② の値を距離とする.
計算量は多いが,精度が良い.

<br>

階層クラスタリングで電力データの建物をクラスタリング
k-means法と同じ分類を階層クラスタリングで行ってみる.

```python
# -*- coding:utf-8 -*-
import pandas as pd
import matplotlib.pyplot as plt
# 階層クラスタリング
from scipy.cluster.hierarchy import dendrogram, linkage, fcluster
# 1号館, 研究館, 体育館のデータを階層的クラスタリングしてみる
#データの読み込み
#単位が同じなので,標準化は必要ない
df = pd.read_csv('./data/energy.csv')
print(df)
# データが多すぎるとデンドログラムを見ても良くわからないので,
# 各建物10個程度に減らす
build1 = df[df['Building'] == 0].head(10)
research = df[df['Building'] == 1].head(10)
gym = df[df['Building'] == 2].head(10)
df = pd.concat([build1,research,gym])
df.index = range(0,30)
# まずは図示してみる
# 通常は事前にクラスターが分からないことに注意
plt.scatter(df['Lighting'], df['Lamp'], alpha=0.8, c=list(df['Building']))
plt.grid()
plt.title('raw data')
plt.xlabel('Lighting')
plt.ylabel('Lamp')
plt.show()
```

P14の結果載せる

<br>

階層クラスタリングで電力
データの建物をクラスタリ
ング
デンドログラムで各点がどのように統合されているかを確認

```python
"""
クラスタリングの実行
method で データの結合の方法を指示します
･ average 重みのない平均距離
･ centroid 重みのない重心までの距離
･ complete 最大距離
･ median 重みのある重心までの距離
･ single 最小距離
･ ward 内部平方距離
･ weighted 重みのある平均距離
metric で距離の測り方を選択します
・euclidean ユークリッド距離
・cosine コサイン類似度
・correlation 相関係数
・canberra キャンベラ距離
・chebyshev チェビシェフ距離
・cityblock 都市ブロック距離
・hamming ハミング距離
・jaccard Jaccard係数
"""
res = linkage(df[['Lighting','Lamp’]]
 , method = 'average’
 , metric = 'euclidean' )
# デンドログラムの図示
dendrogram(res)
plt.title("Dedrogram")
plt.ylabel("Threshold")
plt.show()
```

P15の結果載せる

<br>

階層クラスタリングで電力
データの建物をクラスタリ
ング
最後に結果を確認.データの数が違うので一概には言えないが,
k-meansでは上手くいっていなかった点も上手く分類できている.
```python
# クラスターの数(t)を指定して,どのクラスターにそれぞれが属するかを得る
clusters = fcluster(res, t=3, criterion='maxclust')
print(clusters)
plt.scatter(df['Lighting'], df['Lamp'], alpha=0.8, c=clusters)
plt.grid()
plt.title('prediction')
plt.xlabel('Lighting')
plt.ylabel('Lamp')
plt.show()
```

P16の結果載せる

<br>

## 自然言語処理(NPL,Natural Language Processing)

自然言語処理の流れ
文章
___
トークン化(トークナイザー)
* 文を適当な単位に分割すること
* 分割によって得られた文の構成要素をトークンと呼ぶ
* 方法としては
  * 単語分割
    * 単語に分割する
    * 日本語の場合MeCabやSudochi, Jumanなどの形態素解析ツール(品詞や活用で分類)が有名
  * 文字分割
  * サブワード分割
    * 文字分割を更に分割する (東京タワー → 東京 + タワー)
    * BERTはこれを採用
___
言語モデルによる処理
* 文章の出現しやすさを同時確率でモデル化
* p(私はパンを食べた) > p(私は家を⾷べた) ← “⾷べた” と “パン” が同じ⽂章に出現しやすい
* p(私はパンを食べた)> p(私にパンを食べた) ← “パンを⾷べた” と “は” が同じ⽂章に出現しやすい
* P(w|c)の条件の部分(c)を⽂脈と呼ぶ
* この学習にニューラルネットワークを利用
___
ベクトル化(分散表現の作成)
* トークンに対応付けたベクトル(分散表現)を作成.
___
クラスタリング
* 分散表現をクラスタ数に次元圧縮し, モデルとクラスタのラベルの損失関数を最⼩化する.

<br>

形態素解析
* 形態素解析
  * 文章を最小の意味を持つ言語単位（形態素）に分割し,それぞれの品詞を識別する処理
  * テキスト解析の前段階の処理として行われることが多い
* 手順
  * データの作成
    * PDFなどから直接処理することも可能だが,.txtデータにすると楽
    * 入力例文: 「太陽が昇る東の空が美しい」
  * テキストの前処理
    * 不要なスペースや記号を除去しテキストを処理しやすい形に整理
  * 形態素への分割
    * 文章を形態素と呼ばれる最小単位に分割
    * 「太陽」,「が」,「昇る」,「東」,「の」,「空」,「が」,「美しい」
  * 品詞のタグ付け
    * 分割された形態素に品詞情報を付与
    * 「太陽」: 名詞
    * 「が」: 助詞
    * 「昇る」: 動詞
    * 「東」: 名詞
    * 「の」: 助詞
    * 「空」: 名詞
    * 「が」: 助詞
    * 「美しい」: 形容詞

  <br>

形態素解析
* Pythonで利用可能な(日本語)形態素解析パッケージ
  * Mecab
    * 日本語のオープンソース形態素解析システム
    * pythonのライブラリとしてはmecab-python3
  * janome
    * Pythonで書かれた日本語形態素解析器
    * MecabよりインストールなどがPythonに最適化されており,インストールなどが楽
    * ただし,遅いので大規模な処理では余り使われない
* どっちでも良いが,Mecabを今回は使う
  * pip install mecab-python3==0.996.5
  * == 0.996.5 は新しいVerだとMacで利用するときに色々面倒くさいので古いバージョンを指定しています.
* ついでにWordCloudもInstallしておく.
  * pip install wordcloud

<br>

形態素解析 → ワードクラウドを試してみる.   (cf. https://rinsaka.com/python/nltk/05-wordcloud.html)

* 千葉商科大学の理念(https://www.cuc.ac.jp/about_cuc/outline/spirits/index.html )の
WordCloudを作成する.
* テキスト部分をコピペ→UTF-8のcuc.txtとしてdataフォルダに保存

P21の図

<br>

形態素解析 → ワードクラウドを試してみる.  (cf. https://rinsaka.com/python/nltk/05-wordcloud.html)

```python
from wordcloud import WordCloud
import os
import re
import MeCab as mc
def strip_CRLF_from_Text(text):
"""テキストファイルの改行，タブを削除し，形態素解析を実行する．
改行前後が日本語文字の場合は改行を削除する．
それ以外はスペースに置換する．
"""
# 改行前後の文字が日本語文字の場合は改行を削除する
plaintext = re.sub('([ぁ-んー]+|[ァ-ンー]+|[\\u4e00-\\u9FFF]+|[ぁ-んァ-ンー\\u4e00-\\u9FFF]+)(\n)([ぁ-んー]+|[ァンー]+|[\\u4e00-\\u9FFF]+|[ぁ-んァ-ンー\\u4e00-\\u9FFF]+)',
r'\1\3',
text)
# 残った改行とタブ記号はスペースに置換する
plaintext = plaintext.replace('\n', ' ').replace('\t', ' ')
return plaintext
def mecab_wakati(text):
"""
MeCabで分かち書き．
ただし品詞は名詞だけに限定．
"""
t = mc.Tagger()
# t = mc.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')
node = t.parseToNode(text)
# print(node)
sent = ""
while(node):
# print(node.surface, node.feature)
if node.surface != "": # ヘッダとフッタを除外
word_type = node.feature.split(",")[0]
# 名詞だけをリストに追加する
if word_type in ["名詞"]:
sent += node.surface + " " # node.surface は「表層形」
# 動詞（の原型），形容詞，副詞もリストに加えたい場合は次の２行を有効にする
#if word_type in [ "動詞", "形容詞","副詞"]:
# sent += node.feature.split(",")[6] + " " # node.feature.split(",")[6] は形態素解析結果の
「原型」
node = node.next
if node is None:
break
return sent
# テキストファイル読み込み
f = open('data/cuc.txt', encoding='utf-8')
raw = f.read()
f.close()
print(raw)
text = strip_CRLF_from_Text(raw)
print(text)
sent = mecab_wakati(text)
print(sent)
# フォントの保存先を指定する（環境によって書き換えてください）
#font_path = "C:\\WINDOWS\\FONTS\\MEIRYO.TTC" ## Windows 版はこちら
font_path = "/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc" ## Mac 版はこちら
# WordCloud画像を生成する
wc = WordCloud(max_font_size=36
,font_path=font_path
,background_color='white').generate(sent)
wc.to_file("cuc.png")
```
P22の結果載せる

<br>

WordCloud
* 今回は,頻出単語(文章中の出現回数)ほど大きく文字が表示されている.
* 重要度などの情報を与えることも可能だが,重要度を測るには他の手法が必要.
* Bertの固有単語抽出など

```python
import matplotlib.pyplot as plt
import japanize_matplotlib
from wordcloud import WordCloud
# 日本語のテキストデータ（基本計画の内容を代表するキーワード）
#最初に出てきたものほど大きく表示される
#こういったリストを別の手法で作成することが必要
japanese_text = """
公的統計, 経済統計, 国民経済計算, データ, 統計改革, 統計ニーズ, 国際比較, 統計データ, PDCA, 統計リソース,
デジタル化, 情報基盤, 統計調査, データ審査, 品質管理, ユーザー視点, 効率化, 統計作成, 政策立案, 統計行政,
報告者, 利便性, 統計システム, 可視化, 報告, 支援, モニタリング, 評価, 総合的, 品質表示, 更新, 保存, 管理,
正確性, 新たな統計, 対応, 負担軽減, 変化, 進展, 進化, 計画期間, 目標, 統計委員会, 建議, 方策, 様々な観点,
社会経済, 統計作成方法, 仕様, 整備, デジタル経済, 実態把握, 報告者負担, 利用者利便性, 統計ユーザー,
エラーチェック, 汎用ツール, 改善, 効率的, 統計リソース, 統計改革, 防止, 確保, 第Ⅳ期基本計画, 第Ⅲ期基本計画,
相互関連, 整合性, 進め方, 効果的, 活用
"""
# 日本語フォントの使用
font_path = "/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc" #Mac
#font_path = "C:\\WINDOWS\\FONTS\\MEIRYO.TTC" #Win
# 日本語ワードクラウドの生成
japanese_wordcloud = WordCloud(width=800
, height=400
, background_color='white'
, font_path=font_path
, colormap='viridis').generate(japanese_text)
# 日本語ワードクラウドの表示
plt.figure(figsize=(10, 5))
plt.imshow(japanese_wordcloud, interpolation='bilinear')
plt.axis('off')
plt.show()
```

P23の結果載せる

<br>

BERTについて

BERT
* Bidirectional Encoder Representations from Transformers
* 2018年にGoogleから発表されたニューラル言語モデ ル.• 後継にELECTRAというのがある(こっちのほうが軽く高速) • SEOなど検索エンジンの精度向上に利用 • 文章をトークンに分割したものを入力として受けて,それぞれのトークンに対応するベクトルを出力する
* 既存の方法では, トークン間の関係に関してあ まり上手く処理できなかった.
* 文章に現れるトークンに応じて各トークンの関係を決めるAttention
(注意機構)を散り入れている.
* より深く文脈を考慮したトークンの分散表現を得ることができる

P25の図

<br>

穴埋め
* BERTやchatGPTが行っているのは文章の穴埋め.
* 私はりんごを[MASK]
* MASKの部分に入る文字列の確率計算をしている.
* 私はりんごを行った ← 確率低い
* 私はりんごを食べた ← 確率高い
* コーパス(文例集)から教師あり学習をして,あらゆる語彙の連なりやすさの確率を計算している.
* Wikipediaやスクローリングして取得した文章
* � 食べた 私はりんごを = コーパス中の頻度(私はりんごを食べた)
コーパス中の頻度(私はりんごを)
* � 食べた 私はりんごを = コーパス中の頻度(私はりんごを行った)
コーパス中の頻度(私はりんごを)
* 単語間の確率による距離をベクトル表現する(学習)

<br>

学習
* BERTは二段階の学習を行っている.
* 事前学習
* (日本語など)言語全般に関して大規模なテキストコーパス(Wikipediaなど)で学習
* こちらのモデルがGoogleによって公開されている(ライブラリとして利用可能)
* ファインチューニング
* 事前学習済みのBERTモデルをタスク(穴埋め,ラベリング,校正などの用途)によって追加学習さ
せる.
* タスクに関連した新たなデータセットが必要
* ラベリングをするのであれば,ラベル付けされた教師データが必要

<br>

データセット
* BERTの利用のためには,目的に応じたデータセットが必要
* 日本語データセットとして有名なもの
  * Twitter日本語評判分析データセット
  * https://www.db.info.gifu-u.ac.jp/sentiment_analysis/
  * Twitterの商品に関するポジティブ,ネガティブ,ニュートラルのラベリングデータ
* SNOW D18日本語感情表現辞書
  * 日本語を48の勘定に分類
  * 安らぎ,楽しさ親しみ,尊敬・尊さ,感謝,気持ちが良い,誇らしい,感動,喜び,悲しさ,寂しさ不満,切なさ,苦しさ,不安,憂鬱,辛さ,好き,嫌悪,恥ずかしい,焦り,驚き,怒り,幸福感,恨み,恐れ（恐縮等の意味で）,恐怖,悔しさ,祝う気持ち,困惑,きまずさ,興奮,悩み,願望,失望,あわれみ,見下し,謝罪,ためらい,不快,怠さ,あきれ,心配,緊張,妬み,憎い,残念,情けない,穏やか
* livedorニュースコーパス
  * ニュース記事をサイト別/ジャンル別に分類
* 有価証券報告書ネガポジデータセット
  * https://github.com/chakki-works/chABSA-dataset
  * TIS株式会社が公開している上場企業の有価証券報告書を用いて作成されたマルチラベルのネガポジデータセット
  *  ネガティブ, ポジティブ, ニュートラルの3ラベル

<br>

BERT利用の流れ

トークン化
* MeCab(Fugashi);PythonのMeCabライブラリ
* iadic ; MeCab用のシソーラス
___
ベクトル化
___
学習
* FacebookのPyTorchを利用とニュラルネットワーク言語モデルライブラリであるTransformersを利用
* 事前学習
  * 大規模な文章コーパスを用いて汎用的な言語のパターンを学習する
  * BERTでは, ある単語を周りの単語を予測するタスクを学習
    * ランダムに選ばれた15%のトークンをMASKという特殊トークンに置き換え, その位置のトークンを予測する(マスク付き⾔語モデル)
    * ⼊⼒された2つの⽂が連続したものであるかを学習( Next Sentence Prediction)
  * ⼤抵はこの部分はすでに⾏われたモデルを利⽤する
    * ⽇本語で有名なものは東北⼤がWikipediaの⽇本語記事ので学習したもの(cl-tohoku/bert-base-japanese-whole-word-masking)
* ファインチューニング
  * 事前学習を利⽤して,個別のタスクのラベル付きデータからタスクに特化した学習を⾏う.

<br>

P30の図

文章の穴埋め
*  今日は[MASK]に行く → MASK部分を予測
___
文章分類
* ポジティブ,ネガティブなどに文章を分類(マーケティングやレ
コメンドなど
___
マルチラベル文章分類
* ポジティブかつネガティブなど
___
固有表現抽出
* 文章から人名・組織名といった固有名詞を抽出する
___
文章校正
* Grammary的な
___
データの可視化と類似文章検索
* PCAからクラスタリング

<br>

人工知能の歴史
cf. 寺野隆雄,生成系AIの歴史・原理・現状,千葉商科大学 2023年 第1回FD 「生成系AIに関するFD」 ,2023/05/18

第1次AIブーム
* 1956年: ダートマス会議でスタート
* 汎⽤問題解決機(問題:現状と理想との差異)
* 問題の解決⼿法をプログラム
* 1960年代はじめ:機械翻訳の失敗で収斂
___
第2次AIブーム
* 1980年代はじめ:エキスパートシステム,機械翻訳
* 沢山の知識を詰め込む
* 1980年代はじめ:第5世代コンピュータープロジェクト
* 1990年代はじめ:知識獲得・脆弱性で収斂
___
第3次AIブーム
* 2010年から現在:ANN(ArYfical Neural Network)の復活
* 検索エンジン分野で発達
* ゲームでの成功(チェス,将棋,囲碁)
* パターン認識での成功(画像解析,音声解析,etc…)
___
第4次AIブーム
* 現在:生成系AI
* 言語・絵画・音声

<br>

GPU計算とCPU計算
* PCにおける計算は通常CPUによって行われます.
* これまでに実行してきたPythonプログラムは全てCPUを用いた計算.
* 一方でGPU(Graphics Processing Unit,画像処理装置,いわゆるグラボ)を利用した計算も可能
* ニューラルネットワークモデルは,GPUを用いて計算が行われることが多い.

CPU計算の特徴
* 汎⽤性: CPUは汎⽤的な計算に最適化. 様々な種類のタスクを処理可能.
* シリアル処理: CPUは⼀度に⼀つのタスクを処理するシリアル処理に適している
* 少数のコア: CPUは⽐較的少数のコアを保有.それぞれのコアが⾼速で複雑な計算を実⾏可能

GPU計算の特徴
* 特化した計算: GPUはグラフィックス処理や機械学習のような特定の種類の計算に最適化
* 並列処理: GPUは並列処理に特化.同時に多数の計算を実⾏可能
* 多数のコア: GPUには数百から数千のコアを保有.⼀度に多数の簡単な計算を実⾏可能

<br>

GPU計算でマルチラベル分類
* この講義ではGPU計算はローカル環境(それぞれのPC)で行いません.
  * WindowsとMacで環境構築が全く異なり面倒
  *  学生のPCのGPUが貧弱
  *  処理が重く,時間がかかるのでノートPCには不向き
* なので,Googleの提供しているPython計算用のSaaS Google Colaboratryを利用します.
* 今回はColaboratry上で, 文章のマルチラベル分類を試してみます.
*  マルチラベル分類
   * 選択肢の中から復数のカテゴリーを選ぶ分類
   * multi-hot ベクトル
    文章が属すカテゴリーを表す0,1の2値ベクトル
* 今回は有価証券報告書データを利用して[ネガティブ,ニュートラル,ポジティブ]に分類
  * ネガティブと判定されると[1,0,0]
  * ニュートラルと判定されると[0,1,0]

<br>

Googleのアカウントを作る

P34の図

<br>

Google Driveの設定をする
* データなどはGoogleのクラウドストレージに保存されます.
* Google Drive上にこの授業のフォルダとデータを入れるフォルダ
を作ります

P35の図

<br>

Google Driveの設定をする

P36の図

<br>

Google Driveの設定をする
今回の授業用のフォルダをクリックして中に入ります.Teamsで配布された multi_label.ipynb をドラッグアンドドロップでフォルダに保存します.

P37の図

<br>

Google Colaboratoryが利用できるようにする
* Google のアカウントにGoogle Colaboratoryをインストールします.

P38の図

<br>

Google Colaboratoryの利用
* sldsフォルダのmul9_label.ipynb をダブルクリックするとGoogle Colaboratoryが開く.
* 右上の設定をGPU計算に変更する

P39の図

<br>

Google Colaboratoryの利用
* Runtime → run all でプログラムが動く

P40の図

<br>

Google Colaboratry の利用
出てきたらGoogle Driveのアクセスを許可する

P41の図

<br>

</details>



# 画像認識

## 顔による年齢識別

事例として顔画像からの年齢識別を行ってみましょう. データとして,16歳から62歳までの2,000人の有名人の160,000以上の画像が含まれるデータセット[Cross-Age Celebrity Dataset (CACD)](http://bcsiriuschen.github.io/CARC/)を用います.

![The dataset metadata](/images/CACD.png)

`The dataset metadata only can be downloaded`をクリックしてメタデータを, `Original face images (detected and croped by openCV face detector) can be downloaded`をクリックして画像データをダウンロードしてください(3Gあるので通信環境に注意).

`CACD2000.tar.gz`は展開して,`celebrity2000_meta.mat`とともにプログラムを配置するディレクトリ内の`data`ディレクトリに保存しておきましょう.

### 画像ファイルの形式

機械学習において利用されるラベル付き画像データの形式はいくつかあるが,CACDのような`.mat`ファイル,画像とCSVなどのラベルの組み合わせ,ラベル名フォルダ別の画像ファイルなどのパターンが存在する. いずれにも対応できるようにしておく必要があるが, この資料では最も単純な最後のラベル別に名前がつけられたフォルダに保存された画像ファイルを扱う.

先ほどダウンロードした`celebrity2000_meta.mat`は,メタデータのみが含まれており,画像は別になっています.メタデータに従って,年齢別に画像をフォルダに保存してみましょう.

~~~ sh
> ls
face_image.py
data

> ls data
CACD2000
celebrity2000_meta.mat
~~~


::: note
- `.mat`ファイル
---
    - MATLABのファイル
    - 基本的には `scipy` を利用して読み込む.
    - フォーマット形式がMATLAB `v7.3`の場合には,`HDF5`を扱うライブラリ`h5py`を利用する.
    - `HDF5(Hierarchical Data Froamt version 5)`はディレクトリ構造に似た階層型のデータフォーマット
:::


まずは,`h5py`を利用して`celebrity2000_meta.mat`を読み込み,中身を確認してみましょう.

::: warn
必要なライブラリとして`scipy`,`h5py`と画像処理用の`pillow`を`pip install`しておいてください.
:::

~~~ py
import h5py #HDF5を扱うライブラリ
from PIL import Image #画像の表示/保存/書き込みなどを扱うライブラリ
import os
import numpy as np
import scipy.io

#画像データの保存先
image_dir = 'data/CACD2000'

# .matファイルの読み込み（古い形式の場合）
## 辞書型として読み込まれる
file = scipy.io.loadmat('data/celebrity2000_meta.mat')

# 辞書のKeyを表示する
print('keys:',file.keys())
# >>> dict_keys(['__header__', '__version__', '__globals__', 'celebrityData', 'celebrityImageData'])

#celebrityImageDataの確認
print(file['celebrityImageData'])

"""
[[(array([[53],
         [53],
         [53],
         ...,
         [23],
         [23],
         [23]], dtype=uint8), array([[   1],
         [   1],
         [   1],
         ...,
         [2000],
         [2000],
         [2000]], dtype=uint16), array([[2004],
         [2004],
         [2004],
         ...,
         [2013],
         [2013],
         [2013]], dtype=uint16), array([], shape=(0, 0), dtype=uint8), array([[ 1],
         [ 1],
         [ 1],
         ...,
         [50],
         [50],
         [50]], dtype=uint8), array([[1],
         [1],
         [1],
         ...,
         [0],
         [0],
         [0]], dtype=uint8), array([[1951],
         [1951],
         [1951],
         ...,
         [1990],
         [1990],
         [1990]], dtype=uint16), array([[array(['53_Robin_Williams_0001.jpg'], dtype='<U26')],
         [array(['53_Robin_Williams_0002.jpg'], dtype='<U26')],
         [array(['53_Robin_Williams_0003.jpg'], dtype='<U26')],
         ...,
         [array(['23_Katie_Findlay_0011.jpg'], dtype='<U25')],
         [array(['23_Katie_Findlay_0012.jpg'], dtype='<U25')],
         [array(['23_Katie_Findlay_0013.jpg'], dtype='<U25')]], dtype=object))                ]]
"""
# 7個目に画像のファイル名が入っているので
# celebrityImageDataから画像ファイル名を抽出
image_data = file['celebrityImageData']
jpg_files = [str(image_name[0][0]) for image_name in image_data[0][0][7]]

# 抽出された.jpgファイル名のリストを上から10個表示
print(jpg_files[:10])

# 画像データの取得と表示
## 名前データを利用して画像をいくつか開いてみます.
for n in jpg_files[:10]:
    img_path = os.path.join(image_dir, n)  # パスを結合し、ファイル名を取得

    if os.path.exists(img_path):
        img = Image.open(img_path)  # 画像ファイルを開く
        img.show()  # 画像を表示
    else:
        print(f"Image file not found: {img_path}")
    #>>> 画像が表示されます
~~~




::: warn
- HDF5の利用例



`CACD`データのうち一番上の`The dataset metadata and features used in this paper`からダウンロードできる`celebrity2000.mat`は,`HDF5`のデータとなっているため,`scipy`で読み込んでみるとエラーが出ます.

~~~ py
file = scipy.io.loadmat('data/celebrity2000.mat')
"""
Traceback (most recent call last):
  File "/Users/akagi/Desktop/face_image.py", line 86, in <module>
    file = scipy.io.loadmat('data/celebrity2000.mat')
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/akagi/.pyenv/versions/3.12.3/lib/python3.12/site-packages/scipy/io/matlab/_mio.py", line 226, in loadmat
    MR, _ = mat_reader_factory(f, **kwargs)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/akagi/.pyenv/versions/3.12.3/lib/python3.12/site-packages/scipy/io/matlab/_mio.py", line 80, in mat_reader_factory
    raise NotImplementedError('Please use HDF reader for matlab v7.3 '
NotImplementedError: Please use HDF reader for matlab v7.3 files, e.g. h5py

"""
~~~


今回は`celebrity2000_meta.mat`を利用するので必要ありませんが,試しに同じように画像を表示してみましょう.

<details>
    <summary> 詳細 開く/閉じる </summary>


`HDF5`は多重の辞書型ような構造をしており,`key`によってデータにアクセスできます.

~~~ py
import h5py #HDF5を扱うライブラリ
from PIL import Image #画像の表示/保存/書き込みなどを扱うライブラリ
import os
import numpy as np

# .matファイル(HDF5)の読み込み
with h5py.File('data/celebrity2000.mat', 'r') as file:
    # List all keys in the .mat file
    print('keys:',list(file.keys()))
    # >>> keys: ['#refs#', 'celebrityData', 'celebrityImageData']

    ## ラベルの確認
    print('DataKeys:',file['celebrityImageData'].keys())
    # >>> DataKeys: <KeysViewHDF5 ['age', 'birth', 'feature', 'identity', 'lfw', 'name', 'rank', 'year']>

    # 年齢データの確認
    print('age:',file['celebrityImageData']['age'])
    # >>> age: <HDF5 dataset "age": shape (1, 163446), type "<f8">
    print('age:',file['celebrityImageData']['age'][0])
    # >>> age: [53. 53. 53. ... 23. 23. 23.]

    # 名前データの確認
    print('name:',file['celebrityImageData']['name'])
    # >>> name: <HDF5 dataset "name": shape (1, 163446), type "|O">
    print('name:',file['celebrityImageData']['name'][0])
    # >>> name: [<HDF5 object reference> <HDF5 object reference> <HDF5 object reference>
    #... <HDF5 object reference> <HDF5 object reference>
    #<HDF5 object reference>]
    # ↑ <HDF5 object reference>は他のHDF5オブジェクトへの参照 #refs#に入っている.

    # nameデータを参照して表示
    name_references = file['celebrityImageData']['name'][0]
    names = []
    for ref in name_references:
        name = file[ref][()].tobytes().decode('utf-16')  # utf-16でデコード
        names.append(name)

    # 最初の10件の名前を表示
    print('names:', names[:10])
    # names: ['53_Robin_Williams_0001.jpg'
    # , '53_Robin_Williams_0002.jpg'
    # , '53_Robin_Williams_0003.jpg'
    # , '53_Robin_Williams_0004.jpg'
    # , '53_Robin_Williams_0005.jpg'
    # , '53_Robin_Williams_0006.jpg'
    # , '53_Robin_Williams_0007.jpg'
    # , '53_Robin_Williams_0009.jpg'
    # , '53_Robin_Williams_0010.jpg'
    # , '53_Robin_Williams_0011.jpg']

    # 画像データの取得と表示
    ## 名前データを利用して画像をいくつか開いてみます.
    for n in names[:10]:
        img_path = os.path.join(image_dir, n)  # パスを結合し、ファイル名を取得

        if os.path.exists(img_path):
            img = Image.open(img_path)  # 画像ファイルを開く
            img.show()  # 画像を表示
        else:
            print(f"Image file not found: {img_path}")
    # >>> 画像が表示される
~~~

同じ用にデータを抽出できることが確認できます.

</details>

:::


それでは,`celebrity2000_meta.mat`から年齢別にフォルダを分けて画像を保存してみます.年齢区分は,`10`,`20`,...,`100`としてみましょう. 画像ファイル名の先頭の数字が年齢を表しているので,そちらを利用しても構いませんが,せっかくなのでメタデータを利用してみましょう. 年齢は`image_data[0][0][0]`に入っているようです.

研究であれば画像データの枚数は多いほど良いですが, 今回は一通りの流れを体験してみることが目的なので学生の環境でも利用しやすいように各年代100枚だけコピーします.

~~~ py

import os
import shutil
import scipy.io
from collections import defaultdict

# 画像ディレクトリの設定
image_dir = 'data/CACD2000'
output_dir = 'data/sorted_images'

# .matファイルの読み込み
file = scipy.io.loadmat('data/celebrity2000_meta.mat')

# celebrityImageDataから年齢と画像ファイル名を抽出
image_data = file['celebrityImageData']
# 年齢情報
ages = image_data[0][0][0].flatten()
# 画像ファイル名
jpg_files = [str(image_name[0][0]) for image_name in image_data[0][0][7]]

# 年代ごとの画像カウント
age_group_counts = defaultdict(int)

# 年齢別のフォルダに画像をコピー（各年代最大100枚）
for age, jpg_file in zip(ages, jpg_files):
    age_group = (age // 10) * 10
    if age_group > 100:
        age_group = 100  # 100代以上は100代フォルダに保存

    # 各年代ごとに100枚までコピー
    if age_group_counts[age_group] < 100:
        folder_path = os.path.join(output_dir, f'{age_group}s')
        os.makedirs(folder_path, exist_ok=True)

        src_path = os.path.join(image_dir, jpg_file)
        dst_path = os.path.join(folder_path, jpg_file)

        shutil.copy(src_path, dst_path)
        age_group_counts[age_group] += 1
~~~

結果を確認してみます.

::: warn

Shell コマンドにおける`|` は`パイプ`といって `head -20`は先頭20個のみ
`|` の左側のコマンドによる標準出力を右側のコマンドに渡すことができます.

今回は`ls data/sorted_images/10s`で表示される結果の,先頭20個のみを表示しています.

:::

~~~ sh
> ls data/sorted_images
10s 20s 30s 40s 50s 60s
> ls data/sorted_images/10s |head -20
14_Aaron_Johnson_0001.jpg
14_Aaron_Johnson_0002.jpg
14_Adelaide_Kane_0001.jpg
14_Adelaide_Kane_0002.jpg
14_Adelaide_Kane_0003.jpg
14_Adelaide_Kane_0004.jpg
14_Adelaide_Kane_0005.jpg
14_Adelaide_Kane_0006.jpg
14_Adelaide_Kane_0010.jpg
14_Adelaide_Kane_0011.jpg
14_Adelaide_Kane_0013.jpg
14_Adelaide_Kane_0014.jpg
14_Adelaide_Kane_0015.jpg
14_Adelaide_Kane_0018.jpg
14_Adelaide_Kane_0019.jpg
14_Alex_Pettyfer_0004.jpg
14_Alex_Pettyfer_0005.jpg
14_Alex_Pettyfer_0007.jpg
14_Alex_Pettyfer_0008.jpg
14_Alex_Pettyfer_0009.jpg
~~~

データには10代から60代までのみが含まれていたようです. 各フォルダの中身を確認してもちゃんと保存できていることがわかりますね.


機械学習モデルの性能を評価するためには,学習に利用する訓練用データと,学習の結果を判定するテスト用データに分ける必要があります. 続いて,学習用とテスト用でフォルダに分割してみましょう.

学習データの分割には, 指定した割合でデータを分割してくれる`sklearn`の`train_test_split`を用います.

~~~ py
import os
import shutil
from sklearn.model_selection import train_test_split

data_dir = 'data/sorted_images'
output_dir = 'data/sorted_images_split'

# 画像ファイルのパスを収集し、年齢別に分類
age_groups = ['10s', '20s', '30s', '40s', '50s', '60s']
for age_group in age_groups:
    images = os.listdir(os.path.join(data_dir, age_group))
    train_images, val_images = train_test_split(images
                                               ,test_size=0.2 #2割をテスト用データにする
                                               , random_state=42)

    train_dir = os.path.join(output_dir, 'train', age_group)
    val_dir = os.path.join(output_dir, 'val', age_group)
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(val_dir, exist_ok=True)

    for image in train_images:
        shutil.copy(os.path.join(data_dir, age_group, image), os.path.join(train_dir, image))
    for image in val_images:
        shutil.copy(os.path.join(data_dir, age_group, image), os.path.join(val_dir, image))
~~~

以下のような形でデータが保存されていることを確認しましょう.

~~~ sh
data/sorted_images_split
├── train
│      ├── 10s
│      ├── 20s
│      ├── 30s
│      ├── 40s
│      ├── 50s
│      └── 60s
└── val
        ├── 10s
        ├── 20s
        ├── 30s
        ├── 40s
        ├── 50s
        └── 60s
~~~



### 画像認識の実施

::: note
- バッチ処理
:::

::: note
- エポック数
:::





yakagika