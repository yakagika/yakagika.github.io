<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">
        <meta name="viewport" content="width=device-width">

        <title>yakagika - 特別講義DS Ch13 教師あり/なし学習</title>

        <!-- Stylesheets. -->
        <link rel="stylesheet" type="text/css" href="../style.css?v=0">

        <!-- RSS. -->
        <link rel="alternate" type="application/rss+xml" title="yakagika" href="https://yakagika.github.io/rss.xml">

        <!-- Metadata. -->
        <meta name="keywords" content="yakagika Haskell ExchangeAlgebra">
        <meta name="description" content="Personal home page and blog of yakagika.">
        
        <!-- KaTeXのスタイルシートとJavaScriptのリンクを動的に挿入 -->
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
        <style>
            .katex-display {
                display: block;
                margin: 1em 0;
                text-align: center;
            }

            .katex .frac {
                vertical-align: baseline;
                -webkit-vertical-align: baseline; /* Safari用のベンダープリフィックス */
            }

            .katex .sqrt {
                vertical-align: baseline;
                -webkit-vertical-align: baseline; /* Safari用のベンダープリフィックス */
            }

            .katex .strut {
                height: 1em;
                -webkit-height: 1em; /* Safari用のベンダープリフィックス */
            }

            .katex .base {
                font-family: 'KaTeX_Main', 'Arial', sans-serif;
            }

            /* Safari専用のスタイルを追加 */
                @media screen and (-webkit-min-device-pixel-ratio:0) {
                    .katex {
                        line-height: normal !important;
                    }
                }
            @media not all and (min-resolution: .001dpcm) {
                @supports (-webkit-appearance:none) {
                    .katex {
                        line-height: normal !important;
                    }
                }
            }
        </style>
        
        <meta property="og:description" content="資料" />
    </head>
    <body>
        <div id="navigation">
            <h1>Contents</h1>
            <a href="../">Home</a>
            <a href="../posts.html">Blog</a>
            <a href="../lectures.html">Lecture</a>
            <a href="../research.html">Research</a>
            <a href="../contact.html">Contact</a>
            <!-- <a href="/cv.html">CV</a> -->
            <h1>Links</h1>
            <a href="http://github.com/yakagika">GitHub</a>
            <a href="https://researchmap.jp/k-akagi">researchmap</a>

        </div>

        <div id="content">
    <h1>特別講義DS Ch13 教師あり/なし学習</h1>
<div class="soft">
    資料<br />
    Published on 2024-03-29 under the tag <a title="All pages tagged 'datascience'." href="../tags/datascience.html">datascience</a>, <a title="All pages tagged 'statistics'." href="../tags/statistics.html">statistics</a>, <a title="All pages tagged 'python'." href="../tags/python.html">python</a>
</div>

<!-- 前後の章へのナビゲーション -->
<div class="chapter-navigation">
    <nav>
        
            <a class="nav-link prev" href="slds12.html">← Previous Chapter</a>
        
        
            <a class="nav-link next" href="slds14.html">Next Chapter →</a>
        
    </nav>
</div>

<br>

<div class="toc"><div class="header">Table of Contents</div>
<ul>
<li><a href="#教師ありなし学習-執筆中" id="toc-教師ありなし学習-執筆中"><span class="toc-section-number">1</span> 教師あり/なし学習 (執筆中)</a>
<ul>
<li><a href="#主成分分析principle-component-analysis" id="toc-主成分分析principle-component-analysis"><span class="toc-section-number">1.0.1</span> 主成分分析(Principle component analysis)</a></li>
<li><a href="#k-means法" id="toc-k-means法"><span class="toc-section-number">1.0.2</span> k-means法</a></li>
<li><a href="#階層クラスタリング" id="toc-階層クラスタリング"><span class="toc-section-number">1.0.3</span> 階層クラスタリング</a></li>
<li><a href="#自然言語処理nplnatural-language-processing" id="toc-自然言語処理nplnatural-language-processing"><span class="toc-section-number">1.1</span> 自然言語処理(NPL,Natural Language Processing)</a></li>
</ul></li>
</ul>
</div>
<h2 data-number="1" id="教師ありなし学習-執筆中"><span class="header-section-number">1</span> 教師あり/なし学習 (執筆中)</h2>
<p>それぞれ様々な手法があり,使い分ける必要があります</p>
<p>教師あり学習
* 回帰
* 重回帰ロジスティック回帰
* 決定木分析
* k-近傍法
* サポートベクターマシン
* ニューラルネットワーク
* パーセプトロン,畳込み,再起,etc</p>
<p>教師なし学習
* クラスタリング
* 階層(ウォード法
* 非階層(k-means法
* 主成分分析
* 確率密度推定</p>
<p><br></p>
<p>P3の図</p>
<h4 data-number="1.0.1" id="主成分分析principle-component-analysis"><span class="header-section-number">1.0.1</span> 主成分分析(Principle component analysis)</h4>
<ul>
<li>多数のデータが有る際に, それら全てを使うことは大変です.
<ul>
<li>多次元になると可視化が困難</li>
<li>データの説明が困難</li>
<li>モデル化が困難</li>
<li>計算が困難</li>
</ul></li>
<li>主成分分析
<ul>
<li>多数の変数の持つ情報を損なわずに圧縮する技術(次数削減)であり,予測モデル構築の前処理としてもよく使われる.</li>
<li>主成分の分散が最大になる軸を探し,その軸に直行する軸の中で分散が最大になる軸を探していく.</li>
</ul></li>
</ul>
<p><br></p>
<p>P4の図</p>
<p>非線形SVMで天気を当ててるときに選ぶべき説明変数は?
* 先週は電力データで休日かどうかを当てることを考えたが,今度は天気を当ててみよう.
* Lightning,Lamp,Power,Airがある中でどのように説明変数を選ぶきだろうか.
* 休日と同じ変数だと,正解率はあまり良くない.
* 変数を2つ選ぶとして,今回は 4C2 = 4*3/2 = 6通りだが, 変数が増えればその組み合わせは莫⼤になる.</p>
<p><br></p>
<p>主成分を使って天気を当てよう.
電力データの次元削減をして, その主成分で天気を当てる非線形SVMを実行してみよう.
第1成分と第2成分で,情報の85%以上を説明できることが分かる.この2成分を利用して,SVMを実行してみる.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -*- coding: utf-8 -*-</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co"># svmのパッケージ</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn <span class="im">import</span> svm</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">#主成分分析</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.decomposition <span class="im">import</span> PCA</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">#4つある電力データを主成分分析で次元削減してみる</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co"># データを読み込み</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">&quot;./data/energy_data.csv&quot;</span>)</span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 主成分分析のターゲットとなる電力データを抽出</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> df[[<span class="st">'Lighting'</span>, <span class="st">'Lamp'</span>, <span class="st">'Power'</span>, <span class="st">'Air'</span>]]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">#データを標準化します</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># apply メソッドは,DataFlame全体に与えた関数を適用</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co"># axis = 1 行に対して</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="co"># axis = 0 列に対して</span></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> target.<span class="bu">apply</span>(<span class="kw">lambda</span> x: (x<span class="op">-</span>x.mean())<span class="op">/</span>x.std(), axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">ラムダ式を使わなければ</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">def standardization(x):</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">return (x-x.mean())/x.std()</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">target = target.apply(standardization(), axis=1)</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">#主成分分析の実行</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>pca <span class="op">=</span> PCA()</span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>pca.fit(target)</span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 寄与率</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co"># その主成分で情報量のどの程度を説明できているか</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'第1主成分の寄与率'</span>, pca.explained_variance_ratio_[<span class="dv">0</span>])</span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'第2主成分の寄与率'</span>, pca.explained_variance_ratio_[<span class="dv">1</span>])</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">#主成分ベクトル</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>feature <span class="op">=</span> pca.transform(target)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a><span class="co"># データを主成分空間に写像</span></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a><span class="co"># n列目が第n主成分空間への写像</span></span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>feature <span class="op">=</span> pca.transform(target)</span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a>plt.scatter(feature[:, <span class="dv">0</span>], feature[:, <span class="dv">1</span>], alpha<span class="op">=</span><span class="fl">0.8</span>, c<span class="op">=</span><span class="bu">list</span>(df[<span class="st">'weather'</span>]))</span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'PC1'</span>)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'PC2'</span>)</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<pre><code>第1主成分の寄与率 0.5903597077929026
第2主成分の寄与率 0.27288683569579475</code></pre>
<p><img src="19_weather.png" /></p>
<p><br></p>
<p>P6の図</p>
<p>説明変数に第1主成分と第2主成分を利用して,非線形SVMを実行(図示のプログラムはほぼ同じなので省略) .
TestDataは,それほど良くないが, TrainDataに関しては,かなり良く予測できている.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#------------------------------------------------------------------</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ここからSVMに適用</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">#------------------------------------------------------------------</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 主成分をDFに追加</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'PC1'</span>] <span class="op">=</span> feature[:,<span class="dv">0</span>]</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>df[<span class="st">'PC2'</span>] <span class="op">=</span> feature[:,<span class="dv">1</span>]</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="co">#説明変数を主成分とする</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'PC1'</span>,<span class="st">'PC2'</span>]]</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">#被説明変数</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'weather'</span>]</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a><span class="co">#トレーニングデータの作成</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>(train_X, test_X ,train_y, test_y) <span class="op">=</span> train_test_split( X</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>, y</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>, stratify<span class="op">=</span> y</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>, random_state <span class="op">=</span> <span class="dv">0</span>)</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co"># SVMオブジェクトを定義</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>clf <span class="op">=</span> svm.SVC(kernel<span class="op">=</span><span class="st">'rbf'</span>)</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a><span class="co">#学習</span></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>clf.fit(train_X, train_y)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a><span class="co">#結果の表示</span></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'正解率(train):'</span>, clf.score(train_X,train_y))</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'正解率(test):'</span>, clf.score(test_X,test_y))</span></code></pre></div>
<pre><code>第1主成分の寄与率 0.5903597077929026
第2主成分の寄与率 0.27288683569579475
正解率(train): 0.9130434782608695
正解率(test): 0.625</code></pre>
<p><br></p>
<p>P7の図</p>
<h4 data-number="1.0.2" id="k-means法"><span class="header-section-number">1.0.2</span> k-means法</h4>
<p>最も広く使われているクラスタリング手法(データを類似度の高いグループに分ける手法). データをk個のグループに分割する場合は以下の手順で行われる.事前にクラスター数を指定する必要がある.
1. 入力データをプロットする
2. ランダムにk個の点をプロットする
3. 各ランダム点を,クラスター1,クラスター2,…,クラスターkの重心点とみなす.
4. 入力データの各点について,k個の重心点の中で最も近いものを選び,そのクラスターに分類する.
5. クラスター毎に重心を計算する.
6. 5.で定めたk個の重心を新しいクラスターの重心とする.
7. 4-6を設定した上限回数か,重心の移動距離が十分に小さくなるまで繰り返す.</p>
<p><br></p>
<p>P8の図</p>
<p>これまでのデータはクラスタリングにあまり適していないので新しく配られたデータenergy.csv をクラスタリングしてみる.
* energy.csvは, 5~6⽉の1号館,研究館,体育館のデータを結合したもの.
* 建物を適切にクラスタリングできるかやってみよう.</p>
<p><br></p>
<p>k-means法で電力データの建物をクラスタリング
本来の教師なし学習では,事前にクラスターが分からないが,ここでは練習用として,実際のクラスタと,k-means法による結果を比較してみる.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -*- coding:utf-8 -*-</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.cluster <span class="im">import</span> KMeans</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 1号館, 研究館, 体育館のデータをk-means法でクラスタリングしてみる</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a><span class="co">#データの読み込み</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">#単位が同じなので,標準化は必要ない</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'./data/energy.csv'</span>)</span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a><span class="co"># まずは図示してみる</span></span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 通常は事前にクラスターが分からないことに注意</span></span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'Lighting'</span>], df[<span class="st">'Lamp'</span>], alpha<span class="op">=</span><span class="fl">0.8</span>, c<span class="op">=</span><span class="bu">list</span>(df[<span class="st">'Building'</span>]))</span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'raw data'</span>)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Lighting'</span>)</span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Lamp'</span>)</span>
<span id="cb5-17"><a href="#cb5-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb5-18"><a href="#cb5-18" aria-hidden="true" tabindex="-1"></a><span class="co">#k-means法で分類</span></span>
<span id="cb5-19"><a href="#cb5-19" aria-hidden="true" tabindex="-1"></a><span class="co"># init = 'random' とすると kmeans法になる</span></span>
<span id="cb5-20"><a href="#cb5-20" aria-hidden="true" tabindex="-1"></a><span class="co"># init を指定しないとk-means++という手法になる</span></span>
<span id="cb5-21"><a href="#cb5-21" aria-hidden="true" tabindex="-1"></a><span class="co"># k-means++ は 初期の重心を広範に取る手法でk-meansより安定した結果が得られる</span></span>
<span id="cb5-22"><a href="#cb5-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 分割するクラスタ数は3に設定</span></span>
<span id="cb5-23"><a href="#cb5-23" aria-hidden="true" tabindex="-1"></a>kmeans <span class="op">=</span> KMeans(init<span class="op">=</span><span class="st">'random'</span>, n_clusters<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb5-24"><a href="#cb5-24" aria-hidden="true" tabindex="-1"></a><span class="co">#重心を計算</span></span>
<span id="cb5-25"><a href="#cb5-25" aria-hidden="true" tabindex="-1"></a>kmeans.fit(df[[<span class="st">'Lighting'</span>,<span class="st">'Lamp'</span>]])</span>
<span id="cb5-26"><a href="#cb5-26" aria-hidden="true" tabindex="-1"></a>prediction <span class="op">=</span> kmeans.predict(df[[<span class="st">'Lighting'</span>,<span class="st">'Lamp'</span>]])</span>
<span id="cb5-27"><a href="#cb5-27" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(prediction)</span>
<span id="cb5-28"><a href="#cb5-28" aria-hidden="true" tabindex="-1"></a><span class="co"># 結果を図示</span></span>
<span id="cb5-29"><a href="#cb5-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 通常は事前にクラスターが分からないことに注意</span></span>
<span id="cb5-30"><a href="#cb5-30" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'Lighting'</span>], df[<span class="st">'Lamp'</span>], alpha<span class="op">=</span><span class="fl">0.8</span>, c<span class="op">=</span>prediction)</span>
<span id="cb5-31"><a href="#cb5-31" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb5-32"><a href="#cb5-32" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'prediction'</span>)</span>
<span id="cb5-33"><a href="#cb5-33" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Lighting'</span>)</span>
<span id="cb5-34"><a href="#cb5-34" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Lamp'</span>)</span>
<span id="cb5-35"><a href="#cb5-35" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>P9の結果載せる</p>
<p><br></p>
<p>P10の図</p>
<h4 data-number="1.0.3" id="階層クラスタリング"><span class="header-section-number">1.0.3</span> 階層クラスタリング</h4>
<p>似たデータ点を集めて一つの点とみなすことを全体が一つの点になるまで繰り返すクラスタリング手法を階層クラスタリングといいます(k-means法は非階層クラスタリング).
｢似ている｣ことを表す概念に様々な距離を用います.
* ユークリッド距離
* マハラノビス距離
* コサイン類似度,etc…</p>
<p>階層クラスタリングは,全てのデータ点ごとの距離を計算するのであまり大きなデータには使えません(時間がかかる)
一方で,デンドログラムという結合順序を表す図を得ることで,直感的な理解が可能になります.</p>
<p><br></p>
<p>ユークリッド距離
* ユークリッド距離
一般的によく用いられる距離.ピタゴラスの定理で求められる.</p>
<p>数式</p>
<ul>
<li>標準化ユークリッド距離
ユークリッド距離を標準化したもの. 各要素の重みをなくしたユークリッド距離.</li>
</ul>
<p>数式</p>
<p><br></p>
<p>コサイン類似度
* n次元ベクトルの向きの類似性をcosθで表す.</p>
<p>数式</p>
<p>θ=0° → cosθ=1 ∶同じ向き
θ=90° → cosθ=0 ∶直行
θ=180° → cosθ=−1:逆向き</p>
<p><br></p>
<p>P13の図</p>
<p>クラスター同士の距離
階層クラスタリングなどでは,複数の点からなるクラスター間の距離
をどのように定義するかによっても,結果が異なります.
* 最短距離法
2つのクラスターのデータ間距離の最小のものをクラスター間の
距離とする.
計算量が少ないが,クラスターが帯状になりやすい.
* 群平均法
各クラスター同士の全てのデータの組み合わせの距離を測りそ
の平均をクラスター間距離とする.
* ウォード法
①各クラスターを結合した場合の重心(平均)と各クラスターの データとのユークリッド距離
②もともとのクラスターの重心と各クラスターのデータとのユークリッド距離
① - ② の値を距離とする.
計算量は多いが,精度が良い.</p>
<p><br></p>
<p>階層クラスタリングで電力データの建物をクラスタリング
k-means法と同じ分類を階層クラスタリングで行ってみる.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="co"># -*- coding:utf-8 -*-</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 階層クラスタリング</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.cluster.hierarchy <span class="im">import</span> dendrogram, linkage, fcluster</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># 1号館, 研究館, 体育館のデータを階層的クラスタリングしてみる</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">#データの読み込み</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">#単位が同じなので,標準化は必要ない</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'./data/energy.csv'</span>)</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co"># データが多すぎるとデンドログラムを見ても良くわからないので,</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co"># 各建物10個程度に減らす</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>build1 <span class="op">=</span> df[df[<span class="st">'Building'</span>] <span class="op">==</span> <span class="dv">0</span>].head(<span class="dv">10</span>)</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>research <span class="op">=</span> df[df[<span class="st">'Building'</span>] <span class="op">==</span> <span class="dv">1</span>].head(<span class="dv">10</span>)</span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a>gym <span class="op">=</span> df[df[<span class="st">'Building'</span>] <span class="op">==</span> <span class="dv">2</span>].head(<span class="dv">10</span>)</span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.concat([build1,research,gym])</span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a>df.index <span class="op">=</span> <span class="bu">range</span>(<span class="dv">0</span>,<span class="dv">30</span>)</span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co"># まずは図示してみる</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 通常は事前にクラスターが分からないことに注意</span></span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'Lighting'</span>], df[<span class="st">'Lamp'</span>], alpha<span class="op">=</span><span class="fl">0.8</span>, c<span class="op">=</span><span class="bu">list</span>(df[<span class="st">'Building'</span>]))</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'raw data'</span>)</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Lighting'</span>)</span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Lamp'</span>)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>P14の結果載せる</p>
<p><br></p>
<p>階層クラスタリングで電力
データの建物をクラスタリ
ング
デンドログラムで各点がどのように統合されているかを確認</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a><span class="co">クラスタリングの実行</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="co">method で データの結合の方法を指示します</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a><span class="co">･ average 重みのない平均距離</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a><span class="co">･ centroid 重みのない重心までの距離</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a><span class="co">･ complete 最大距離</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a><span class="co">･ median 重みのある重心までの距離</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a><span class="co">･ single 最小距離</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="co">･ ward 内部平方距離</span></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a><span class="co">･ weighted 重みのある平均距離</span></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a><span class="co">metric で距離の測り方を選択します</span></span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a><span class="co">・euclidean ユークリッド距離</span></span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a><span class="co">・cosine コサイン類似度</span></span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a><span class="co">・correlation 相関係数</span></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a><span class="co">・canberra キャンベラ距離</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a><span class="co">・chebyshev チェビシェフ距離</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a><span class="co">・cityblock 都市ブロック距離</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a><span class="co">・hamming ハミング距離</span></span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a><span class="co">・jaccard Jaccard係数</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>res <span class="op">=</span> linkage(df[[<span class="st">'Lighting'</span>,<span class="st">'Lamp’]]</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a><span class="er"> , method = 'average’</span></span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a> , metric <span class="op">=</span> <span class="st">'euclidean'</span> )</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a><span class="co"># デンドログラムの図示</span></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>dendrogram(res)</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Dedrogram&quot;</span>)</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Threshold&quot;</span>)</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>P15の結果載せる</p>
<p><br></p>
<p>階層クラスタリングで電力
データの建物をクラスタリ
ング
最後に結果を確認.データの数が違うので一概には言えないが,
k-meansでは上手くいっていなかった点も上手く分類できている.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># クラスターの数(t)を指定して,どのクラスターにそれぞれが属するかを得る</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>clusters <span class="op">=</span> fcluster(res, t<span class="op">=</span><span class="dv">3</span>, criterion<span class="op">=</span><span class="st">'maxclust'</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(clusters)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'Lighting'</span>], df[<span class="st">'Lamp'</span>], alpha<span class="op">=</span><span class="fl">0.8</span>, c<span class="op">=</span>clusters)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'prediction'</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'Lighting'</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Lamp'</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>P16の結果載せる</p>
<p><br></p>
<h3 data-number="1.1" id="自然言語処理nplnatural-language-processing"><span class="header-section-number">1.1</span> 自然言語処理(NPL,Natural Language Processing)</h3>
<p>自然言語処理の流れ
文章
<strong><em>
トークン化(トークナイザー)
* 文を適当な単位に分割すること
* 分割によって得られた文の構成要素をトークンと呼ぶ
* 方法としては
* 単語分割
* 単語に分割する
* 日本語の場合MeCabやSudochi, Jumanなどの形態素解析ツール(品詞や活用で分類)が有名
* 文字分割
* サブワード分割
* 文字分割を更に分割する (東京タワー → 東京 + タワー)
* BERTはこれを採用
</em></strong>
言語モデルによる処理
* 文章の出現しやすさを同時確率でモデル化
* p(私はパンを食べた) &gt; p(私は家を⾷べた) ← “⾷べた” と “パン” が同じ⽂章に出現しやすい
* p(私はパンを食べた)&gt; p(私にパンを食べた) ← “パンを⾷べた” と “は” が同じ⽂章に出現しやすい
* P(w|c)の条件の部分(c)を⽂脈と呼ぶ
* この学習にニューラルネットワークを利用
<strong><em>
ベクトル化(分散表現の作成)
* トークンに対応付けたベクトル(分散表現)を作成.
</em></strong>
クラスタリング
* 分散表現をクラスタ数に次元圧縮し, モデルとクラスタのラベルの損失関数を最⼩化する.</p>
<p><br></p>
<p>形態素解析
* 形態素解析
* 文章を最小の意味を持つ言語単位（形態素）に分割し,それぞれの品詞を識別する処理
* テキスト解析の前段階の処理として行われることが多い
* 手順
* データの作成
* PDFなどから直接処理することも可能だが,.txtデータにすると楽
* 入力例文: 「太陽が昇る東の空が美しい」
* テキストの前処理
* 不要なスペースや記号を除去しテキストを処理しやすい形に整理
* 形態素への分割
* 文章を形態素と呼ばれる最小単位に分割
* 「太陽」,「が」,「昇る」,「東」,「の」,「空」,「が」,「美しい」
* 品詞のタグ付け
* 分割された形態素に品詞情報を付与
* 「太陽」: 名詞
* 「が」: 助詞
* 「昇る」: 動詞
* 「東」: 名詞
* 「の」: 助詞
* 「空」: 名詞
* 「が」: 助詞
* 「美しい」: 形容詞</p>
<p><br></p>
<p>形態素解析
* Pythonで利用可能な(日本語)形態素解析パッケージ
* Mecab
* 日本語のオープンソース形態素解析システム
* pythonのライブラリとしてはmecab-python3
* janome
* Pythonで書かれた日本語形態素解析器
* MecabよりインストールなどがPythonに最適化されており,インストールなどが楽
* ただし,遅いので大規模な処理では余り使われない
* どっちでも良いが,Mecabを今回は使う
* pip install mecab-python3==0.996.5
* == 0.996.5 は新しいVerだとMacで利用するときに色々面倒くさいので古いバージョンを指定しています.
* ついでにWordCloudもInstallしておく.
* pip install wordcloud</p>
<p><br></p>
<p>形態素解析 → ワードクラウドを試してみる. (cf. https://rinsaka.com/python/nltk/05-wordcloud.html)</p>
<ul>
<li>千葉商科大学の理念(https://www.cuc.ac.jp/about_cuc/outline/spirits/index.html )の
WordCloudを作成する.</li>
<li>テキスト部分をコピペ→UTF-8のcuc.txtとしてdataフォルダに保存</li>
</ul>
<p>P21の図</p>
<p><br></p>
<p>形態素解析 → ワードクラウドを試してみる. (cf. https://rinsaka.com/python/nltk/05-wordcloud.html)</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> os</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> MeCab <span class="im">as</span> mc</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> strip_CRLF_from_Text(text):</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;テキストファイルの改行，タブを削除し，形態素解析を実行する．</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a><span class="co">改行前後が日本語文字の場合は改行を削除する．</span></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="co">それ以外はスペースに置換する．</span></span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a><span class="co"># 改行前後の文字が日本語文字の場合は改行を削除する</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a>plaintext <span class="op">=</span> re.sub(<span class="st">'([ぁ-んー]+|[ァ-ンー]+|[</span><span class="ch">\\</span><span class="st">u4e00-</span><span class="ch">\\</span><span class="st">u9FFF]+|[ぁ-んァ-ンー</span><span class="ch">\\</span><span class="st">u4e00-</span><span class="ch">\\</span><span class="st">u9FFF]+)(</span><span class="ch">\n</span><span class="st">)([ぁ-んー]+|[ァンー]+|[</span><span class="ch">\\</span><span class="st">u4e00-</span><span class="ch">\\</span><span class="st">u9FFF]+|[ぁ-んァ-ンー</span><span class="ch">\\</span><span class="st">u4e00-</span><span class="ch">\\</span><span class="st">u9FFF]+)'</span>,</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a><span class="vs">r'\1\3'</span>,</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>text)</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a><span class="co"># 残った改行とタブ記号はスペースに置換する</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a>plaintext <span class="op">=</span> plaintext.replace(<span class="st">'</span><span class="ch">\n</span><span class="st">'</span>, <span class="st">' '</span>).replace(<span class="st">'</span><span class="ch">\t</span><span class="st">'</span>, <span class="st">' '</span>)</span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> plaintext</span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> mecab_wakati(text):</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a><span class="co">MeCabで分かち書き．</span></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a><span class="co">ただし品詞は名詞だけに限定．</span></span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a>t <span class="op">=</span> mc.Tagger()</span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a><span class="co"># t = mc.Tagger('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd/')</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>node <span class="op">=</span> t.parseToNode(text)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a><span class="co"># print(node)</span></span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a>sent <span class="op">=</span> <span class="st">&quot;&quot;</span></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span>(node):</span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a><span class="co"># print(node.surface, node.feature)</span></span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> node.surface <span class="op">!=</span> <span class="st">&quot;&quot;</span>: <span class="co"># ヘッダとフッタを除外</span></span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a>word_type <span class="op">=</span> node.feature.split(<span class="st">&quot;,&quot;</span>)[<span class="dv">0</span>]</span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a><span class="co"># 名詞だけをリストに追加する</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> word_type <span class="kw">in</span> [<span class="st">&quot;名詞&quot;</span>]:</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a>sent <span class="op">+=</span> node.surface <span class="op">+</span> <span class="st">&quot; &quot;</span> <span class="co"># node.surface は「表層形」</span></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 動詞（の原型），形容詞，副詞もリストに加えたい場合は次の２行を有効にする</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a><span class="co">#if word_type in [ &quot;動詞&quot;, &quot;形容詞&quot;,&quot;副詞&quot;]:</span></span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a><span class="co"># sent += node.feature.split(&quot;,&quot;)[6] + &quot; &quot; # node.feature.split(&quot;,&quot;)[6] は形態素解析結果の</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a>「原型」</span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>node <span class="op">=</span> node.<span class="bu">next</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> node <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a><span class="cf">break</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a><span class="cf">return</span> sent</span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a><span class="co"># テキストファイル読み込み</span></span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>f <span class="op">=</span> <span class="bu">open</span>(<span class="st">'data/cuc.txt'</span>, encoding<span class="op">=</span><span class="st">'utf-8'</span>)</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a>raw <span class="op">=</span> f.read()</span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>f.close()</span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(raw)</span>
<span id="cb9-47"><a href="#cb9-47" aria-hidden="true" tabindex="-1"></a>text <span class="op">=</span> strip_CRLF_from_Text(raw)</span>
<span id="cb9-48"><a href="#cb9-48" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(text)</span>
<span id="cb9-49"><a href="#cb9-49" aria-hidden="true" tabindex="-1"></a>sent <span class="op">=</span> mecab_wakati(text)</span>
<span id="cb9-50"><a href="#cb9-50" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(sent)</span>
<span id="cb9-51"><a href="#cb9-51" aria-hidden="true" tabindex="-1"></a><span class="co"># フォントの保存先を指定する（環境によって書き換えてください）</span></span>
<span id="cb9-52"><a href="#cb9-52" aria-hidden="true" tabindex="-1"></a><span class="co">#font_path = &quot;C:\\WINDOWS\\FONTS\\MEIRYO.TTC&quot; ## Windows 版はこちら</span></span>
<span id="cb9-53"><a href="#cb9-53" aria-hidden="true" tabindex="-1"></a>font_path <span class="op">=</span> <span class="st">&quot;/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc&quot;</span> <span class="co">## Mac 版はこちら</span></span>
<span id="cb9-54"><a href="#cb9-54" aria-hidden="true" tabindex="-1"></a><span class="co"># WordCloud画像を生成する</span></span>
<span id="cb9-55"><a href="#cb9-55" aria-hidden="true" tabindex="-1"></a>wc <span class="op">=</span> WordCloud(max_font_size<span class="op">=</span><span class="dv">36</span></span>
<span id="cb9-56"><a href="#cb9-56" aria-hidden="true" tabindex="-1"></a>,font_path<span class="op">=</span>font_path</span>
<span id="cb9-57"><a href="#cb9-57" aria-hidden="true" tabindex="-1"></a>,background_color<span class="op">=</span><span class="st">'white'</span>).generate(sent)</span>
<span id="cb9-58"><a href="#cb9-58" aria-hidden="true" tabindex="-1"></a>wc.to_file(<span class="st">&quot;cuc.png&quot;</span>)</span></code></pre></div>
<p>P22の結果載せる</p>
<p><br></p>
<p>WordCloud
* 今回は,頻出単語(文章中の出現回数)ほど大きく文字が表示されている.
* 重要度などの情報を与えることも可能だが,重要度を測るには他の手法が必要.
* Bertの固有単語抽出など</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> japanize_matplotlib</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> wordcloud <span class="im">import</span> WordCloud</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="co"># 日本語のテキストデータ（基本計画の内容を代表するキーワード）</span></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co">#最初に出てきたものほど大きく表示される</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a><span class="co">#こういったリストを別の手法で作成することが必要</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>japanese_text <span class="op">=</span> <span class="st">&quot;&quot;&quot;</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a><span class="st">公的統計, 経済統計, 国民経済計算, データ, 統計改革, 統計ニーズ, 国際比較, 統計データ, PDCA, 統計リソース,</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a><span class="st">デジタル化, 情報基盤, 統計調査, データ審査, 品質管理, ユーザー視点, 効率化, 統計作成, 政策立案, 統計行政,</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="st">報告者, 利便性, 統計システム, 可視化, 報告, 支援, モニタリング, 評価, 総合的, 品質表示, 更新, 保存, 管理,</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="st">正確性, 新たな統計, 対応, 負担軽減, 変化, 進展, 進化, 計画期間, 目標, 統計委員会, 建議, 方策, 様々な観点,</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="st">社会経済, 統計作成方法, 仕様, 整備, デジタル経済, 実態把握, 報告者負担, 利用者利便性, 統計ユーザー,</span></span>
<span id="cb10-13"><a href="#cb10-13" aria-hidden="true" tabindex="-1"></a><span class="st">エラーチェック, 汎用ツール, 改善, 効率的, 統計リソース, 統計改革, 防止, 確保, 第Ⅳ期基本計画, 第Ⅲ期基本計画,</span></span>
<span id="cb10-14"><a href="#cb10-14" aria-hidden="true" tabindex="-1"></a><span class="st">相互関連, 整合性, 進め方, 効果的, 活用</span></span>
<span id="cb10-15"><a href="#cb10-15" aria-hidden="true" tabindex="-1"></a><span class="st">&quot;&quot;&quot;</span></span>
<span id="cb10-16"><a href="#cb10-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 日本語フォントの使用</span></span>
<span id="cb10-17"><a href="#cb10-17" aria-hidden="true" tabindex="-1"></a>font_path <span class="op">=</span> <span class="st">&quot;/System/Library/Fonts/ヒラギノ角ゴシック W3.ttc&quot;</span> <span class="co">#Mac</span></span>
<span id="cb10-18"><a href="#cb10-18" aria-hidden="true" tabindex="-1"></a><span class="co">#font_path = &quot;C:\\WINDOWS\\FONTS\\MEIRYO.TTC&quot; #Win</span></span>
<span id="cb10-19"><a href="#cb10-19" aria-hidden="true" tabindex="-1"></a><span class="co"># 日本語ワードクラウドの生成</span></span>
<span id="cb10-20"><a href="#cb10-20" aria-hidden="true" tabindex="-1"></a>japanese_wordcloud <span class="op">=</span> WordCloud(width<span class="op">=</span><span class="dv">800</span></span>
<span id="cb10-21"><a href="#cb10-21" aria-hidden="true" tabindex="-1"></a>, height<span class="op">=</span><span class="dv">400</span></span>
<span id="cb10-22"><a href="#cb10-22" aria-hidden="true" tabindex="-1"></a>, background_color<span class="op">=</span><span class="st">'white'</span></span>
<span id="cb10-23"><a href="#cb10-23" aria-hidden="true" tabindex="-1"></a>, font_path<span class="op">=</span>font_path</span>
<span id="cb10-24"><a href="#cb10-24" aria-hidden="true" tabindex="-1"></a>, colormap<span class="op">=</span><span class="st">'viridis'</span>).generate(japanese_text)</span>
<span id="cb10-25"><a href="#cb10-25" aria-hidden="true" tabindex="-1"></a><span class="co"># 日本語ワードクラウドの表示</span></span>
<span id="cb10-26"><a href="#cb10-26" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb10-27"><a href="#cb10-27" aria-hidden="true" tabindex="-1"></a>plt.imshow(japanese_wordcloud, interpolation<span class="op">=</span><span class="st">'bilinear'</span>)</span>
<span id="cb10-28"><a href="#cb10-28" aria-hidden="true" tabindex="-1"></a>plt.axis(<span class="st">'off'</span>)</span>
<span id="cb10-29"><a href="#cb10-29" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code></pre></div>
<p>P23の結果載せる</p>
<p><br></p>
<p>BERTについて</p>
<p>BERT
* Bidirectional Encoder Representations from Transformers
* 2018年にGoogleから発表されたニューラル言語モデ ル.• 後継にELECTRAというのがある(こっちのほうが軽く高速) • SEOなど検索エンジンの精度向上に利用 • 文章をトークンに分割したものを入力として受けて,それぞれのトークンに対応するベクトルを出力する
* 既存の方法では, トークン間の関係に関してあ まり上手く処理できなかった.
* 文章に現れるトークンに応じて各トークンの関係を決めるAttention
(注意機構)を散り入れている.
* より深く文脈を考慮したトークンの分散表現を得ることができる</p>
<p>P25の図</p>
<p><br></p>
<p>穴埋め
* BERTやchatGPTが行っているのは文章の穴埋め.
* 私はりんごを[MASK]
* MASKの部分に入る文字列の確率計算をしている.
* 私はりんごを行った ← 確率低い
* 私はりんごを食べた ← 確率高い
* コーパス(文例集)から教師あり学習をして,あらゆる語彙の連なりやすさの確率を計算している.
* Wikipediaやスクローリングして取得した文章
* � 食べた 私はりんごを = コーパス中の頻度(私はりんごを食べた)
コーパス中の頻度(私はりんごを)
* � 食べた 私はりんごを = コーパス中の頻度(私はりんごを行った)
コーパス中の頻度(私はりんごを)
* 単語間の確率による距離をベクトル表現する(学習)</p>
<p><br></p>
<p>学習
* BERTは二段階の学習を行っている.
* 事前学習
* (日本語など)言語全般に関して大規模なテキストコーパス(Wikipediaなど)で学習
* こちらのモデルがGoogleによって公開されている(ライブラリとして利用可能)
* ファインチューニング
* 事前学習済みのBERTモデルをタスク(穴埋め,ラベリング,校正などの用途)によって追加学習さ
せる.
* タスクに関連した新たなデータセットが必要
* ラベリングをするのであれば,ラベル付けされた教師データが必要</p>
<p><br></p>
<p>データセット
* BERTの利用のためには,目的に応じたデータセットが必要
* 日本語データセットとして有名なもの
* Twitter日本語評判分析データセット
* https://www.db.info.gifu-u.ac.jp/sentiment_analysis/
* Twitterの商品に関するポジティブ,ネガティブ,ニュートラルのラベリングデータ
* SNOW D18日本語感情表現辞書
* 日本語を48の勘定に分類
* 安らぎ,楽しさ親しみ,尊敬・尊さ,感謝,気持ちが良い,誇らしい,感動,喜び,悲しさ,寂しさ不満,切なさ,苦しさ,不安,憂鬱,辛さ,好き,嫌悪,恥ずかしい,焦り,驚き,怒り,幸福感,恨み,恐れ（恐縮等の意味で）,恐怖,悔しさ,祝う気持ち,困惑,きまずさ,興奮,悩み,願望,失望,あわれみ,見下し,謝罪,ためらい,不快,怠さ,あきれ,心配,緊張,妬み,憎い,残念,情けない,穏やか
* livedorニュースコーパス
* ニュース記事をサイト別/ジャンル別に分類
* 有価証券報告書ネガポジデータセット
* https://github.com/chakki-works/chABSA-dataset
* TIS株式会社が公開している上場企業の有価証券報告書を用いて作成されたマルチラベルのネガポジデータセット
* ネガティブ, ポジティブ, ニュートラルの3ラベル</p>
<p><br></p>
<p>BERT利用の流れ</p>
<p>トークン化
* MeCab(Fugashi);PythonのMeCabライブラリ
* iadic ; MeCab用のシソーラス
<strong><em>
ベクトル化
</em></strong>
学習
* FacebookのPyTorchを利用とニュラルネットワーク言語モデルライブラリであるTransformersを利用
* 事前学習
* 大規模な文章コーパスを用いて汎用的な言語のパターンを学習する
* BERTでは, ある単語を周りの単語を予測するタスクを学習
* ランダムに選ばれた15%のトークンをMASKという特殊トークンに置き換え, その位置のトークンを予測する(マスク付き⾔語モデル)
* ⼊⼒された2つの⽂が連続したものであるかを学習( Next Sentence Prediction)
* ⼤抵はこの部分はすでに⾏われたモデルを利⽤する
* ⽇本語で有名なものは東北⼤がWikipediaの⽇本語記事ので学習したもの(cl-tohoku/bert-base-japanese-whole-word-masking)
* ファインチューニング
* 事前学習を利⽤して,個別のタスクのラベル付きデータからタスクに特化した学習を⾏う.</p>
<p><br></p>
<p>P30の図</p>
<p>文章の穴埋め
* 今日は[MASK]に行く → MASK部分を予測
<strong><em>
文章分類
* ポジティブ,ネガティブなどに文章を分類(マーケティングやレ
コメンドなど
</em></strong>
マルチラベル文章分類
* ポジティブかつネガティブなど
<strong><em>
固有表現抽出
* 文章から人名・組織名といった固有名詞を抽出する
</em></strong>
文章校正
* Grammary的な
___
データの可視化と類似文章検索
* PCAからクラスタリング</p>
<p><br></p>
<p>人工知能の歴史
cf. 寺野隆雄,生成系AIの歴史・原理・現状,千葉商科大学 2023年 第1回FD 「生成系AIに関するFD」 ,2023/05/18</p>
<p>第1次AIブーム
* 1956年: ダートマス会議でスタート
* 汎⽤問題解決機(問題:現状と理想との差異)
* 問題の解決⼿法をプログラム
* 1960年代はじめ:機械翻訳の失敗で収斂
<strong><em>
第2次AIブーム
* 1980年代はじめ:エキスパートシステム,機械翻訳
* 沢山の知識を詰め込む
* 1980年代はじめ:第5世代コンピュータープロジェクト
* 1990年代はじめ:知識獲得・脆弱性で収斂
</em></strong>
第3次AIブーム
* 2010年から現在:ANN(ArYfical Neural Network)の復活
* 検索エンジン分野で発達
* ゲームでの成功(チェス,将棋,囲碁)
* パターン認識での成功(画像解析,音声解析,etc…)
___
第4次AIブーム
* 現在:生成系AI
* 言語・絵画・音声</p>
<p><br></p>
<p>GPU計算とCPU計算
* PCにおける計算は通常CPUによって行われます.
* これまでに実行してきたPythonプログラムは全てCPUを用いた計算.
* 一方でGPU(Graphics Processing Unit,画像処理装置,いわゆるグラボ)を利用した計算も可能
* ニューラルネットワークモデルは,GPUを用いて計算が行われることが多い.</p>
<p>CPU計算の特徴
* 汎⽤性: CPUは汎⽤的な計算に最適化. 様々な種類のタスクを処理可能.
* シリアル処理: CPUは⼀度に⼀つのタスクを処理するシリアル処理に適している
* 少数のコア: CPUは⽐較的少数のコアを保有.それぞれのコアが⾼速で複雑な計算を実⾏可能</p>
<p>GPU計算の特徴
* 特化した計算: GPUはグラフィックス処理や機械学習のような特定の種類の計算に最適化
* 並列処理: GPUは並列処理に特化.同時に多数の計算を実⾏可能
* 多数のコア: GPUには数百から数千のコアを保有.⼀度に多数の簡単な計算を実⾏可能</p>
<p><br></p>
<p>GPU計算でマルチラベル分類
* この講義ではGPU計算はローカル環境(それぞれのPC)で行いません.
* WindowsとMacで環境構築が全く異なり面倒
* 学生のPCのGPUが貧弱
* 処理が重く,時間がかかるのでノートPCには不向き
* なので,Googleの提供しているPython計算用のSaaS Google Colaboratryを利用します.
* 今回はColaboratry上で, 文章のマルチラベル分類を試してみます.
* マルチラベル分類
* 選択肢の中から復数のカテゴリーを選ぶ分類
* multi-hot ベクトル
文章が属すカテゴリーを表す0,1の2値ベクトル
* 今回は有価証券報告書データを利用して[ネガティブ,ニュートラル,ポジティブ]に分類
* ネガティブと判定されると[1,0,0]
* ニュートラルと判定されると[0,1,0]</p>
<p><br></p>
<p>Googleのアカウントを作る</p>
<p>P34の図</p>
<p><br></p>
<p>Google Driveの設定をする
* データなどはGoogleのクラウドストレージに保存されます.
* Google Drive上にこの授業のフォルダとデータを入れるフォルダ
を作ります</p>
<p>P35の図</p>
<p><br></p>
<p>Google Driveの設定をする</p>
<p>P36の図</p>
<p><br></p>
<p>Google Driveの設定をする
今回の授業用のフォルダをクリックして中に入ります.Teamsで配布された multi_label.ipynb をドラッグアンドドロップでフォルダに保存します.</p>
<p>P37の図</p>
<p><br></p>
<p>Google Colaboratoryが利用できるようにする
* Google のアカウントにGoogle Colaboratoryをインストールします.</p>
<p>P38の図</p>
<p><br></p>
<p>Google Colaboratoryの利用
* sldsフォルダのmul9_label.ipynb をダブルクリックするとGoogle Colaboratoryが開く.
* 右上の設定をGPU計算に変更する</p>
<p>P39の図</p>
<p><br></p>
<p>Google Colaboratoryの利用
* Runtime → run all でプログラムが動く</p>
<p>P40の図</p>
<p><br></p>
<p>Google Colaboratry の利用
出てきたらGoogle Driveのアクセスを許可する</p>
<p>P41の図</p>
<p><br></p>


<!-- 前後の章へのナビゲーション -->
<div class="chapter-navigation">
    <nav>
        
            <a class="nav-link prev" href="slds12.html">← Previous Chapter</a>
        
        
            <a class="nav-link next" href="slds14.html">Next Chapter →</a>
        
    </nav>
</div>

    <div style="clear: both"></div>

    <div id="footer">
        Site proudly generated by
        <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
    </div>
</div>

        <!-- GUID -->
        <div style="display: none">ce0f13b2-4a83-4c1c-b2b9-b6d18f4ee6d2</div>

        
        <!-- KaTeX JavaScript and auto-render extension -->
        <script>
          document.addEventListener("DOMContentLoaded", function() {
            renderMathInElement(document.body, {
              delimiters: [
                {left: "$$", right: "$$", display: true},
                {left: "\[", right: "\]", display: true},
                {left: "$", right: "$", display: false} // インライン数式用のデリミタを追加
              ]
            });
          });
        </script>
        
    </body>

</html>
