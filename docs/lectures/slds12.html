<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>yakagika - 特別講義DS Ch12 一般化線形モデル</title>

    <!-- Stylesheets. -->
    <link rel="stylesheet" type="text/css" href="../style.css?v=0">
    <link href="https://fonts.googleapis.com/css2?family=Fira+Code:wght@400;700&display=swap" rel="stylesheet">
    <!-- RSS. -->
    <link rel="alternate" type="application/rss+xml" title="yakagika" href="https://yakagika.github.io/rss.xml">

    <!-- Metadata. -->

    <meta name="keywords" content="yakagika Haskell ExchangeAlgebra">
    <meta name="description" content="Personal home page and blog of yakagika.">

    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css" integrity="sha384-Xi8rHCmBmhbuyyhbI88391ZKP2dmfnOl4rT9ZfRI7mLTdk1wblIUnrIq35nqwEvC" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js" integrity="sha384-X/XCfMm41VSsqRNQgDerQczD69XqmjOOOwYQvr/uuC+j4OPoNhVgjdGFwhvN02Ja" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js" integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
    

    
      <meta property="og:description" content="資料" />
    
  </head>

  <body>

    <!-- ハンバーガーメニューのボタン（小画面時に表示） -->
     <!-- ヘッダーを上部に固定し、その中にハンバーガーを配置 -->
    <header class="site-header">
      <div class="site-title">
        <a href="../">
          <img src="../favicon.ico" alt="Home" style="width: 32px; height: 32px;">
        </a>
      </div>
      <div class="hamburger" onclick="toggleMenu()">☰</div>
    </header>

    <!-- ナビゲーションに drawer-menu クラスを付与 -->
    <div id="navigation" class="drawer-menu">
      <h1>Contents</h1>
      <a href="../">Home</a>
      <a href="../posts.html">Blog</a>
      <a href="../lectures.html">Lecture</a>
      <a href="../research.html">Research</a>
      <a href="../contact.html">Contact</a>
      <!-- <a href="/cv.html">CV</a> -->

      <h1>Links</h1>
      <a href="http://github.com/yakagika" target="_blank" rel="noopener">GitHub</a>
      <a href="https://researchmap.jp/k-akagi" target="_blank" rel="noopener">researchmap</a>

      
      <div id="lecture-toc">
        <h1>Index</h1>
        <!-- The TOC will be generated here by JavaScript -->
      </div>
      
      
    </div>

    <div id="content">
    <h1>特別講義DS Ch12 一般化線形モデル</h1>
<div class="soft">
    資料<br />
    Published on 2025-11-04 under the tag <a title="All pages tagged 'datascience'." href="../tags/datascience.html">datascience</a>, <a title="All pages tagged 'statistics'." href="../tags/statistics.html">statistics</a>, <a title="All pages tagged 'python'." href="../tags/python.html">python</a>
</div>

<!-- 前後の章へのナビゲーション -->
<div class="chapter-navigation">
    <nav>
        
            <a class="nav-link prev" href="slds11.html">← Previous Chapter</a>
        
        
            <a class="nav-link next" href="slds13.html">Next Chapter →</a>
        
    </nav>
</div>

<br>

<div class="toc"><div class="header">Table of Contents</div>
<ul>
<li><a href="#統計モデリング" id="toc-統計モデリング"><span class="toc-section-number">1</span> 統計モデリング</a></li>
<li><a href="#基礎知識-ベイズ統計学概要" id="toc-基礎知識-ベイズ統計学概要"><span class="toc-section-number">2</span> 基礎知識 ベイズ統計学概要</a>
<ul>
<li><a href="#ベイズの定理" id="toc-ベイズの定理"><span class="toc-section-number">2.1</span> ベイズの定理</a>
<ul>
<li><a href="#条件付き確率" id="toc-条件付き確率"><span class="toc-section-number">2.1.1</span> 条件付き確率</a></li>
<li><a href="#ベイズの定理-1" id="toc-ベイズの定理-1"><span class="toc-section-number">2.1.2</span> ベイズの定理</a></li>
<li><a href="#条件付き確率-1" id="toc-条件付き確率-1"><span class="toc-section-number">2.1.3</span> 条件付き確率</a></li>
<li><a href="#ベイズ確率" id="toc-ベイズ確率"><span class="toc-section-number">2.1.4</span> ベイズ確率</a></li>
</ul></li>
<li><a href="#ベイズ更新" id="toc-ベイズ更新"><span class="toc-section-number">2.2</span> ベイズ更新</a>
<ul>
<li><a href="#独立性" id="toc-独立性"><span class="toc-section-number">2.2.1</span> 独立性</a></li>
<li><a href="#複数の事象からなる条件付き確率" id="toc-複数の事象からなる条件付き確率"><span class="toc-section-number">2.2.2</span> 複数の事象からなる条件付き確率</a></li>
<li><a href="#ベイズ更新の適用" id="toc-ベイズ更新の適用"><span class="toc-section-number">2.2.3</span> ベイズ更新の適用</a></li>
<li><a href="#ベイズ統計学の性質" id="toc-ベイズ統計学の性質"><span class="toc-section-number">2.2.4</span> ベイズ統計学の性質</a></li>
</ul></li>
<li><a href="#マルコフ連鎖モンテカルロ法" id="toc-マルコフ連鎖モンテカルロ法"><span class="toc-section-number">2.3</span> マルコフ連鎖モンテカルロ法</a>
<ul>
<li><a href="#尤度likelihood" id="toc-尤度likelihood"><span class="toc-section-number">2.3.1</span> 尤度(Likelihood)</a></li>
<li><a href="#最尤推定-maximum-likelihood-estimation" id="toc-最尤推定-maximum-likelihood-estimation"><span class="toc-section-number">2.3.2</span> 最尤推定 (Maximum Likelihood Estimation)</a></li>
<li><a href="#マルコフ連鎖モンテカルロ法-mcmcmarkov-chain-monte-carlo-method" id="toc-マルコフ連鎖モンテカルロ法-mcmcmarkov-chain-monte-carlo-method"><span class="toc-section-number">2.3.3</span> マルコフ連鎖モンテカルロ法 MCMC(Markov chain Monte Carlo Method)</a></li>
</ul></li>
</ul></li>
<li><a href="#ベイズ統計学による統計モデリング実践" id="toc-ベイズ統計学による統計モデリング実践"><span class="toc-section-number">3</span> ベイズ統計学による統計モデリング実践</a>
<ul>
<li><a href="#重回帰での結果確認" id="toc-重回帰での結果確認"><span class="toc-section-number">3.1</span> 重回帰での結果確認</a></li>
<li><a href="#過分散と個別差" id="toc-過分散と個別差"><span class="toc-section-number">3.2</span> 過分散と個別差</a></li>
<li><a href="#一般化線形モデルglm" id="toc-一般化線形モデルglm"><span class="toc-section-number">3.3</span> 一般化線形モデル(GLM)</a></li>
<li><a href="#個体差のモデリング" id="toc-個体差のモデリング"><span class="toc-section-number">3.4</span> 個体差のモデリング</a>
<ul>
<li><a href="#線形回帰での問題" id="toc-線形回帰での問題"><span class="toc-section-number">3.4.1</span> 線形回帰での問題</a></li>
<li><a href="#ベイズ推論による解決" id="toc-ベイズ推論による解決"><span class="toc-section-number">3.4.2</span> ベイズ推論による解決</a></li>
<li><a href="#モデルの仮定" id="toc-モデルの仮定"><span class="toc-section-number">3.4.3</span> モデルの仮定</a></li>
<li><a href="#ランダム傾きとランダム切片" id="toc-ランダム傾きとランダム切片"><span class="toc-section-number">3.4.4</span> ランダム傾きとランダム切片</a></li>
</ul></li>
<li><a href="#モデル全体像" id="toc-モデル全体像"><span class="toc-section-number">3.5</span> モデル全体像</a>
<ul>
<li><a href="#推定する分布" id="toc-推定する分布"><span class="toc-section-number">3.5.1</span> 推定する分布</a></li>
<li><a href="#線形予測子" id="toc-線形予測子"><span class="toc-section-number">3.5.2</span> 線形予測子</a></li>
<li><a href="#尤度関数" id="toc-尤度関数"><span class="toc-section-number">3.5.3</span> 尤度関数</a></li>
<li><a href="#事前分布" id="toc-事前分布"><span class="toc-section-number">3.5.4</span> 事前分布</a></li>
<li><a href="#超事前分布" id="toc-超事前分布"><span class="toc-section-number">3.5.5</span> 超事前分布</a></li>
</ul></li>
</ul></li>
<li><a href="#環境構築" id="toc-環境構築"><span class="toc-section-number">4</span> 環境構築</a>
<ul>
<li><a href="#anacondaのインストール" id="toc-anacondaのインストール"><span class="toc-section-number">4.1</span> <code>anaconda</code>のインストール</a></li>
</ul></li>
<li><a href="#pymc-実装" id="toc-pymc-実装"><span class="toc-section-number">5</span> PyMC 実装</a>
<ul>
<li><a href="#事前準備" id="toc-事前準備"><span class="toc-section-number">5.1</span> 事前準備</a></li>
<li><a href="#座標設定" id="toc-座標設定"><span class="toc-section-number">5.2</span> 座標設定</a></li>
<li><a href="#モデル構築" id="toc-モデル構築"><span class="toc-section-number">5.3</span> モデル構築</a>
<ul>
<li><a href="#モデル全体像との対応関係" id="toc-モデル全体像との対応関係"><span class="toc-section-number">5.3.1</span> モデル全体像との対応関係</a></li>
</ul></li>
<li><a href="#mcmc" id="toc-mcmc"><span class="toc-section-number">5.4</span> MCMC</a></li>
<li><a href="#モデル診断と可視化" id="toc-モデル診断と可視化"><span class="toc-section-number">5.5</span> モデル診断と可視化</a></li>
<li><a href="#結果の可視化" id="toc-結果の可視化"><span class="toc-section-number">5.6</span> 結果の可視化</a>
<ul>
<li><a href="#事後予測チェックposterior-predictive-check-ppc" id="toc-事後予測チェックposterior-predictive-check-ppc"><span class="toc-section-number">5.6.1</span> 事後予測チェック(Posterior Predictive Check, PPC)</a></li>
<li><a href="#予測値の確認" id="toc-予測値の確認"><span class="toc-section-number">5.6.2</span> 予測値の確認</a></li>
<li><a href="#予測範囲の可視化" id="toc-予測範囲の可視化"><span class="toc-section-number">5.6.3</span> 予測範囲の可視化</a></li>
<li><a href="#その他の可視化手法" id="toc-その他の可視化手法"><span class="toc-section-number">5.6.4</span> その他の可視化手法</a></li>
</ul></li>
</ul></li>
<li><a href="#まとめ-線形回帰-vs-ベイズモデリング" id="toc-まとめ-線形回帰-vs-ベイズモデリング"><span class="toc-section-number">6</span> まとめ (線形回帰 VS ベイズモデリング)</a>
<ul>
<li><a href="#線形回帰頻度主義と統計モデリングベイズ主義の比較" id="toc-線形回帰頻度主義と統計モデリングベイズ主義の比較"><span class="toc-section-number">6.1</span> 線形回帰(頻度主義)と統計モデリング(ベイズ主義)の比較</a>
<ul>
<li><a href="#詳細な説明" id="toc-詳細な説明"><span class="toc-section-number">6.1.1</span> 詳細な説明</a></li>
</ul></li>
<li><a href="#固定効果ダミー変数とランダム効果階層ベイズの比較" id="toc-固定効果ダミー変数とランダム効果階層ベイズの比較"><span class="toc-section-number">6.2</span> 固定効果(ダミー変数)とランダム効果(階層ベイズ)の比較</a>
<ul>
<li><a href="#詳細な説明-1" id="toc-詳細な説明-1"><span class="toc-section-number">6.2.1</span> 詳細な説明</a></li>
</ul></li>
<li><a href="#まとめ" id="toc-まとめ"><span class="toc-section-number">6.3</span> まとめ</a></li>
</ul></li>
</ul>
</div>
<h2 data-number="1" id="統計モデリング"><span class="header-section-number">1</span> 統計モデリング</h2>
<p>前回(Ch11)では線形の重回帰を利用して,ある程度正確な説明/予測が可能となりました.
しかし,これが最善のモデルであるとは限りません.
また, 線形回帰でうまく表せないからと言って,関係がないと断定することもできません.
より正確に説明できるより良いモデルが存在する可能性があります.</p>
<p>前回の事例に即して考えると例えば,</p>
<div class="note">
<ul>
<li>Scholarshipの有無によって,それぞれの傾きが異なるのでは?</li>
</ul>
<p>奨学金をもらっている人のほうがそもそも勉強時間あたりのGPAの伸び率が高いなど,層ごとに異なるパラメータを持つ可能性があります.</p>
<ul>
<li>残差の分布が正規分布ではないのでは?</li>
</ul>
<p>重回帰分析では残差が正規分布であるという仮定のもと分析を行っていますが,そのような検証は行われていません.</p>
</div>
<p>基本的にはデータを特定のモデルで表現する場合には,グラフや特徴量などから考えられる幾つかの可能性を考慮・比較検討し,最善のモデルを選択することが必要になります.
このような行為をモデル選択/統計モデリングといいます.</p>
<p>Ch 12ではこのような,重回帰分析では扱えないモデリング技法として,ベイズ統計学に基づいた一般化線形モデルに関して学習してみましょう.</p>
<h2 data-number="2" id="基礎知識-ベイズ統計学概要"><span class="header-section-number">2</span> 基礎知識 ベイズ統計学概要</h2>
<p>前章までに扱ってきた,統計的仮説検定や回帰分析は無限回試行を行った際に収束する相対度数(<strong>客観確率</strong>)を確率の定義とする<strong>頻度主義</strong>に基づいた統計的手法です(詳細は｢統計学入門(データ活用の統計学)｣などに譲ります.) そのような前提にたった統計学を<strong>伝統的統計学</strong>とも呼びます.</p>
<p>一方で,2000年代初頭から,計算機の性能向上と,MCMCなどのアルゴリズムの開発によって,分析者の情報,知識,経験などによる主観によって定めらる<strong>主観確率</strong>に基づく確率的定義(<strong>ベイズ主義</strong>)を前提とした手法が用いられるようになりました.</p>
<h3 data-number="2.1" id="ベイズの定理"><span class="header-section-number">2.1</span> ベイズの定理</h3>
<p>まずは,ベイズ統計学の中核となる<strong>ベイズ確率(逆確率)</strong>,<strong>ベイズの定理</strong>などの基礎概念の概要を把握しましょう.</p>
<div class="warn">
<p>ここでは,最低限の記法の意味などについて概要を解説します.
これ以前の基本的な確率計算や定義に関しては｢統計学入門｣, ｢データ活用の統計学｣などを,
ベイズやアルゴリズムの詳細に関しては｢データサイエンス実践｣｢データ活用の統計学実践｣などの講義を受講して下さい.</p>
</div>
<p>ベイズ確率やベイズの定理は,1740年代に数学好きの牧師であったトーマス･ベイズによってまとめられました. ベイズは,神学への興味から,｢世界が原初神によって作られたこと｣を｢現在の事象｣によって証明できるか,という問題に関心を寄せていました. ベイズはこのような問題を解くために, 仮の確率を今現在の情報によって更新し,過去の出来事を推測するというアイデア(逆確率)をまとめました.</p>
<p>しかし,ベイズ自身はこれを発表せず, 1763年にリチャード・プライスによってベイズの遺稿が発表されたことで再注目されました(このことによって近年では,ベイズの定理が<strong>ベイズ・プライスの定理</strong>と呼ばれることもあります.)</p>
<p>古典的確率の定立に貢献したラプラスも同様の観点に注目した時期があり,ベイズの定理から確率的な推論を実施する方法などがまとめられましたが,当時の計算・データ環境などからそれ以上深められることはなく,古典的確率に則った頻度主義や伝統的統計学が発展していきます.</p>
<p>(cf. シャロン・バーチュ マグレイン (著), 異端の統計学 ベイズ, 草思社, 2013)</p>
<p>それでは,逆確率やベイズの定理がどのようなものかを見ていきましょう.</p>
<h4 data-number="2.1.1" id="条件付き確率"><span class="header-section-number">2.1.1</span> 条件付き確率</h4>
<p>事象Aの下での事象Bの条件付き確率</p>
<p><span class="math display">P(B|A) = \frac{P(B \cap A)}{P(A)}</span></p>
<p>を変形した</p>
<p><span class="math display">P(B \cap A) = P(A) P(B | A) </span></p>
<p>を乗法定理と呼びます.</p>
<p>このとき AとBは対称なので,</p>
<p><span class="math display">P(B \cap A) = P(A \cap B) = P(B) P(A | B)  </span></p>
<p>も成り立ちます.</p>
<h4 data-number="2.1.2" id="ベイズの定理-1"><span class="header-section-number">2.1.2</span> ベイズの定理</h4>
<p><span class="math inline">A</span> を得られた結果, <span class="math inline">H_1,H_2,...,H_k</span> を原因としたとき,事象Aの下での事象<span class="math inline">H_i</span>の条件付き確率は,</p>
<p><span class="math display"> P(H_i | A) = \frac{P(H_i \cap A)}{P(A)}</span></p>
<p>となります.</p>
<p>ただし, ここで <span class="math inline">H_i</span> は互いに排反で, <span class="math display"> \bigcup_{i=1} H_i = \Omega </span> かつ <span class="math display"> \sum_{i=1} P(H_i) =1 </span></p>
<p>このとき,乗法定理から</p>
<p><span class="math display"> P(H_i \cap A) = P(H_i)P(A | H_i)</span></p>
<p>が求まります. これを代入して,</p>
<p><span class="math display"> P(H_i | A) = \frac{P(H_i)P(A | H_i)}{P(A)} </span></p>
<p>となり,これをベイズの定理といいます.</p>
<p>また,</p>
<p><span class="math display"> P(A) = \sum_{i=1}^{k} P(A \cap H_i) = \sum_{i=1}^{k} P(H_i)P(A|H_i) </span></p>
<p>であるから,</p>
<p><span class="math display"> P(H_i |A) = \frac{P(H_i)P(A|H_i)}{\sum_{i=1}^{k} P(H_i)P(A|H_i) }</span></p>
<p>と表す場合もあります.</p>
<p>このとき, <span class="math inline">P(H_i)</span>を(Aが起こる)<strong>事前確率</strong>,<span class="math inline">P(H_i|A)</span>を(Aが起こる)<strong>事後確率</strong>といいます. ベイズ統計学ではしばしば事前確率に<strong>主観確率</strong>が用いられ,これを基礎とする統計的方法を<strong>ベイズ統計学</strong>といいます.</p>
<div class="note">
<ul>
<li>客観確率</li>
</ul>
<p>頻度説では、事象<span class="math inline">A</span>の起こる確率<span class="math inline">P(A)</span>を生起回数の相対頻度で求めています.これは誰が計算しても同一の客観的な値といえます.</p>
<ul>
<li>主観確率</li>
</ul>
<p>研究者が主観的にある確率を与えて分析を行います. この場合確率は,研究者の経験,情報,知識によって異なる値が用いられます.
当然,主観確率によって結果は異なります. ただし,後に見るように多量のデータによって主観性を減らせる.</p>
<ul>
<li>事前確率と事後確率の意味</li>
</ul>
<p><strong>事前確率<span class="math inline">P(H_i)</span></strong>は,データ<span class="math inline">A</span>が観測される<strong>前</strong>の,仮説<span class="math inline">H_i</span>に対する(主観的な)確信度を表します. つまり,データを観測する前に,どの仮説が真である可能性が高いかを表す確率です.</p>
<p><strong>事後確率<span class="math inline">P(H_i|A)</span></strong>は,データ<span class="math inline">A</span>が観測された<strong>後</strong>の,仮説<span class="math inline">H_i</span>に対する確信度を表します. つまり,データ<span class="math inline">A</span>が得られたという情報を踏まえた上で,どの仮説が真である可能性が高いかを表す確率です.</p>
</div>
<p>ベイズの定理は,事前確率とデータから事後確率を計算する方法を提供します. この過程では,データが得られることで,仮説に対する確信度が更新されます. 例えば,事前確率が低かった仮説でも,データがその仮説を支持するものであれば,事後確率は高くなります. 逆に,事前確率が高かった仮説でも,データがその仮説と矛盾するものであれば,事後確率は低くなります.</p>
<p>条件付き確率の範囲では, ｢原因から結果の確率を計算｣しますが,ベイズの定理では結果から原因を考えています.</p>
<h4 data-number="2.1.3" id="条件付き確率-1"><span class="header-section-number">2.1.3</span> 条件付き確率</h4>
<p>事象Aが起こったと分かっている場合に事象Bの起こる確率</p>
<p><span class="math display"> P(B|A) = \frac{P(A \cap B)}{P(A)}</span></p>
<p>例:選ばれた壺から出るたまの確率を計算</p>
<p>壺に白,赤それぞれ3つの玉が入っている.白玉には1,1,2と数字,赤玉には1,2,2と数字が書かれている.
次に出る玉が白であることがわかっている場合に1の玉がでる確率は?</p>
<p><img src="../images/slds/ch12/conditional_probability.png" /></p>
<p><span class="math display">
P(1 | 白) = \frac{白 \cap 1}{P(白)} = \frac{2 / 6}{ 1/ 2} = \frac{2}{3}
</span></p>
<p>𝑃(玉の色|選んだ壺)であり, 𝑃(結果|原因)の計算</p>
<h4 data-number="2.1.4" id="ベイズ確率"><span class="header-section-number">2.1.4</span> ベイズ確率</h4>
<p>事象Bが起こったと分かっている場合に,事象Aが起きている確率</p>
<p><span class="math display"> P(H_i | A) = \frac{P(H_i)P(A | H_i)}{P(A)} </span></p>
<p>例. 出た玉から壺が選ばれている確率を計算</p>
<p>2つの壺があり,</p>
<ul>
<li><p>第1の壺には白玉が3個,黒玉が1個</p></li>
<li><p>第2の壺には白玉が1個,黒玉が２個</p></li>
<li><p><span class="math inline">𝐻_1</span>:第1の壺から取り出す</p></li>
<li><p><span class="math inline">𝐻_2</span>:第2の壺から取り出す</p></li>
<li><p><span class="math inline">A</span>:白玉が出たという事象</p></li>
</ul>
<p>とすると,いずれかの壺から玉を1個取り出したところ白玉であった,どちらの壺から取り出した確率が高いか.</p>
<p><img src="../images/slds/ch12/beys_example.png" /></p>
<p>まず,どちらの壺から取り出すかは<strong>五分五分</strong>であると仮定する. ここでこの仮定が完全に分析者の主観に基づくのであれば<strong>主観確率</strong>であり,何らかの実験等によって確率0.5であると確かめられている場合には<strong>客観確率</strong>とみなされる.</p>
<p><span class="math display">P(H_1) = P(H_2) = \frac{1}{2}</span></p>
<p><span class="math display">P(A|H_1) = \frac{3}{4}, \quad P(A|H_2) = \frac{1}{3}</span></p>
<p>であるから,</p>
<p><span class="math display">P(H_1|A) = \frac{\frac{1}{2} \cdot \frac{3}{4}}{\frac{1}{2} \cdot \frac{3}{4} + \frac{1}{2} \cdot \frac{1}{3}} = \frac{9}{13}</span></p>
<p><span class="math display">P(H_2|A) = \frac{\frac{1}{2} \cdot \frac{1}{3}}{\frac{1}{2} \cdot \frac{3}{4} + \frac{1}{2} \cdot \frac{1}{3}} = \frac{4}{13}</span></p>
<p>この｢<span class="math inline">H_1</span>の壺が選ばれている｣という結果は,壺が選ばれる確率が等しいという主観確率が正しければ正しいと言えます.</p>
<p>この事例で計算されているのは 𝑃(選んだ壺|玉の色) であり, 𝑃(原因|結果)という計算をしていることになります.</p>
<p>このように事例を見てみると, 主観確率を利用した計算は,主観確率の正しさに依存しているためあまり実用性があるようには思えません.
そこで重要になってくるのが,<strong>ベイズ更新</strong>という概念です.</p>
<h3 data-number="2.2" id="ベイズ更新"><span class="header-section-number">2.2</span> ベイズ更新</h3>
<h4 data-number="2.2.1" id="独立性"><span class="header-section-number">2.2.1</span> 独立性</h4>
<p>事象<span class="math inline">A</span>の起こる確率が他の事象<span class="math inline">B</span>に影響されない場合,事象<span class="math inline">A</span>と事象<span class="math inline">B</span>は<strong>独立である</strong>という.</p>
<p>このとき</p>
<p><span class="math display">P(A) = P(A|B)</span></p>
<p>が成り立ちます. 乗法定理 <span class="math inline">P(A \cap B) = P(B)P(A|B)</span> より,</p>
<p><span class="math display">P(A \cap B) = P(A)P(B)</span></p>
<p>が成り立つ.</p>
<p><strong>例:</strong> サイコロを2回投げて連続で1の目が出る確率を考える.</p>
<ul>
<li>事象<span class="math inline">A</span>: 1回目1が出る</li>
<li>事象<span class="math inline">B</span>: 2回目1が出る</li>
</ul>
<p><span class="math display">P(A \cap B) = \frac{1}{36}, \quad P(A) \cdot P(B) = \frac{1}{6} \cdot \frac{1}{6} = \frac{1}{36}</span></p>
<p>なので,事象<span class="math inline">A</span>と<span class="math inline">B</span>は独立といえる.</p>
<p>ベイズの目的は,複数のデータを利用して事後確率の推定を更新していくことにあります.</p>
<h4 data-number="2.2.2" id="複数の事象からなる条件付き確率"><span class="header-section-number">2.2.2</span> 複数の事象からなる条件付き確率</h4>
<p>複数の事象<span class="math inline">A, B, C</span>がある場合の条件付き確率は以下のように表されます.</p>
<p><span class="math display">P(B \cap C | A) = \frac{P(A \cap B \cap C)}{P(A)}</span></p>
<p>この式は,乗法定理(積の法則)により以下のように書き換えられます.</p>
<p><span class="math display">\Leftrightarrow P(A \cap B \cap C) = P(B \cap C | A)P(A)</span></p>
<p>同様に,<span class="math inline">A</span>が<span class="math inline">B \cap C</span>の条件下で起こる確率は以下の通りです.</p>
<p><span class="math display">P(A | B \cap C) = \frac{P(A \cap B \cap C)}{P(B \cap C)}</span></p>
<p>これも積の法則として以下のように書き換えられます.</p>
<p><span class="math display">\Leftrightarrow P(A \cap B \cap C) = P(A | B \cap C)P(B \cap C)</span></p>
<p>これらの式を代入して整理すると,以下の関係が得られます.</p>
<p><span class="math display">P(A | B \cap C) = \frac{P(B \cap C | A)P(A)}{P(B \cap C)}</span></p>
<h4 data-number="2.2.3" id="ベイズ更新の適用"><span class="header-section-number">2.2.3</span> ベイズ更新の適用</h4>
<p>原因<span class="math inline">H</span>と,現在の状況<span class="math inline">B, C</span>があるとき,事後確率<span class="math inline">P(H | B \cap C)</span>はベイズの定理により以下のように計算されます.</p>
<p><span class="math display">P(H | B \cap C) = \frac{P(B \cap C | H)P(H)}{P(B \cap C)}</span></p>
<p>もし事象<span class="math inline">B</span>と<span class="math inline">C</span>が独立であれば,条件付き確率<span class="math inline">P(B \cap C | H)</span>は<span class="math inline">P(B | H)P(C | H)</span>となり,分母の<span class="math inline">P(B \cap C)</span>は<span class="math inline">P(B)P(C)</span>となります. さらに,<span class="math inline">P(H | C) = \frac{P(C | H)P(H)}{P(C)}</span>の関係を利用すると,上記の式は以下のように変形できます.</p>
<p><span class="math display">P(H | B \cap C) = \frac{P(B | H)P(C | H)P(H)}{P(B)P(C)} = \frac{P(B | H)P(H | C)}{P(B)}</span></p>
<p>ここで,情報<span class="math inline">C</span>によって既に求められた事後確率<span class="math inline">P(H | C)</span>を新しい事前確率<span class="math inline">P(H)^*</span>とすると,ベイズ更新の式はより簡潔に表現できます.</p>
<p><span class="math display">P(H | B \cap C) = \frac{P(B | H)P(H)^*}{P(B)}</span></p>
<p>この方法により,情報が与えられるたびに事後確率を計算し,それを次のデータに対する事前分布として利用することで,分布の推定を逐次的に更新していくことができます.</p>
<h4 data-number="2.2.4" id="ベイズ統計学の性質"><span class="header-section-number">2.2.4</span> ベイズ統計学の性質</h4>
<p>事前確率に関する情報がなく,<span class="math inline">P(H)</span>が完全に主観的に決められたとしても,データ (<span class="math inline">A,B,C, \dots</span>)が大量にある場合は,</p>
<p><span class="math display">P(H|A \cap B \cap C \cap D \dots) = \frac{P(A \cap B \cap C \cap D \dots|H) P(H)}{P(A \cap B \cap C \cap D \dots)}</span></p>
<p>以下のように段階的に更新を実施することができます.</p>
<p>まず,データ<span class="math inline">A</span>が得られたとき:</p>
<p><span class="math display">P(H|A) = \frac{P(A|H)P(H)}{P(A)}</span></p>
<p>次に,データ<span class="math inline">B</span>が得られたとき,前の事後確率<span class="math inline">P(H|A) = \frac{P(A|H)P(H)}{P(A)} = P(H)^*</span> を新しい事前確率として利用:</p>
<p><span class="math display">P(H|A \cap B) = \frac{P(B|H)P(H|A)}{P(B)} = \frac{P(B|H)P(H)^*}{P(B)} </span></p>
<p>さらに,データ<span class="math inline">C</span>が得られたとき,前の事後確率<span class="math inline">P(H|A \cap B) = P(H)^{**}</span>を新しい事前確率として利用:</p>
<p><span class="math display">P(H|A \cap B \cap C) = \frac{P(C|H)P(H)^{**}}{P(C)} </span></p>
<p>このように,データ<span class="math inline">D, E, \dots</span>が得られるたびに,前の事後確率を事前確率として利用して更新を続けていきます. データ( <span class="math inline">A,B,C,D, \dots</span> )を大量に集めれば,最初に設定した事前確率<span class="math inline">P(H)</span>の影響が少なくなり,事後確率 <span class="math inline">P(H|A \cap B \cap C \cap D \dots)</span>が安定することが知られています.</p>
<div class="note">
<p>この性質は,<strong>ベイズ的一致性(Bayesian consistency)</strong>と呼ばれます. 具体的には以下のような性質が成り立ちます:</p>
<ul>
<li><p><strong>一致性</strong>: データが増えるにつれて,事後確率分布は真のパラメータ値に収束します. つまり,サンプルサイズが無限大に近づくと,事後確率は真の値に集中していきます.</p></li>
<li><p><strong>事後確率の安定性</strong>: 異なる事前分布から出発しても,データが十分に多ければ,最終的な事後分布はほぼ同じ結果になります. これは,データが増えるにつれて,尤度関数<span class="math inline">P(A \cap B \cap C \cap D \dots|H)</span>の影響が事前確率<span class="math inline">P(H)</span>の影響を上回るためです.</p></li>
<li><p><strong>事前確率の影響の減少</strong>: データが少ないときは事前確率<span class="math inline">P(H)</span>の選択が結果に大きく影響しますが,データが増えるにつれて,その影響は相対的に小さくなります. 最終的には,データの情報が支配的になり,事前確率の選択が結果に与える影響は限定的になります.</p></li>
</ul>
</div>
<p>しかし,このような更新には大量のデータが必要であることに加えて,データごとに分布を推定し直すには膨大な計算が必要となるため, ラプラスらの時代には現実的ではありませんでした.</p>
<p>現代では, 情報通信技術の発展によって大量のデータと,大量の計算が可能になりました.また,計算のためのアルゴリズム(マルコフ連鎖モンテカルロ法など)が発明されたことで,ベイズ更新に基づく推定が現実的な選択肢となりベイズ統計学が利用されるようになっています.</p>
<h3 data-number="2.3" id="マルコフ連鎖モンテカルロ法"><span class="header-section-number">2.3</span> マルコフ連鎖モンテカルロ法</h3>
<h4 data-number="2.3.1" id="尤度likelihood"><span class="header-section-number">2.3.1</span> 尤度(Likelihood)</h4>
<p>尤度とは,モデルのデータへの当てはまりの良さを表す統計量です.</p>
<ul>
<li><span class="math inline">\theta</span>を母数とする確率分布から観測データ<span class="math inline">y_i</span>が発生した場合,その確率は<span class="math inline">P(y_i|\theta)</span>と表されます.</li>
<li>尤度<span class="math inline">L(\theta|Y)</span>は,観測されたデータ<span class="math inline">Y = \{y_1, y_2, \dots, y_n\}</span>が与えられたときの,特定のパラメータ<span class="math inline">\theta</span>の「もっともらしさ」を示す関数で,以下のように定義されます.</li>
</ul>
<p><span class="math display">L(\theta|Y) = \prod_{i} P(y_i|\theta)</span></p>
<p>尤度はそのままでは計算しにくい場合が多いため,対数化した対数尤度を用いることが一般的です.</p>
<p><span class="math display">\log L(\theta|Y) = \sum_{i} \log P(y_i|\theta)</span></p>
<p>前段で説明したベイズ更新と尤度には密接な関係があります.</p>
<p>ベイズの定理は以下のように表されました:</p>
<p><span class="math display">P(H|A) = \frac{P(A|H)P(H)}{P(A)}</span></p>
<p>この式において,<span class="math inline">P(A|H)</span>は「仮説<span class="math inline">H</span>が真であるときにデータ<span class="math inline">A</span>が観測される確率」であり,これは<strong>尤度関数</strong>に対応します.</p>
<p>一般的な尤度の説明ではパラメータを<span class="math inline">\theta</span>で表しましたが,ベイズ統計学では仮説<span class="math inline">H</span>をパラメータとみなすことができます. 複数のデータ<span class="math inline">Y = \{y_1, y_2, \dots, y_n\}</span>が得られた場合,ベイズの定理は以下のようになります:</p>
<p><span class="math display">P(H|Y) = \frac{P(Y|H)P(H)}{P(Y)} = \frac{L(H|Y)P(H)}{P(Y)}</span></p>
<p>ここで,<span class="math inline">P(Y|H) = \prod_{i} P(y_i|H)</span>は尤度関数<span class="math inline">L(H|Y) = \prod_{i} P(y_i|H)</span>と一致します. したがって,ベイズの定理は尤度関数を用いて以下のように表現できます:</p>
<p><span class="math display">P(H|Y) \propto L(H|Y)P(H)</span></p>
<p>つまり,事後確率は「尤度×事前確率」に比例します. この関係から以下のことがわかります:</p>
<ul>
<li>尤度<span class="math inline">L(H|Y)</span>が大きいほど,その仮説<span class="math inline">H</span>の事後確率<span class="math inline">P(H|Y)</span>も大きくなります.</li>
<li>データが増えるにつれて,尤度関数の影響が事前確率の影響を上回り,事後確率が安定していきます.</li>
<li>ベイズ更新では,新しいデータが得られるたびに尤度関数を計算し,それと事前確率を組み合わせて事後確率を更新していきます.</li>
</ul>
<h4 data-number="2.3.2" id="最尤推定-maximum-likelihood-estimation"><span class="header-section-number">2.3.2</span> 最尤推定 (Maximum Likelihood Estimation)</h4>
<p>この対数尤度を最大にするようなパラメータの推定量<span class="math inline">\hat{\theta}</span>を推定することを<strong>最尤推定</strong>といいます.</p>
<p><span class="math inline">\hat{\theta}</span>は数理的に求めることが可能な場合もありますが,モデルが複雑な場合は困難なので機械的に求めることが多く,その手法の代表的なものがマルコフ連鎖モンテカルロ法(MCMC)です.</p>
<h4 data-number="2.3.3" id="マルコフ連鎖モンテカルロ法-mcmcmarkov-chain-monte-carlo-method"><span class="header-section-number">2.3.3</span> マルコフ連鎖モンテカルロ法 MCMC(Markov chain Monte Carlo Method)</h4>
<p>前段で説明したベイズ統計学において,事後確率<span class="math inline">P(H|Y) \propto L(H|Y)P(H)</span>を計算するには,分母の正規化定数<span class="math inline">P(Y)</span>を求める必要があります. しかし,モデルが複雑な場合やパラメータが多変数の場合,この正規化定数の計算は解析的に困難です. また,最尤推定においても,解析的に最尤推定量<span class="math inline">\hat{\theta}</span>が求められない(難しい)場合があります.</p>
<p>このような場合に用いられるのが,計算機による繰り返し計算で少しずつパラメータの値を変化させ,最適な値を探し出す方法である<strong>マルコフ連鎖モンテカルロ法(MCMC)</strong>です.</p>
<div class="note">
<ul>
<li>MCMCの基本的な仕組み</li>
</ul>
<p>MCMCは以下のような手順で動作します:</p>
<ol type="1">
<li><p><strong>初期値の設定</strong>: 複数の初期値<span class="math inline">\theta</span>を適当に決めます.</p></li>
<li><p><strong>ランダムな探索</strong>: ランダムに<span class="math inline">\theta</span>を少し増減させます.</p></li>
<li><p><strong>尤度の評価</strong>: 新しい<span class="math inline">\theta</span>の値での尤度を計算し,前の値と比較します.</p></li>
<li><p><strong>移動の決定</strong>: 尤度が改善したら,<span class="math inline">\theta</span>をその方向にして次のステップに進みます. ただし,<strong>メトロポリス法</strong>などの手法では,たまに尤度が悪くなっても一定の確率でそちらに進むことで,局所最適解に陥ることを防ぎます.</p></li>
<li><p><strong>繰り返し</strong>: この過程を繰り返すことで,パラメータ空間を探索し,最尤推定量や事後分布のサンプルを生成します.</p></li>
</ol>
<p><img src="../images/slds/ch12/mcmc.png" /></p>
</div>
<p>MCMCにより,ベイズ統計学における複雑な事後分布の推定や,最尤推定における困難な最適化問題を,計算機上で効率的に解決できるようになりました.</p>
<h2 data-number="3" id="ベイズ統計学による統計モデリング実践"><span class="header-section-number">3</span> ベイズ統計学による統計モデリング実践</h2>
<p>それでは,実際にベイズ統計学に基づいた統計モデリングを実施してみましょう.
前章で重回帰で実施したものと良く似た<a href="https://github.com/yakagika/yakagika.github.io/blob/main/slds_data/ch12/hierarchical_regression.csv">こちらのデータ</a>を事例として利用します.</p>
<table>
<thead>
<tr class="header">
<th>GPA</th>
<th>Scholarship</th>
<th>Study_Hours</th>
<th>Sports_hours</th>
<th>Part_time_Work</th>
<th>StudentID</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1.348928</td>
<td>True</td>
<td>10.348188</td>
<td>5.398119</td>
<td>14.944201</td>
<td>0</td>
</tr>
<tr class="even">
<td>1.968662</td>
<td>False</td>
<td>8.803971</td>
<td>3.799566</td>
<td>15.340118</td>
<td>1</td>
</tr>
<tr class="odd">
<td>2.246881</td>
<td>True</td>
<td>10.367043</td>
<td>5.139604</td>
<td>19.937536</td>
<td>2</td>
</tr>
<tr class="even">
<td>1.167275</td>
<td>True</td>
<td>2.049724</td>
<td>4.229373</td>
<td>21.009315</td>
<td>3</td>
</tr>
<tr class="odd">
<td>3.295929</td>
<td>True</td>
<td>9.121312</td>
<td>5.227035</td>
<td>14.271377</td>
<td>4</td>
</tr>
<tr class="even">
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
<td>…</td>
</tr>
<tr class="odd">
<td>0.974230</td>
<td>False</td>
<td>4.256551</td>
<td>0.396158</td>
<td>13.299289</td>
<td>185</td>
</tr>
<tr class="even">
<td>2.208293</td>
<td>False</td>
<td>14.652655</td>
<td>1.969618</td>
<td>10.523032</td>
<td>186</td>
</tr>
<tr class="odd">
<td>0.786660</td>
<td>False</td>
<td>10.040932</td>
<td>7.733749</td>
<td>13.232559</td>
<td>187</td>
</tr>
<tr class="even">
<td>2.068987</td>
<td>True</td>
<td>6.073965</td>
<td>8.289935</td>
<td>13.540597</td>
<td>188</td>
</tr>
<tr class="odd">
<td>2.531604</td>
<td>True</td>
<td>11.848414</td>
<td>4.501928</td>
<td>11.335318</td>
<td>189</td>
</tr>
</tbody>
</table>
<p>各自でダウンロードして利用して下さい.</p>
<h3 data-number="3.1" id="重回帰での結果確認"><span class="header-section-number">3.1</span> 重回帰での結果確認</h3>
<p>まずは,データの読み込みを行います. 必要なライブラリは各自で <code>pip install</code>してください.</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, mean_absolute_error</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.read_csv(<span class="st">'hierarchical_regression.csv'</span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a>                ,dtype<span class="op">=</span>{<span class="st">'GPA'</span>: <span class="bu">float</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Scholarship'</span>: <span class="bu">bool</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Study_Hours'</span>: <span class="bu">float</span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Sports_hours'</span>: <span class="bu">float</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Part_time_Work'</span>: <span class="bu">float</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'StudentID'</span>:<span class="bu">int</span>})</span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(df)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a><span class="co">          GPA  Scholarship  Study_Hours  Sports_hours  Part_time_Work  StudentID</span></span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a><span class="co">0    1.348928         True    10.348188      5.398119       14.944201          0</span></span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a><span class="co">1    1.968662        False     8.803971      3.799566       15.340118          1</span></span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a><span class="co">2    2.246881         True    10.367043      5.139604       19.937536          2</span></span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a><span class="co">3    1.167275         True     2.049724      4.229373       21.009315          3</span></span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a><span class="co">4    3.295929         True     9.121312      5.227035       14.271377          4</span></span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a><span class="co">..        ...          ...          ...           ...             ...        ...</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a><span class="co">185  0.974230        False     4.256551      0.396158       13.299289        185</span></span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a><span class="co">186  2.208293        False    14.652655      1.969618       10.523032        186</span></span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a><span class="co">187  0.786660        False    10.040932      7.733749       13.232559        187</span></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a><span class="co">188  2.068987         True     6.073965      8.289935       13.540597        188</span></span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a><span class="co">189  2.531604         True    11.848414      4.501928       11.335318        189</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span></code></pre></div>
<p>まずはデータを可視化してみます.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="co"># ペアプロット</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>sns.pairplot(df</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>            ,<span class="bu">vars</span><span class="op">=</span>[<span class="st">&quot;GPA&quot;</span>, <span class="st">&quot;Study_Hours&quot;</span>, <span class="st">&quot;Sports_hours&quot;</span>, <span class="st">&quot;Part_time_Work&quot;</span>]</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>            ,hue<span class="op">=</span><span class="st">&quot;Scholarship&quot;</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>            ,diag_kind<span class="op">=</span><span class="st">&quot;hist&quot;</span>)</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>plt.suptitle(<span class="st">&quot;Pairplot with Scholarship&quot;</span>, y<span class="op">=</span><span class="fl">1.02</span>)</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'PairplotwithScholarship.png'</span>)</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="co">#joinplotで男女別に密度プロットを表示</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>fig <span class="op">=</span> sns.jointplot(df</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>               ,x <span class="op">=</span><span class="st">&quot;Study_Hours&quot;</span></span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>               ,y <span class="op">=</span><span class="st">&quot;GPA&quot;</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>               ,hue<span class="op">=</span><span class="st">'Scholarship'</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>               ,joint_kws <span class="op">=</span> <span class="bu">dict</span>(alpha<span class="op">=</span><span class="fl">0.5</span>))</span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'kde.png'</span>)</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code></pre></div>
<p><img src="../images/slds/ch12/PairplotwithScholarship.png" /></p>
<p>以前のデータと概ね似た傾向(奨学金ありが少し高い,勉強時間とGPAに相関,勉強時間とバイト時間に負の相関など,)が確認できます.
ただし,前回のデータと比べるとデータ全体のばらつきが大きいことが分かります.</p>
<p><img src="../images/slds/ch12/kde-compare.png" /></p>
<p>前回と同様の手法で,線形重回帰の実施と結果の確認を行います.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 数量化</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pd.get_dummies(df, columns<span class="op">=</span>[<span class="st">'Scholarship'</span>], dtype<span class="op">=</span><span class="st">'int'</span>)</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">#標準化</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>df[[<span class="st">'Scholarship_True'</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>   ,<span class="st">'Study_Hours'</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>   ,<span class="st">'Part_time_Work'</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>   ,<span class="st">'Sports_hours'</span>]] <span class="op">=</span> scaler.fit_transform(df[[<span class="st">'Scholarship_True'</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>                                               ,<span class="st">'Study_Hours'</span></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>                                               ,<span class="st">'Part_time_Work'</span></span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>                                               ,<span class="st">'Sports_hours'</span>]])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a><span class="co">#線形回帰</span></span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 説明変数(X)と目的変数(y)に分割</span></span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> df[[<span class="st">'Scholarship_True'</span>, <span class="st">'Study_Hours'</span>]]</span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> df[<span class="st">'GPA'</span>]</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a><span class="co"># 切片(定数項)を追加</span></span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>X <span class="op">=</span> sm.add_constant(X)</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>lm <span class="op">=</span> sm.OLS(y, X).fit()</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">[通常の線形回帰の結果]</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(lm.summary())</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>lm_preds <span class="op">=</span> lm.predict(X)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>plt.scatter(df[<span class="st">'GPA'</span>], lm_preds, alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolors<span class="op">=</span><span class="st">&quot;k&quot;</span>)</span>
<span id="cb3-31"><a href="#cb3-31" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;Predicted&quot;</span>)</span>
<span id="cb3-32"><a href="#cb3-32" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Actual&quot;</span>)</span>
<span id="cb3-33"><a href="#cb3-33" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Actual vs. Predicted&quot;</span>)</span>
<span id="cb3-34"><a href="#cb3-34" aria-hidden="true" tabindex="-1"></a>plt.plot([df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()], [df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()], color<span class="op">=</span><span class="st">&quot;red&quot;</span>, linestyle<span class="op">=</span><span class="st">&quot;--&quot;</span>)  <span class="co"># 完全一致のライン</span></span>
<span id="cb3-35"><a href="#cb3-35" aria-hidden="true" tabindex="-1"></a>plt.grid()</span>
<span id="cb3-36"><a href="#cb3-36" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'lm.png'</span>)</span>
<span id="cb3-37"><a href="#cb3-37" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb3-38"><a href="#cb3-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-39"><a href="#cb3-39" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb3-40"><a href="#cb3-40" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(lm_preds, label <span class="op">=</span> <span class="st">'Predicted'</span>)</span>
<span id="cb3-41"><a href="#cb3-41" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(df[<span class="st">'GPA'</span>], label <span class="op">=</span> <span class="st">'Actual'</span>)</span>
<span id="cb3-42"><a href="#cb3-42" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">'Actual/Predicted'</span>)</span>
<span id="cb3-43"><a href="#cb3-43" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">'GPA'</span>)</span>
<span id="cb3-44"><a href="#cb3-44" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb3-45"><a href="#cb3-45" aria-hidden="true" tabindex="-1"></a>plt.legend()</span>
<span id="cb3-46"><a href="#cb3-46" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'lm_kde.png'</span>)</span>
<span id="cb3-47"><a href="#cb3-47" aria-hidden="true" tabindex="-1"></a>plt.close()</span>
<span id="cb3-48"><a href="#cb3-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-49"><a href="#cb3-49" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span>
<span id="cb3-50"><a href="#cb3-50" aria-hidden="true" tabindex="-1"></a><span class="co">                            OLS Regression Results                            </span></span>
<span id="cb3-51"><a href="#cb3-51" aria-hidden="true" tabindex="-1"></a><span class="co">==============================================================================</span></span>
<span id="cb3-52"><a href="#cb3-52" aria-hidden="true" tabindex="-1"></a><span class="co">Dep. Variable:                    GPA   R-squared:                       0.444</span></span>
<span id="cb3-53"><a href="#cb3-53" aria-hidden="true" tabindex="-1"></a><span class="co">Model:                            OLS   Adj. R-squared:                  0.438</span></span>
<span id="cb3-54"><a href="#cb3-54" aria-hidden="true" tabindex="-1"></a><span class="co">Method:                 Least Squares   F-statistic:                     74.76</span></span>
<span id="cb3-55"><a href="#cb3-55" aria-hidden="true" tabindex="-1"></a><span class="co">Date:                Tue, 04 Nov 2025   Prob (F-statistic):           1.38e-24</span></span>
<span id="cb3-56"><a href="#cb3-56" aria-hidden="true" tabindex="-1"></a><span class="co">Time:                        16:50:17   Log-Likelihood:                -187.55</span></span>
<span id="cb3-57"><a href="#cb3-57" aria-hidden="true" tabindex="-1"></a><span class="co">No. Observations:                 190   AIC:                             381.1</span></span>
<span id="cb3-58"><a href="#cb3-58" aria-hidden="true" tabindex="-1"></a><span class="co">Df Residuals:                     187   BIC:                             390.8</span></span>
<span id="cb3-59"><a href="#cb3-59" aria-hidden="true" tabindex="-1"></a><span class="co">Df Model:                           2                                         </span></span>
<span id="cb3-60"><a href="#cb3-60" aria-hidden="true" tabindex="-1"></a><span class="co">Covariance Type:            nonrobust                                         </span></span>
<span id="cb3-61"><a href="#cb3-61" aria-hidden="true" tabindex="-1"></a><span class="co">====================================================================================</span></span>
<span id="cb3-62"><a href="#cb3-62" aria-hidden="true" tabindex="-1"></a><span class="co">                       coef    std err          t      P&gt;|t|      [0.025      0.975]</span></span>
<span id="cb3-63"><a href="#cb3-63" aria-hidden="true" tabindex="-1"></a><span class="co">------------------------------------------------------------------------------------</span></span>
<span id="cb3-64"><a href="#cb3-64" aria-hidden="true" tabindex="-1"></a><span class="co">const                2.0000      0.047     42.122      0.000       1.906       2.094</span></span>
<span id="cb3-65"><a href="#cb3-65" aria-hidden="true" tabindex="-1"></a><span class="co">Scholarship_True     0.1385      0.047      2.917      0.004       0.045       0.232</span></span>
<span id="cb3-66"><a href="#cb3-66" aria-hidden="true" tabindex="-1"></a><span class="co">Study_Hours          0.5620      0.047     11.835      0.000       0.468       0.656</span></span>
<span id="cb3-67"><a href="#cb3-67" aria-hidden="true" tabindex="-1"></a><span class="co">==============================================================================</span></span>
<span id="cb3-68"><a href="#cb3-68" aria-hidden="true" tabindex="-1"></a><span class="co">Omnibus:                       11.718   Durbin-Watson:                   1.974</span></span>
<span id="cb3-69"><a href="#cb3-69" aria-hidden="true" tabindex="-1"></a><span class="co">Prob(Omnibus):                  0.003   Jarque-Bera (JB):               14.133</span></span>
<span id="cb3-70"><a href="#cb3-70" aria-hidden="true" tabindex="-1"></a><span class="co">Skew:                          -0.457   Prob(JB):                     0.000853</span></span>
<span id="cb3-71"><a href="#cb3-71" aria-hidden="true" tabindex="-1"></a><span class="co">Kurtosis:                       3.975   Cond. No.                         1.01</span></span>
<span id="cb3-72"><a href="#cb3-72" aria-hidden="true" tabindex="-1"></a><span class="co">==============================================================================</span></span>
<span id="cb3-73"><a href="#cb3-73" aria-hidden="true" tabindex="-1"></a><span class="co">&quot;&quot;&quot;</span></span></code></pre></div>
<p>全般的に有意ではあるものの,精度がそれほど高くありません.
<img src="../images/slds/ch12/lm.png" />
<img src="../images/slds/ch12/lm_kde.png" /></p>
<p>グラフを見てみると, 元のデータは予測値に対して分散が大きくデータの分散を予測モデルが説明しきれていないことが分かります.
R2を確認しても,データに含まれている変数では全体の変動のうち44%ほどしか説明できていません.</p>
<h3 data-number="3.2" id="過分散と個別差"><span class="header-section-number">3.2</span> 過分散と個別差</h3>
<p>このように,観察データの分散が,モデルの予測から逸脱しており,モデルではデータの分散が説明できていない状態を<strong>過分散</strong>といいます.</p>
<p>モデルの説明力を上げるためには,このようなモデルによって説明できていないばらつきを説明する拡張が必要となります.</p>
<p>ここでは,｢データ(の観測対象)=学生｣なので,｢ばらつき=学生差｣と仮定してみます.
例えば,そもそも過去の学習経験,勉強環境などにより,ベースとなる学力が学生それぞれで異なるなど,学生差によるばらつきを表現するために線形モデルを拡張した<strong>一般化線形モデル</strong>を構築してみましょう.</p>
<h3 data-number="3.3" id="一般化線形モデルglm"><span class="header-section-number">3.3</span> 一般化線形モデル(GLM)</h3>
<p><strong>一般化線形モデル(Generalized Linear Model, GLM)</strong>は,線形回帰分析を一般化した統計モデルです. 線形回帰分析では以下の式における</p>
<p><span class="math display">
Y_i = \beta_0 + \beta_1 X_i + \epsilon_i \quad (i=1,2,\dots,n)
</span></p>
<p><strong>誤差項</strong> <span class="math inline">\epsilon_i</span> のみが<strong>正規分布</strong>に従うと仮定して分析を実施していました.</p>
<p><span class="math display">
\epsilon_i \sim N(0, \sigma^2)
</span></p>
<p>これに対し,一般化線形モデルは以下の3つの要素から構成されます:</p>
<ol type="1">
<li><strong>確率分布</strong>: 目的変数<span class="math inline">y_i</span>が従う確率分布(正規分布,ポアソン分布,二項分布など)</li>
<li><strong>線形予測子</strong>: 説明変数の線形結合<span class="math inline">\eta_i = \beta_0 + \beta_1 X_{1i} + \beta_2 X_{2i} + \cdots</span></li>
<li><strong>リンク関数</strong>: 目的変数の期待値<span class="math inline">E[y_i]</span>と線形予測子<span class="math inline">\eta_i</span>を結び付ける関数<span class="math inline">g(E[y_i]) = \eta_i</span></li>
</ol>
<p>線形回帰は,一般化線形モデルの特殊ケース(正規分布 + 恒等リンク関数)として捉えることができます. 一般化線形モデルにより,正規分布以外の分布や,非線形な関係もモデル化できるようになります.</p>
<p>ベイズ統計学のモデルにおいて最尤推定する対象は, モデルの母数 <span class="math inline">\theta</span>であり,
データがどのような分布であるかを観察し,それを再現するモデルを構築します.</p>
<p>そこで,今回の目的変数であるGPAの分布を確認してみましょう.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">5</span>))</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>sns.kdeplot(df[<span class="st">&quot;GPA&quot;</span>], fill<span class="op">=</span><span class="va">True</span>, bw_adjust<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">&quot;Kernel Density Estimate of GPA&quot;</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">&quot;GPA&quot;</span>)</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">&quot;Density&quot;</span>)</span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>plt.grid(<span class="va">True</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>plt.savefig(<span class="st">'kde-gpa.png'</span>)</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>plt.close()</span></code></pre></div>
<p><img src="../images/slds/ch12/gpa-kde.png" /></p>
<p>カーネル密度プロットの結果から,今回のデータは正規分布していると仮定して問題なさそうです.</p>
<p><span class="math display"> y_i \sim N(\mu_i, \sigma^2) </span></p>
<p>ここで,今回は正規分布の母数である平均 <span class="math inline">\mu</span> を推定する目的としてみます.</p>
<div class="warn">
<p>モデルの設計では分散 <span class="math inline">\sigma</span> を推定の目的とすることも可能ですが,単純化のために固定とします.
今回は説明用の単純なモデルですが,必要に応じてより複雑なモデルを資料に追加していきます.</p>
</div>
<p>このとき,説明変数としてgpaに影響するもの配下のように仮定します.</p>
<ul>
<li><span class="math inline">S_t</span> (Study Hours): 勉強時間</li>
<li><span class="math inline">S_s</span> (Scholarship): 奨学金</li>
<li><span class="math inline">\alpha_i</span>: 学生個別のもともとの学力（ランダム切片）</li>
</ul>
<p>これらの変数を用いて, <span class="math inline">\mu</span> を表す式を以下のように立てます.</p>
<p><span class="math display">\mu_i = \alpha_i + \beta_{st} \cdot S_t + \beta_{ss} \cdot S_s</span></p>
<p>このような式を,<strong>線形予測子(linear predictor)</strong>といいます.</p>
<div class="note">
<p>線形予測子は,説明変数とパラメータの線形結合で構成される式です.</p>
<p>一般化線形モデル(GLM)では,線形予測子<span class="math inline">\eta_i</span>と目的変数の期待値<span class="math inline">E[y_i]</span>をリンク関数<span class="math inline">g(\cdot)</span>で結び付けます:</p>
<p><span class="math display">g(E[y_i]) = \eta_i = \alpha_i + \beta_{st} \cdot S_t + \beta_{ss} \cdot S_s</span></p>
<p>今回は正規分布を仮定しているため,恒等リンク関数(identity link function) <span class="math inline">g(x) = x</span> を使用します. 恒等リンク関数では,</p>
<p><span class="math display">g(E[y_i]) = E[y_i] = \mu_i = \eta_i</span></p>
<p>となり,<span class="math inline">\mu_i = E[y_i] = \eta_i</span>という関係が成り立ちます. これにより,線形回帰と同じ形式になります.</p>
<p>他の分布では,例えば:</p>
<ul>
<li>ポアソン分布: 対数リンク関数 <span class="math inline">g(E[y_i]) = \log(E[y_i]) = \eta_i</span></li>
<li>二項分布: ロジットリンク関数 <span class="math inline">g(E[y_i]) = \log\left(\frac{E[y_i]}{1-E[y_i]}\right) = \eta_i</span></li>
</ul>
<p>などが用いられます.</p>
<p>今回は線形モデルとの接続のために,正規分布を仮定していますが,そもそもデータが正規分布では説明できないことが明らかな場合には,適した分布を選択することでモデルの精度が格段に上がります.</p>
</div>
<p>ここで,解くべき問題は,</p>
<p><span class="math display">E[y_i] = \mu_i = \alpha_i + \beta_{st} \cdot S_t + \beta_{ss} \cdot S_s</span></p>
<p>となるような<span class="math inline">\alpha_i, \beta_{st}, \beta_{ss}</span>を推定することです.</p>
<h3 data-number="3.4" id="個体差のモデリング"><span class="header-section-number">3.4</span> 個体差のモデリング</h3>
<p>恒等リンク関数を用いたモデリングでは,線形回帰と同じ形式の式となりますが,
一般化線形モデルでは全体の分布を仮定しているため個体差をモデルに取り組むことが可能となります.</p>
<h4 data-number="3.4.1" id="線形回帰での問題"><span class="header-section-number">3.4.1</span> 線形回帰での問題</h4>
<p>個体差をモデルに取り込もうとする場合,線形回帰では以下のような問題が発生します:</p>
<ul>
<li><p><strong>ダミー変数の導入</strong>: 各学生の個別の学力を推定するために,学生ごとにダミー変数を導入する方法が考えられます.</p></li>
<li><p><strong>データ数が少ない場合の問題</strong>: 各学生毎にデータ数が少ない場合(今回は1人1データ),各ダミー変数は誤差と等しくなってしまいます.</p></li>
<li><p><strong>過学習の問題</strong>: 結果として,データ = 予測値 (<span class="math inline">\hat{y}_i = y_i</span>) としているのと変わらなくなります. これは推定の意味がなく,<strong>過学習</strong>を起こしている状態です.</p></li>
</ul>
<h4 data-number="3.4.2" id="ベイズ推論による解決"><span class="header-section-number">3.4.2</span> ベイズ推論による解決</h4>
<p>ベイズ統計学では,この問題を以下のように解決します:</p>
<ul>
<li><p><strong>階層構造の導入</strong>: 各学生の学力<span class="math inline">\alpha_i</span>に対して,事前分布として正規分布<span class="math inline">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>を設定します. これにより,<strong>階層ベイズモデル(hierarchical Bayesian model)</strong>または<strong>ランダム効果モデル(random effects model)</strong>を構築します.</p></li>
<li><p><strong>分布の母数を推定</strong>: 推定するのは個別の学力<span class="math inline">\alpha_i</span>ではなく,その分布の母数<span class="math inline">(\mu_{\alpha}, \sigma_{\alpha})</span>です. これにより,データ数が少なくても,全体の分布から情報を借用(<strong>borrowing strength</strong>, 情報の共有)して推定を行うことができます. 各学生のデータが少なくても,他の学生のデータと組み合わせることで,より安定した推定が可能になります.</p></li>
<li><p><strong>新しいデータへの対応</strong>: 分布の母数<span class="math inline">(\mu_{\alpha}, \sigma_{\alpha})</span>が推定されているため,新しい学生のデータが得られた場合でも,推定された分布<span class="math inline">N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>に基づいて学力を推定することが可能になります. これにより,汎化性能の高いモデルを構築できます.</p></li>
</ul>
<p><img src="../images/slds/ch12/random_image.png" /></p>
<p>それでは実際に,モデルを構築してみましょう.</p>
<h4 data-number="3.4.3" id="モデルの仮定"><span class="header-section-number">3.4.3</span> モデルの仮定</h4>
<p>階層ベイズモデルでは,仮説に従って以下のように分布を設定します.</p>
<ul>
<li><strong>仮説: 基礎学力は学生によって異なる</strong>
<ul>
<li>個別に異なる<span class="math inline">\alpha_i</span></li>
</ul></li>
<li><strong>仮定: 基礎学力は正規分布に従う</strong>
<ul>
<li><span class="math inline">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span></li>
<li>これを<strong>事前分布(prior distribution)</strong>といいます.</li>
</ul></li>
<li><strong>更にそれぞれの母数の分布を仮定する</strong>
<ul>
<li><span class="math inline">\mu_{\alpha} \sim N(0, 1)</span></li>
<li><span class="math inline">\sigma_{\alpha} \sim HN(1)</span></li>
<li>ここで,<span class="math inline">HN</span>は<strong>半正規分布(Half-Normal distribution)</strong>を表します.</li>
</ul></li>
</ul>
<p><img src="../images/slds/ch12/dist_setting.png" /></p>
<p>このように,パラメータの分布のパラメータ(超パラメータ)についても分布を仮定することで,階層的な構造を持つベイズモデルを構築します. これは<strong>階層事前分布(hierarchical prior)</strong>と呼ばれます.</p>
<h4 data-number="3.4.4" id="ランダム傾きとランダム切片"><span class="header-section-number">3.4.4</span> ランダム傾きとランダム切片</h4>
<p>これまで,学生の基礎学力の違い(ランダム切片<span class="math inline">\alpha_i</span>)について考えてきましたが,他にも個体差として考慮できる要素があります.</p>
<ul>
<li><strong>仮説: 勉強時間による効果は人によって違う</strong></li>
</ul>
<p>ただし,学生個別の個体差<span class="math inline">\alpha_i</span>(ランダム切片)のように,学生個別の勉強時間あたりのGPAの上昇率<span class="math inline">\beta_{st,i} \cdot S_t</span>という推定は,今回のデータではモデル化<strong>できません</strong>.</p>
<p>このような個別の傾きを<strong>ランダム傾き(random slope)</strong>といいます.</p>
<h5 data-number="3.4.4.1" id="なぜモデル化できないのか"><span class="header-section-number">3.4.4.1</span> なぜモデル化できないのか?</h5>
<ul>
<li><p><strong>学生1人につき1つしかデータがない</strong>: 今回のデータでは,各学生について1つの観測値しかありません.</p></li>
<li><p><strong>切片はそれぞれの値から推定できる</strong>: 切片<span class="math inline">\alpha_i</span>は,以下のように各学生のデータから直接推定できます:
<span class="math display">\alpha_i = y_i - \beta_{st} \cdot S_t - \beta_{ss} \cdot S_s</span></p></li>
<li><p><strong>傾きは複数の観測点が必要</strong>: 傾きは「変化の傾向(勾配)」なので,推定には複数の観測点が必要です. 傾き<span class="math inline">\beta_{st,i}</span>を推定するには,少なくとも2点以上のデータが必要で,以下のように計算されます:
<span class="math display">\beta_{st,i} = \frac{y_i - y_j}{S_{t,i} - S_{t,j}}</span></p></li>
</ul>
<p>そのため,1人1データしかない今回のケースでは,学生ごとのランダム傾きを推定することはできません. ランダム傾きをモデル化するには,各学生について複数の時点でのデータ(縦断データ)が必要になります.</p>
<p><img src="../images/slds/ch12/random_slope.png" /></p>
<p>したがって今回は,学生個別の学習能力は全体で同一(固定効果)として扱います. また,奨学金についても同様に学生ごとの差は無いものとして扱います(ランダム傾きモデルに関しては,今後学生の需要があれば資料に追加します.)</p>
<h3 data-number="3.5" id="モデル全体像"><span class="header-section-number">3.5</span> モデル全体像</h3>
<p>これまで説明してきた内容を整理すると,以下のような階層ベイズモデルになります:</p>
<h4 data-number="3.5.1" id="推定する分布"><span class="header-section-number">3.5.1</span> 推定する分布</h4>
<p><span class="math display">y_i \sim N(\mu_i, \sigma^2)</span></p>
<p>ここで,<span class="math inline">y_i</span>は学生<span class="math inline">i</span>のGPAです. 誤差の標準偏差<span class="math inline">\sigma</span>は,今回は固定値<span class="math inline">\sigma = 0.3</span>として扱います.</p>
<h4 data-number="3.5.2" id="線形予測子"><span class="header-section-number">3.5.2</span> 線形予測子</h4>
<p><span class="math display">\mu_i = \alpha_i + \beta_{st} \cdot S_t + \beta_{ss} \cdot S_s</span></p>
<p>ここで:</p>
<ul>
<li><span class="math inline">S_t</span>は勉強時間(Study Hours)</li>
<li><span class="math inline">S_s</span>は奨学金受給の有無(Scholarship, 0または1)</li>
<li><span class="math inline">\alpha_i</span>は学生<span class="math inline">i</span>の基礎学力(ランダム切片)</li>
<li><span class="math inline">\beta_{st}</span>は勉強時間の効果(固定効果)</li>
<li><span class="math inline">\beta_{ss}</span>は奨学金の効果(固定効果)</li>
</ul>
<h4 data-number="3.5.3" id="尤度関数"><span class="header-section-number">3.5.3</span> 尤度関数</h4>
<p><span class="math display">p(y | \alpha, \beta_{st}, \beta_{ss}, \sigma) = \prod_{i=1}^{N} N(y_i | \mu_i, \sigma^2)</span></p>
<p>ここで,<span class="math inline">\mu_i = \alpha_i + \beta_{st} \cdot S_t + \beta_{ss} \cdot S_s</span>です.</p>
<h4 data-number="3.5.4" id="事前分布"><span class="header-section-number">3.5.4</span> 事前分布</h4>
<h5 data-number="3.5.4.1" id="ランダム切片階層構造"><span class="header-section-number">3.5.4.1</span> ランダム切片(階層構造)</h5>
<ul>
<li><p><span class="math inline">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>: 学生個別の基礎学力(ランダム切片)</p>
<p>これは非中心パラメータ化により以下のように表現されます:</p>
<p><span class="math display">z_{\alpha,i} \sim N(0, 1)</span>
<span class="math display">\alpha_i = \mu_{\alpha} + \sigma_{\alpha} \cdot z_{\alpha,i}</span></p></li>
</ul>
<div class="note">
<ul>
<li>非中心パラメータ化(non-centered parameterization)</li>
</ul>
<p>階層ベイズモデルにおいて,ランダム切片<span class="math inline">\alpha_i</span>を表現する方法には2通りあります:</p>
<ol type="1">
<li><p><strong>中心パラメータ化(centered parameterization)</strong>:
<span class="math display">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>
これは,直接<span class="math inline">\alpha_i</span>を平均<span class="math inline">\mu_{\alpha}</span>と標準偏差<span class="math inline">\sigma_{\alpha}</span>を持つ正規分布から生成する方法です.</p></li>
<li><p><strong>非中心パラメータ化(non-centered parameterization)</strong>:
<span class="math display">z_{\alpha,i} \sim N(0, 1)</span>
<span class="math display">\alpha_i = \mu_{\alpha} + \sigma_{\alpha} \cdot z_{\alpha,i}</span>
これは,標準正規分布<span class="math inline">N(0, 1)</span>から<span class="math inline">z_{\alpha,i}</span>を生成し,それを線形変換して<span class="math inline">\alpha_i</span>を生成する方法です.</p></li>
</ol>
<p>数学的には,両者は同じ分布<span class="math inline">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>を生成しますが,MCMCサンプリングにおける挙動が大きく異なります.</p>
<p><strong>MCMCサンプリングにおける効果:</strong></p>
<p>非中心パラメータ化は,以下の理由からMCMCサンプリングの効率を大幅に向上させます:</p>
<ol type="1">
<li><p><strong>パラメータ間の相関の低減</strong>: 中心パラメータ化では,<span class="math inline">\alpha_i</span>と<span class="math inline">(\mu_{\alpha}, \sigma_{\alpha})</span>の間に強い相関が生じます. 特に,<span class="math inline">\sigma_{\alpha}</span>が小さい場合,<span class="math inline">\alpha_i</span>の値は<span class="math inline">\mu_{\alpha}</span>に近くなり,パラメータ空間の探索が困難になります. 非中心パラメータ化では,<span class="math inline">z_{\alpha,i}</span>は標準正規分布から独立に生成されるため,パラメータ間の相関が低減され,効率的な探索が可能になります.</p></li>
<li><p><strong>ファネル形状の問題の回避</strong>: 階層モデルでは,パラメータ空間が「ファネル(funnel)」形状になることがあります. <span class="math inline">\sigma_{\alpha}</span>が小さい領域では,許容される<span class="math inline">\alpha_i</span>の範囲が狭くなり,サンプリングが困難になります. 非中心パラメータ化により,この問題を回避できます.</p></li>
<li><p><strong>収束の改善</strong>: パラメータ間の相関が低減されることで,マルコフ連鎖がより速く混合し,収束が改善されます. 特に,階層モデルでは,非中心パラメータ化を使用することで,発散(divergence)の発生を大幅に減らすことができます.</p></li>
<li><p><strong>ESS(Effective Sample Size)の向上</strong>: パラメータ間の相関が低減されることで,サンプル間の自己相関が減少し,実質的なサンプルサイズ(ESS)が向上します. これにより,より少ないサンプル数で,より信頼性の高い推定が可能になります.</p></li>
</ol>
<p>ただし,すべての場合に非中心パラメータ化が最適というわけではありません. データが十分に多い場合や,<span class="math inline">\sigma_{\alpha}</span>が大きい場合には,中心パラメータ化でも問題なく動作することがあります. しかし,一般的には,階層モデルでは非中心パラメータ化を使用することが推奨されます.</p>
</div>
<h5 data-number="3.5.4.2" id="固定効果"><span class="header-section-number">3.5.4.2</span> 固定効果</h5>
<ul>
<li><span class="math inline">\beta_{st} \sim N(0, 1)</span>: 勉強時間の効果(固定効果, 学生間で同じ値)</li>
<li><span class="math inline">\beta_{ss} \sim N(0, 1)</span>: 奨学金の効果(固定効果, 学生間で同じ値)</li>
</ul>
<h4 data-number="3.5.5" id="超事前分布"><span class="header-section-number">3.5.5</span> 超事前分布</h4>
<ul>
<li><span class="math inline">\mu_{\alpha} \sim N(0, 1)</span>: ランダム切片の平均</li>
<li><span class="math inline">\sigma_{\alpha} \sim HN(1)</span>: ランダム切片の標準偏差(半正規分布)</li>
</ul>
<p>以下は後述のコードから生成されるモデル全体像を表す図です.</p>
<p><img src="../images/slds/ch12/hierarchical_bayes_model.png" /></p>
<p>このモデルにより,学生間の基礎学力の個体差を考慮しながら(ランダム切片),勉強時間と奨学金がGPAに与える影響を固定効果として推定することができます.</p>
<h2 data-number="4" id="環境構築"><span class="header-section-number">4</span> 環境構築</h2>
<p>それでは,実際にプログラム上でこのモデルを構築,推定してみましょう.</p>
<p><code>Python</code>においてベイズ統計学に基づいた統計モデリングを実施するためのライブラリとして<code>PyMC5</code>があります(古いVersionとして<code>PyMC3</code>があり全く異なる記法などを用いているので注意して下さい). こちらのライブラリは,Pythonのパッケージマネージャーである<code>anaconda</code>を利用します.
通常これまでに利用してきた<code>pip</code>による環境と<code>anaconda</code>環境の併用は困難です. ただし,この講義では,<code>pyenv</code>を環境構築に利用していますので, <code>PyMC</code>を利用するためのディレクトリのlocal環境にのみ<code>anaconda</code>用の環境を構築することが可能です.</p>
<p>まずは,ターミナルで<code>PyMC</code>を実行する用のディレクトリを作成し,そこに移動して下さい.</p>
<p>以下, 移動したディレクトリ内に<code>anaconda</code>環境を構築していきます.</p>
<h3 data-number="4.1" id="anacondaのインストール"><span class="header-section-number">4.1</span> <code>anaconda</code>のインストール</h3>
<div class="warn">
<p><code>MacOS</code>の方は <code>pyenv install -l</code> で<code>anaconda</code>系統の環境が表示されるので <code>pyenv install anacondaXXX</code> でインストール可能です.</p>
<p>執筆時点の最新版 <code>anaconda3-2024.10-1</code>は<code>PyMC5</code>が対応していないため,<code>anaconda3-2024.02-1</code>が推奨されます.</p>
</div>
<p><code>Windows</code>の場合は <code>pyenv</code> でのインストールが提供されていないので,手動でインストールする必要があります.</p>
<p><code>anaconda</code>の<a href="https://www.anaconda.com/download#Downloads">公式ページ</a>に行き,<code>Get Started</code>から指示に従ってアカウント等を作成して下さい.</p>
<p><img src="../images/slds/ch12/anaconda/anaconda-HP.png" /></p>
<p>続いて, Windows版の<code>anaconda</code>のインストーラーをダウンロードします.</p>
<p><img src="../images/slds/ch12/anaconda/anaconda-DW.png" /></p>
<p>ダウンロードが完了したら,インストーラーをダブルクリックして起動します.
設定は変更せず<code>Next</code>をクリックしていきます.</p>
<p><img src="../images/slds/ch12/anaconda/anaconda-Explorer.png" /></p>
<p><img src="../images/slds/ch12/anaconda/anaconda-installer.png" /></p>
<p><img src="../images/slds/ch12/anaconda/anaconda-just-me.png" /></p>
<p>インストール先の設定画面が出たら, <code>pyenv</code>の<code>versions</code>フォルダに保存します.
パスを以下のように指定して次に進みましょう</p>
<p><code>C:\Users\xxx\.pyenv\pyenv-win\versions\anaconda</code>
(XXXの部分を自分のユーザー名にしましょう.)</p>
<p><img src="../images/slds/ch12/anaconda/anaconda-pass.png" /></p>
<p>その後は基本的にデフォルトの設定のまま,<code>Next</code>,<code>Finish</code>を押しましょう.</p>
<p><img src="../images/slds/ch12/anaconda/anaconda-installer-end.png" /></p>
<p><img src="../images/slds/ch12/anaconda/anaconda-installer-end2.png" /></p>
<p><img src="../images/slds/ch12/anaconda/anaconda-installer-end3.png" /></p>
<p>インストールが完了したら,ターミナルを開き作業用フォルダに移動して,<code>pyenv versions</code> コマンドで<code>anaconda</code>がインストールされているか確認しましょう.</p>
<p><img src="../images/slds/ch12/anaconda/anaconda-pyenv.png" /></p>
<p>ローカル環境に<code>anaconda</code>を指定します.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> local anaconda</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> rehash</span></code></pre></div>
<p><code>anaconda</code>では<code>pip</code>ではなく<code>conda</code>を利用してライブラリをインストールします. まずは,<code>anaconda</code>自体を<code>update</code>しましょう.</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> update <span class="at">-n</span> base <span class="at">-c</span> defaults conda</span></code></pre></div>
<div class="sourceCode" id="cb7"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Proceed</span> <span class="er">(</span><span class="ex">[y]/n</span><span class="kw">)</span><span class="ex">?</span> y</span></code></pre></div>
<p>が表示されたら, <code>y</code> を押して<code>Enter</code>します.</p>
<p>続いて<code>PyMC5</code>の公式サイトに従い,<code>anaconda</code>上で<code>PyMC5</code>をインストールします.
<code>pyenv</code>が既に仮想環境ですが,<code>conda create</code>コマンドによって <code>pymc</code>などがインストールされた<code>python</code>の仮想環境を更に構築します.</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> create <span class="at">-c</span> conda-forge <span class="at">-n</span> pymc_env <span class="st">&quot;pymc&gt;=5&quot;</span></span></code></pre></div>
<p>入力後完了したら,仮想環境を有効化するためにterminalを一度初期化します.</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> init powershell</span></code></pre></div>
<div class="warn">
<p><code>MacOS</code>の場合は, <code>conda init zsh</code> コマンドです.</p>
</div>
<p>を入力し, <strong><code>Terminal</code>を閉じて再度作業ディレクトリに移動したあと</strong>に仮想環境を有効化する以下のコマンドを入力して下さい.</p>
<div class="sourceCode" id="cb10"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> activate pymc_env</span></code></pre></div>
<div class="warn">
<p>Macの場合は, このあと仮想環境をlocalに指定します.</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="ex">pyenv</span> versions</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>  <span class="ex">system</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>  <span class="ex">3.12.3</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="ex">*</span> anaconda3-2024.02-1</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>  <span class="ex">anaconda3-2024.02-1/envs/pymc_env</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="ex">❯</span> pyenv local anaconda3-2024.02-1/envs/pymc_env</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="ex">❯</span> pyenv local</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a><span class="ex">anaconda3-2024.02-1/envs/pymc_env</span></span></code></pre></div>
</div>
<p>環境の確認をします.</p>
<div class="sourceCode" id="cb12"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Get-Command</span> python</span></code></pre></div>
<div class="warn">
<p><code>MacOS</code>の場合は, <code>witch python</code> コマンドです.</p>
</div>
<p>以下のように<code>pymc_env</code>内のpythonが表示されれば利用可能な状況になっています.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="ex">CommandType</span>     Name                                               Version    Source</span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="ex">-----------</span>     <span class="at">----</span>                                               <span class="at">-------</span>    <span class="at">------</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Application</span>     python.exe                                         3.11.14... C:<span class="dt">\U</span>sers<span class="dt">\a</span>kagi<span class="dt">\.</span>pyenv<span class="dt">\p</span>yenv-win<span class="dt">\v</span>ersions<span class="dt">\a</span>naconda<span class="dt">\e</span>nvs<span class="dt">\p</span>ymc_env<span class="dt">\p</span>ython.exe</span></code></pre></div>
<p>必要なライブラリをインストールします.</p>
<div class="sourceCode" id="cb14"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install seaborn scikit-learn statsmodels</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="ex">conda</span> install <span class="at">-c</span> conda-forge compilers</span></code></pre></div>
<p>途中で,<code>Proceed ([y]/n)?</code> と表示されたら<code>y</code>と入力してENTERを押します.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Preparing</span> transaction: done</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Verifying</span> transaction: done</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="ex">Executing</span> transaction: done</span></code></pre></div>
<p>などが表示されれば,環境構築完了です.</p>
<p>以降,Terminalで作業ディレクトリに移動した後<code>conda activate pymc_env</code> でこの環境が利用できるようになります.</p>
<h2 data-number="5" id="pymc-実装"><span class="header-section-number">5</span> PyMC 実装</h2>
<p>コードが複雑になるため,先にコードの全体像を示した後, 個別に意味を解説します.
まずは,以下のコードを作業ディレクトリに配置した後,実行してみましょう.</p>
<div class="warn">
<p>最初の画像の保存先やデータの参照先は,環境に応じて変えて下さい.</p>
<p><code>save_dir = '/images/slds/ch12/'</code></p>
</div>
<div class="sourceCode" id="cb16"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz</span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pymc <span class="im">import</span> model_to_graphviz</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, mean_absolute_error</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm, rankdata</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>save_dir <span class="op">=</span> <span class="st">'../../images/slds/ch12/'</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. データ読み込み</span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_csv(<span class="st">'hierarchical_regression.csv'</span></span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>                ,dtype<span class="op">=</span>{<span class="st">'GPA'</span>: <span class="bu">float</span></span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Scholarship'</span>: <span class="bu">bool</span></span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Study_Hours'</span>: <span class="bu">float</span></span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Sports_hours'</span>: <span class="bu">float</span></span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Part_time_Work'</span>: <span class="bu">float</span></span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'StudentID'</span>:<span class="bu">int</span>})</span>
<span id="cb16-28"><a href="#cb16-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-29"><a href="#cb16-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 数量化</span></span>
<span id="cb16-30"><a href="#cb16-30" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.get_dummies(df, columns<span class="op">=</span>[<span class="st">'Scholarship'</span>], dtype<span class="op">=</span><span class="st">'int'</span>)</span>
<span id="cb16-31"><a href="#cb16-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-32"><a href="#cb16-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">#標準化</span></span>
<span id="cb16-33"><a href="#cb16-33" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb16-34"><a href="#cb16-34" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb16-35"><a href="#cb16-35" aria-hidden="true" tabindex="-1"></a>    df[[<span class="st">'Scholarship_True'</span></span>
<span id="cb16-36"><a href="#cb16-36" aria-hidden="true" tabindex="-1"></a>       ,<span class="st">'Study_Hours'</span></span>
<span id="cb16-37"><a href="#cb16-37" aria-hidden="true" tabindex="-1"></a>       ,<span class="st">'Part_time_Work'</span></span>
<span id="cb16-38"><a href="#cb16-38" aria-hidden="true" tabindex="-1"></a>       ,<span class="st">'Sports_hours'</span>]] <span class="op">=</span> scaler.fit_transform(df[[<span class="st">'Scholarship_True'</span></span>
<span id="cb16-39"><a href="#cb16-39" aria-hidden="true" tabindex="-1"></a>                                                   ,<span class="st">'Study_Hours'</span></span>
<span id="cb16-40"><a href="#cb16-40" aria-hidden="true" tabindex="-1"></a>                                                   ,<span class="st">'Part_time_Work'</span></span>
<span id="cb16-41"><a href="#cb16-41" aria-hidden="true" tabindex="-1"></a>                                                   ,<span class="st">'Sports_hours'</span>]])</span>
<span id="cb16-42"><a href="#cb16-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-43"><a href="#cb16-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-44"><a href="#cb16-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ２. 線形回帰</span></span>
<span id="cb16-45"><a href="#cb16-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-46"><a href="#cb16-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-47"><a href="#cb16-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 説明変数(X)と目的変数(y)に分割</span></span>
<span id="cb16-48"><a href="#cb16-48" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df[[<span class="st">'Scholarship_True'</span>, <span class="st">'Study_Hours'</span>]]</span>
<span id="cb16-49"><a href="#cb16-49" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> df[<span class="st">'GPA'</span>]</span>
<span id="cb16-50"><a href="#cb16-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-51"><a href="#cb16-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-52"><a href="#cb16-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 切片(定数項)を追加</span></span>
<span id="cb16-53"><a href="#cb16-53" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> sm.add_constant(X)</span>
<span id="cb16-54"><a href="#cb16-54" aria-hidden="true" tabindex="-1"></a>    lm <span class="op">=</span> sm.OLS(y, X).fit()</span>
<span id="cb16-55"><a href="#cb16-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">[通常の線形回帰の結果]</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb16-56"><a href="#cb16-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(lm.summary())</span>
<span id="cb16-57"><a href="#cb16-57" aria-hidden="true" tabindex="-1"></a>    lm_preds <span class="op">=</span> lm.predict(X)</span>
<span id="cb16-58"><a href="#cb16-58" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-59"><a href="#cb16-59" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb16-60"><a href="#cb16-60" aria-hidden="true" tabindex="-1"></a>    plt.scatter(df[<span class="st">'GPA'</span>], lm_preds, alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolors<span class="op">=</span><span class="st">&quot;k&quot;</span>)</span>
<span id="cb16-61"><a href="#cb16-61" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Predicted&quot;</span>)</span>
<span id="cb16-62"><a href="#cb16-62" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Actual&quot;</span>)</span>
<span id="cb16-63"><a href="#cb16-63" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Actual vs. Predicted&quot;</span>)</span>
<span id="cb16-64"><a href="#cb16-64" aria-hidden="true" tabindex="-1"></a>    plt.plot([df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()], [df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()], color<span class="op">=</span><span class="st">&quot;red&quot;</span>, linestyle<span class="op">=</span><span class="st">&quot;--&quot;</span>)  <span class="co"># 完全一致のライン</span></span>
<span id="cb16-65"><a href="#cb16-65" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span>
<span id="cb16-66"><a href="#cb16-66" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'lm.png'</span>)</span>
<span id="cb16-67"><a href="#cb16-67" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-68"><a href="#cb16-68" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-69"><a href="#cb16-69" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb16-70"><a href="#cb16-70" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(lm_preds, label <span class="op">=</span> <span class="st">'Predicted'</span>)</span>
<span id="cb16-71"><a href="#cb16-71" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(df[<span class="st">'GPA'</span>], label <span class="op">=</span> <span class="st">'Actual'</span>)</span>
<span id="cb16-72"><a href="#cb16-72" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Actual/Predicted'</span>)</span>
<span id="cb16-73"><a href="#cb16-73" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'GPA'</span>)</span>
<span id="cb16-74"><a href="#cb16-74" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb16-75"><a href="#cb16-75" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb16-76"><a href="#cb16-76" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'lm_kde.png'</span>)</span>
<span id="cb16-77"><a href="#cb16-77" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-78"><a href="#cb16-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-79"><a href="#cb16-79" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-80"><a href="#cb16-80" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ３. 一般化線形モデル</span></span>
<span id="cb16-81"><a href="#cb16-81" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-82"><a href="#cb16-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-83"><a href="#cb16-83" aria-hidden="true" tabindex="-1"></a>    coords <span class="op">=</span> {</span>
<span id="cb16-84"><a href="#cb16-84" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;student&quot;</span>: df[<span class="st">&quot;StudentID&quot;</span>].values,</span>
<span id="cb16-85"><a href="#cb16-85" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;obs_id&quot;</span>: np.arange(<span class="bu">len</span>(df))</span>
<span id="cb16-86"><a href="#cb16-86" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-87"><a href="#cb16-87" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-88"><a href="#cb16-88" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> pm.Model(coords<span class="op">=</span>coords) <span class="im">as</span> model:</span>
<span id="cb16-89"><a href="#cb16-89" aria-hidden="true" tabindex="-1"></a>        st <span class="op">=</span> pm.Data(<span class="st">&quot;st&quot;</span>, df[<span class="st">&quot;Study_Hours&quot;</span>])</span>
<span id="cb16-90"><a href="#cb16-90" aria-hidden="true" tabindex="-1"></a>        <span class="co">#ptw = pm.Data(&quot;ptw&quot;, df[&quot;Part_time_Work&quot;])</span></span>
<span id="cb16-91"><a href="#cb16-91" aria-hidden="true" tabindex="-1"></a>        ss <span class="op">=</span> pm.Data(<span class="st">&quot;ss&quot;</span>, df[<span class="st">&quot;Scholarship_True&quot;</span>])</span>
<span id="cb16-92"><a href="#cb16-92" aria-hidden="true" tabindex="-1"></a>        student_idx <span class="op">=</span> pm.Data(<span class="st">&quot;student_idx&quot;</span>, df[<span class="st">&quot;StudentID&quot;</span>])</span>
<span id="cb16-93"><a href="#cb16-93" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-94"><a href="#cb16-94" aria-hidden="true" tabindex="-1"></a>        <span class="co">#ランダム切片</span></span>
<span id="cb16-95"><a href="#cb16-95" aria-hidden="true" tabindex="-1"></a>        z_alpha <span class="op">=</span> pm.Normal(<span class="st">&quot;z_alpha&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>, dims<span class="op">=</span><span class="st">&quot;student&quot;</span>)</span>
<span id="cb16-96"><a href="#cb16-96" aria-hidden="true" tabindex="-1"></a>        mu_alpha <span class="op">=</span> pm.Normal(<span class="st">&quot;mu_alpha&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb16-97"><a href="#cb16-97" aria-hidden="true" tabindex="-1"></a>        sigma_alpha <span class="op">=</span> pm.HalfNormal(<span class="st">&quot;sigma_alpha&quot;</span>, <span class="dv">1</span>)</span>
<span id="cb16-98"><a href="#cb16-98" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> pm.Deterministic(<span class="st">&quot;alpha&quot;</span>, mu_alpha <span class="op">+</span> sigma_alpha <span class="op">*</span> z_alpha, dims<span class="op">=</span><span class="st">&quot;student&quot;</span>)</span>
<span id="cb16-99"><a href="#cb16-99" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-100"><a href="#cb16-100" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-101"><a href="#cb16-101" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 勉強時間の効果</span></span>
<span id="cb16-102"><a href="#cb16-102" aria-hidden="true" tabindex="-1"></a>        beta_st <span class="op">=</span> pm.Normal(<span class="st">&quot;beta_st&quot;</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb16-103"><a href="#cb16-103" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-104"><a href="#cb16-104" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 奨学金の効果</span></span>
<span id="cb16-105"><a href="#cb16-105" aria-hidden="true" tabindex="-1"></a>        beta_ss <span class="op">=</span> pm.Normal(<span class="st">&quot;beta_ss&quot;</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb16-106"><a href="#cb16-106" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb16-107"><a href="#cb16-107" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> alpha[student_idx] <span class="op">+</span> beta_st <span class="op">*</span> st <span class="op">+</span>  beta_ss <span class="op">*</span> ss</span>
<span id="cb16-108"><a href="#cb16-108" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-109"><a href="#cb16-109" aria-hidden="true" tabindex="-1"></a>        <span class="co">#sigma = pm.HalfNormal(&quot;sigma&quot;, 1)</span></span>
<span id="cb16-110"><a href="#cb16-110" aria-hidden="true" tabindex="-1"></a>        sigma <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb16-111"><a href="#cb16-111" aria-hidden="true" tabindex="-1"></a>        gpa_obs <span class="op">=</span> pm.Normal(<span class="st">&quot;GPA&quot;</span>,</span>
<span id="cb16-112"><a href="#cb16-112" aria-hidden="true" tabindex="-1"></a>                            mu<span class="op">=</span>mu,</span>
<span id="cb16-113"><a href="#cb16-113" aria-hidden="true" tabindex="-1"></a>                            sigma<span class="op">=</span>sigma,</span>
<span id="cb16-114"><a href="#cb16-114" aria-hidden="true" tabindex="-1"></a>                            observed<span class="op">=</span>df[<span class="st">&quot;GPA&quot;</span>],</span>
<span id="cb16-115"><a href="#cb16-115" aria-hidden="true" tabindex="-1"></a>                            dims<span class="op">=</span><span class="st">&quot;obs_id&quot;</span></span>
<span id="cb16-116"><a href="#cb16-116" aria-hidden="true" tabindex="-1"></a>                        )</span>
<span id="cb16-117"><a href="#cb16-117" aria-hidden="true" tabindex="-1"></a>        trace <span class="op">=</span> pm.sample(draws <span class="op">=</span><span class="dv">2000</span>, tune<span class="op">=</span><span class="dv">2000</span>, chains<span class="op">=</span><span class="dv">4</span>, target_accept<span class="op">=</span><span class="fl">0.95</span>, return_inferencedata<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-118"><a href="#cb16-118" aria-hidden="true" tabindex="-1"></a>        posterior_predictive <span class="op">=</span> pm.sample_posterior_predictive(trace)</span>
<span id="cb16-119"><a href="#cb16-119" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-120"><a href="#cb16-120" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-121"><a href="#cb16-121" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 4. モデル診断と可視化</span></span>
<span id="cb16-122"><a href="#cb16-122" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-123"><a href="#cb16-123" aria-hidden="true" tabindex="-1"></a>    graph <span class="op">=</span> model_to_graphviz(model)</span>
<span id="cb16-124"><a href="#cb16-124" aria-hidden="true" tabindex="-1"></a>    graph.render(filename<span class="op">=</span> save_dir <span class="op">+</span> <span class="st">&quot;hierarchical_bayes_model&quot;</span>, <span class="bu">format</span><span class="op">=</span><span class="st">&quot;pdf&quot;</span>)</span>
<span id="cb16-125"><a href="#cb16-125" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-126"><a href="#cb16-126" aria-hidden="true" tabindex="-1"></a>    az.plot_trace(trace</span>
<span id="cb16-127"><a href="#cb16-127" aria-hidden="true" tabindex="-1"></a>                 ,var_names<span class="op">=</span>[<span class="st">&quot;sigma_alpha&quot;</span></span>
<span id="cb16-128"><a href="#cb16-128" aria-hidden="true" tabindex="-1"></a>                            ,<span class="st">&quot;alpha&quot;</span></span>
<span id="cb16-129"><a href="#cb16-129" aria-hidden="true" tabindex="-1"></a>                            ,<span class="st">&quot;beta_st&quot;</span></span>
<span id="cb16-130"><a href="#cb16-130" aria-hidden="true" tabindex="-1"></a>                            ,<span class="st">&quot;beta_ss&quot;</span>])</span>
<span id="cb16-131"><a href="#cb16-131" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb16-132"><a href="#cb16-132" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'trace.png'</span>)</span>
<span id="cb16-133"><a href="#cb16-133" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-134"><a href="#cb16-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-135"><a href="#cb16-135" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(az.summary(trace</span>
<span id="cb16-136"><a href="#cb16-136" aria-hidden="true" tabindex="-1"></a>                    ,var_names<span class="op">=</span>[<span class="st">&quot;sigma_alpha&quot;</span></span>
<span id="cb16-137"><a href="#cb16-137" aria-hidden="true" tabindex="-1"></a>                               ,<span class="st">&quot;alpha&quot;</span></span>
<span id="cb16-138"><a href="#cb16-138" aria-hidden="true" tabindex="-1"></a>                               ,<span class="st">&quot;beta_st&quot;</span></span>
<span id="cb16-139"><a href="#cb16-139" aria-hidden="true" tabindex="-1"></a>                               ,<span class="st">&quot;beta_ss&quot;</span>]))</span>
<span id="cb16-140"><a href="#cb16-140" aria-hidden="true" tabindex="-1"></a>    summary_df <span class="op">=</span> az.summary(trace</span>
<span id="cb16-141"><a href="#cb16-141" aria-hidden="true" tabindex="-1"></a>                           ,var_names<span class="op">=</span>[<span class="st">&quot;sigma_alpha&quot;</span></span>
<span id="cb16-142"><a href="#cb16-142" aria-hidden="true" tabindex="-1"></a>                                      ,<span class="st">&quot;alpha&quot;</span></span>
<span id="cb16-143"><a href="#cb16-143" aria-hidden="true" tabindex="-1"></a>                                      ,<span class="st">&quot;beta_st&quot;</span></span>
<span id="cb16-144"><a href="#cb16-144" aria-hidden="true" tabindex="-1"></a>                                      ,<span class="st">&quot;beta_ss&quot;</span>])</span>
<span id="cb16-145"><a href="#cb16-145" aria-hidden="true" tabindex="-1"></a>    summary_df.to_csv(<span class="st">&quot;bayesian_summary.csv&quot;</span></span>
<span id="cb16-146"><a href="#cb16-146" aria-hidden="true" tabindex="-1"></a>                     ,encoding<span class="op">=</span><span class="st">&quot;utf-8-sig&quot;</span>)</span>
<span id="cb16-147"><a href="#cb16-147" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-148"><a href="#cb16-148" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-149"><a href="#cb16-149" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-150"><a href="#cb16-150" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 6. 階層ベイズモデルによる予測精度評価</span></span>
<span id="cb16-151"><a href="#cb16-151" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-152"><a href="#cb16-152" aria-hidden="true" tabindex="-1"></a>    idata <span class="op">=</span> trace.copy()</span>
<span id="cb16-153"><a href="#cb16-153" aria-hidden="true" tabindex="-1"></a>    idata.extend(posterior_predictive)</span>
<span id="cb16-154"><a href="#cb16-154" aria-hidden="true" tabindex="-1"></a>    az.plot_ppc(idata, data_pairs<span class="op">=</span>{<span class="st">&quot;GPA&quot;</span>: <span class="st">&quot;GPA&quot;</span>})</span>
<span id="cb16-155"><a href="#cb16-155" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'ppc.png'</span>)</span>
<span id="cb16-156"><a href="#cb16-156" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-157"><a href="#cb16-157" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-158"><a href="#cb16-158" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-159"><a href="#cb16-159" aria-hidden="true" tabindex="-1"></a>    <span class="co">#------------------------------------------------------------------</span></span>
<span id="cb16-160"><a href="#cb16-160" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測値の可視化</span></span>
<span id="cb16-161"><a href="#cb16-161" aria-hidden="true" tabindex="-1"></a>    <span class="co">#------------------------------------------------------------------</span></span>
<span id="cb16-162"><a href="#cb16-162" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-163"><a href="#cb16-163" aria-hidden="true" tabindex="-1"></a>    posterior_mean <span class="op">=</span> idata.posterior_predictive[<span class="st">&quot;GPA&quot;</span>].mean(dim<span class="op">=</span>(<span class="st">&quot;chain&quot;</span>, <span class="st">&quot;draw&quot;</span>)).values</span>
<span id="cb16-164"><a href="#cb16-164" aria-hidden="true" tabindex="-1"></a>    bayes_rmse <span class="op">=</span> mean_squared_error(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean)</span>
<span id="cb16-165"><a href="#cb16-165" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;[階層ベイズモデルのRMSE] </span><span class="sc">{</span>bayes_rmse<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span>
<span id="cb16-166"><a href="#cb16-166" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 事後予測の平均（予測値）を取り出す</span></span>
<span id="cb16-167"><a href="#cb16-167" aria-hidden="true" tabindex="-1"></a>    posterior_mean <span class="op">=</span> idata.posterior_predictive[<span class="st">&quot;GPA&quot;</span>].mean(dim<span class="op">=</span>(<span class="st">&quot;chain&quot;</span>, <span class="st">&quot;draw&quot;</span>)).values</span>
<span id="cb16-168"><a href="#cb16-168" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-169"><a href="#cb16-169" aria-hidden="true" tabindex="-1"></a>    <span class="co"># プロット</span></span>
<span id="cb16-170"><a href="#cb16-170" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb16-171"><a href="#cb16-171" aria-hidden="true" tabindex="-1"></a>    plt.scatter(df[<span class="st">'GPA'</span>], posterior_mean, alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolors<span class="op">=</span><span class="st">&quot;k&quot;</span>)</span>
<span id="cb16-172"><a href="#cb16-172" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Predicted (Bayesian)&quot;</span>)</span>
<span id="cb16-173"><a href="#cb16-173" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Actual GPA&quot;</span>)</span>
<span id="cb16-174"><a href="#cb16-174" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Bayesian Actual vs. Predicted&quot;</span>)</span>
<span id="cb16-175"><a href="#cb16-175" aria-hidden="true" tabindex="-1"></a>    plt.plot([df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()],</span>
<span id="cb16-176"><a href="#cb16-176" aria-hidden="true" tabindex="-1"></a>             [df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()],</span>
<span id="cb16-177"><a href="#cb16-177" aria-hidden="true" tabindex="-1"></a>             color<span class="op">=</span><span class="st">&quot;red&quot;</span>, linestyle<span class="op">=</span><span class="st">&quot;--&quot;</span>, label<span class="op">=</span><span class="st">&quot;Perfect prediction&quot;</span>)</span>
<span id="cb16-178"><a href="#cb16-178" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span>
<span id="cb16-179"><a href="#cb16-179" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb16-180"><a href="#cb16-180" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'bayes.png'</span>)</span>
<span id="cb16-181"><a href="#cb16-181" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-182"><a href="#cb16-182" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-183"><a href="#cb16-183" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb16-184"><a href="#cb16-184" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(posterior_mean, label <span class="op">=</span> <span class="st">'Predicted'</span>)</span>
<span id="cb16-185"><a href="#cb16-185" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(df[<span class="st">'GPA'</span>], label <span class="op">=</span> <span class="st">'Actual'</span>)</span>
<span id="cb16-186"><a href="#cb16-186" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Actual/Predicted'</span>)</span>
<span id="cb16-187"><a href="#cb16-187" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'GPA'</span>)</span>
<span id="cb16-188"><a href="#cb16-188" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb16-189"><a href="#cb16-189" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb16-190"><a href="#cb16-190" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'bayes_kde.png'</span>)</span>
<span id="cb16-191"><a href="#cb16-191" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-192"><a href="#cb16-192" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-193"><a href="#cb16-193" aria-hidden="true" tabindex="-1"></a>    <span class="co">#------------------------------------------------------------------</span></span>
<span id="cb16-194"><a href="#cb16-194" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測範囲の可視化</span></span>
<span id="cb16-195"><a href="#cb16-195" aria-hidden="true" tabindex="-1"></a>    <span class="co">#------------------------------------------------------------------</span></span>
<span id="cb16-196"><a href="#cb16-196" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-197"><a href="#cb16-197" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x軸用の study_hours の範囲（100点）</span></span>
<span id="cb16-198"><a href="#cb16-198" aria-hidden="true" tabindex="-1"></a>    study_grid <span class="op">=</span> np.linspace(df[<span class="st">&quot;Study_Hours&quot;</span>].<span class="bu">min</span>(), df[<span class="st">&quot;Study_Hours&quot;</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb16-199"><a href="#cb16-199" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-200"><a href="#cb16-200" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金なしの予測</span></span>
<span id="cb16-201"><a href="#cb16-201" aria-hidden="true" tabindex="-1"></a>    predict_df_0 <span class="op">=</span> pd.DataFrame({</span>
<span id="cb16-202"><a href="#cb16-202" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Study_Hours&quot;</span>: study_grid,</span>
<span id="cb16-203"><a href="#cb16-203" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Scholarship_True&quot;</span>: <span class="dv">0</span>,</span>
<span id="cb16-204"><a href="#cb16-204" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;StudentID&quot;</span>: <span class="dv">0</span></span>
<span id="cb16-205"><a href="#cb16-205" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb16-206"><a href="#cb16-206" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-207"><a href="#cb16-207" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金ありの予測</span></span>
<span id="cb16-208"><a href="#cb16-208" aria-hidden="true" tabindex="-1"></a>    predict_df_1 <span class="op">=</span> pd.DataFrame({</span>
<span id="cb16-209"><a href="#cb16-209" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Study_Hours&quot;</span>: study_grid,</span>
<span id="cb16-210"><a href="#cb16-210" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Scholarship_True&quot;</span>: <span class="dv">1</span>,</span>
<span id="cb16-211"><a href="#cb16-211" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;StudentID&quot;</span>: <span class="dv">0</span></span>
<span id="cb16-212"><a href="#cb16-212" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb16-213"><a href="#cb16-213" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-214"><a href="#cb16-214" aria-hidden="true" tabindex="-1"></a>    <span class="co"># より簡単な方法：事後分布から直接予測</span></span>
<span id="cb16-215"><a href="#cb16-215" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 事後分布のサンプルを取得（新しいAPIを使用）</span></span>
<span id="cb16-216"><a href="#cb16-216" aria-hidden="true" tabindex="-1"></a>    posterior_samples <span class="op">=</span> az.extract(trace)</span>
<span id="cb16-217"><a href="#cb16-217" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-218"><a href="#cb16-218" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測計算</span></span>
<span id="cb16-219"><a href="#cb16-219" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> <span class="bu">len</span>(posterior_samples.draw)</span>
<span id="cb16-220"><a href="#cb16-220" aria-hidden="true" tabindex="-1"></a>    n_grid <span class="op">=</span> <span class="bu">len</span>(study_grid)</span>
<span id="cb16-221"><a href="#cb16-221" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-222"><a href="#cb16-222" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測値を格納する配列（奨学金なし・あり）</span></span>
<span id="cb16-223"><a href="#cb16-223" aria-hidden="true" tabindex="-1"></a>    predictions_0 <span class="op">=</span> np.zeros((n_samples, n_grid))</span>
<span id="cb16-224"><a href="#cb16-224" aria-hidden="true" tabindex="-1"></a>    predictions_1 <span class="op">=</span> np.zeros((n_samples, n_grid))</span>
<span id="cb16-225"><a href="#cb16-225" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-226"><a href="#cb16-226" aria-hidden="true" tabindex="-1"></a>    <span class="co"># パラメータを一度に取得</span></span>
<span id="cb16-227"><a href="#cb16-227" aria-hidden="true" tabindex="-1"></a>    mu_alpha_vals <span class="op">=</span> posterior_samples.mu_alpha.values</span>
<span id="cb16-228"><a href="#cb16-228" aria-hidden="true" tabindex="-1"></a>    sigma_alpha_vals <span class="op">=</span> posterior_samples.sigma_alpha.values</span>
<span id="cb16-229"><a href="#cb16-229" aria-hidden="true" tabindex="-1"></a>    z_alpha_vals <span class="op">=</span> posterior_samples.z_alpha.values  <span class="co"># 全学生のz_alpha    </span></span>
<span id="cb16-230"><a href="#cb16-230" aria-hidden="true" tabindex="-1"></a>    beta_st_vals <span class="op">=</span> posterior_samples.beta_st.values    </span>
<span id="cb16-231"><a href="#cb16-231" aria-hidden="true" tabindex="-1"></a>    beta_ss_vals <span class="op">=</span> posterior_samples.beta_ss.values</span>
<span id="cb16-232"><a href="#cb16-232" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-233"><a href="#cb16-233" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb16-234"><a href="#cb16-234" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 各サンプルでのパラメータ（学生0の切片を計算）</span></span>
<span id="cb16-235"><a href="#cb16-235" aria-hidden="true" tabindex="-1"></a>        alpha_0 <span class="op">=</span> mu_alpha_vals[i] <span class="op">+</span> sigma_alpha_vals[i] <span class="op">*</span> z_alpha_vals[<span class="dv">0</span>, i]  <span class="co"># 学生0の切片</span></span>
<span id="cb16-236"><a href="#cb16-236" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-237"><a href="#cb16-237" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 奨学金なしの予測値の計算</span></span>
<span id="cb16-238"><a href="#cb16-238" aria-hidden="true" tabindex="-1"></a>        mu_pred_0 <span class="op">=</span> alpha_0 <span class="op">+</span> beta_st_vals[i] <span class="op">*</span> study_grid <span class="op">+</span> beta_ss_vals[i] <span class="op">*</span> predict_df_0[<span class="st">&quot;Scholarship_True&quot;</span>]</span>
<span id="cb16-239"><a href="#cb16-239" aria-hidden="true" tabindex="-1"></a>        predictions_0[i, :] <span class="op">=</span> np.random.normal(mu_pred_0, <span class="fl">0.3</span>)</span>
<span id="cb16-240"><a href="#cb16-240" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb16-241"><a href="#cb16-241" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 奨学金ありの予測値の計算</span></span>
<span id="cb16-242"><a href="#cb16-242" aria-hidden="true" tabindex="-1"></a>        mu_pred_1 <span class="op">=</span> alpha_0 <span class="op">+</span> beta_st_vals[i] <span class="op">*</span> study_grid <span class="op">+</span> beta_ss_vals[i] <span class="op">*</span> predict_df_1[<span class="st">&quot;Scholarship_True&quot;</span>]</span>
<span id="cb16-243"><a href="#cb16-243" aria-hidden="true" tabindex="-1"></a>        predictions_1[i, :] <span class="op">=</span> np.random.normal(mu_pred_1, <span class="fl">0.3</span>)</span>
<span id="cb16-244"><a href="#cb16-244" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-245"><a href="#cb16-245" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 平均と予測区間（94%）</span></span>
<span id="cb16-246"><a href="#cb16-246" aria-hidden="true" tabindex="-1"></a>    mean_pred_0 <span class="op">=</span> np.mean(predictions_0, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-247"><a href="#cb16-247" aria-hidden="true" tabindex="-1"></a>    lower_pred_0 <span class="op">=</span> np.percentile(predictions_0, <span class="dv">3</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-248"><a href="#cb16-248" aria-hidden="true" tabindex="-1"></a>    upper_pred_0 <span class="op">=</span> np.percentile(predictions_0, <span class="dv">97</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-249"><a href="#cb16-249" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-250"><a href="#cb16-250" aria-hidden="true" tabindex="-1"></a>    mean_pred_1 <span class="op">=</span> np.mean(predictions_1, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-251"><a href="#cb16-251" aria-hidden="true" tabindex="-1"></a>    lower_pred_1 <span class="op">=</span> np.percentile(predictions_1, <span class="dv">3</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-252"><a href="#cb16-252" aria-hidden="true" tabindex="-1"></a>    upper_pred_1 <span class="op">=</span> np.percentile(predictions_1, <span class="dv">97</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb16-253"><a href="#cb16-253" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-254"><a href="#cb16-254" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 実データも合わせて表示（奨学金の有無で色分け）</span></span>
<span id="cb16-255"><a href="#cb16-255" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb16-256"><a href="#cb16-256" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-257"><a href="#cb16-257" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金なしの予測線</span></span>
<span id="cb16-258"><a href="#cb16-258" aria-hidden="true" tabindex="-1"></a>    plt.plot(study_grid, mean_pred_0, color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, label<span class="op">=</span><span class="st">&quot;Bayesian prediction (Scholarship=0)&quot;</span>)</span>
<span id="cb16-259"><a href="#cb16-259" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(study_grid, lower_pred_0, upper_pred_0, color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">&quot;94% CI (Scholarship=0)&quot;</span>)</span>
<span id="cb16-260"><a href="#cb16-260" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-261"><a href="#cb16-261" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金ありの予測線</span></span>
<span id="cb16-262"><a href="#cb16-262" aria-hidden="true" tabindex="-1"></a>    plt.plot(study_grid, mean_pred_1, color<span class="op">=</span><span class="st">&quot;green&quot;</span>, label<span class="op">=</span><span class="st">&quot;Bayesian prediction (Scholarship=1)&quot;</span>)</span>
<span id="cb16-263"><a href="#cb16-263" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(study_grid, lower_pred_1, upper_pred_1, color<span class="op">=</span><span class="st">&quot;green&quot;</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">&quot;94% CI (Scholarship=1)&quot;</span>)</span>
<span id="cb16-264"><a href="#cb16-264" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-265"><a href="#cb16-265" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金の有無で実測値を色分け</span></span>
<span id="cb16-266"><a href="#cb16-266" aria-hidden="true" tabindex="-1"></a>    scholarship_0 <span class="op">=</span> df[df[<span class="st">&quot;Scholarship_True&quot;</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb16-267"><a href="#cb16-267" aria-hidden="true" tabindex="-1"></a>    scholarship_1 <span class="op">=</span> df[df[<span class="st">&quot;Scholarship_True&quot;</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb16-268"><a href="#cb16-268" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-269"><a href="#cb16-269" aria-hidden="true" tabindex="-1"></a>    plt.scatter(scholarship_0[<span class="st">&quot;Study_Hours&quot;</span>], scholarship_0[<span class="st">&quot;GPA&quot;</span>], </span>
<span id="cb16-270"><a href="#cb16-270" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">&quot;red&quot;</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">&quot;Observed GPA (Scholarship=0)&quot;</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb16-271"><a href="#cb16-271" aria-hidden="true" tabindex="-1"></a>    plt.scatter(scholarship_1[<span class="st">&quot;Study_Hours&quot;</span>], scholarship_1[<span class="st">&quot;GPA&quot;</span>], </span>
<span id="cb16-272"><a href="#cb16-272" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">&quot;orange&quot;</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">&quot;Observed GPA (Scholarship=1)&quot;</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb16-273"><a href="#cb16-273" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-274"><a href="#cb16-274" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Study Hours&quot;</span>)</span>
<span id="cb16-275"><a href="#cb16-275" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;GPA&quot;</span>)</span>
<span id="cb16-276"><a href="#cb16-276" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Bayesian Regression Lines with 94% Prediction Intervals&quot;</span>)</span>
<span id="cb16-277"><a href="#cb16-277" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb16-278"><a href="#cb16-278" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb16-279"><a href="#cb16-279" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb16-280"><a href="#cb16-280" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">&quot;bayes_prediction_band.png&quot;</span>)</span>
<span id="cb16-281"><a href="#cb16-281" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-282"><a href="#cb16-282" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-283"><a href="#cb16-283" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-284"><a href="#cb16-284" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-285"><a href="#cb16-285" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 7. 包括的な結果表示</span></span>
<span id="cb16-286"><a href="#cb16-286" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb16-287"><a href="#cb16-287" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-288"><a href="#cb16-288" aria-hidden="true" tabindex="-1"></a>    <span class="co"># パラメータの解釈可能性を向上</span></span>
<span id="cb16-289"><a href="#cb16-289" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">=== モデル結果の解釈 ===&quot;</span>)</span>
<span id="cb16-290"><a href="#cb16-290" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;勉強時間の効果 (beta_st): </span><span class="sc">{</span>np<span class="sc">.</span>mean(beta_st_vals)<span class="sc">:.3f}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(beta_st_vals)<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-291"><a href="#cb16-291" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;奨学金の効果 (beta_ss): </span><span class="sc">{</span>np<span class="sc">.</span>mean(beta_ss_vals)<span class="sc">:.3f}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(beta_ss_vals)<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-292"><a href="#cb16-292" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;学生間のばらつき (sigma_alpha): </span><span class="sc">{</span>np<span class="sc">.</span>mean(sigma_alpha_vals)<span class="sc">:.3f}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(sigma_alpha_vals)<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-293"><a href="#cb16-293" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb16-294"><a href="#cb16-294" aria-hidden="true" tabindex="-1"></a>    <span class="co">## 効果量の計算</span></span>
<span id="cb16-295"><a href="#cb16-295" aria-hidden="true" tabindex="-1"></a>    effect_size_study <span class="op">=</span> np.mean(beta_st_vals) <span class="op">/</span> np.mean(sigma_alpha_vals)</span>
<span id="cb16-296"><a href="#cb16-296" aria-hidden="true" tabindex="-1"></a>    effect_size_scholarship <span class="op">=</span> np.mean(beta_ss_vals) <span class="op">/</span> np.mean(sigma_alpha_vals)</span>
<span id="cb16-297"><a href="#cb16-297" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;勉強時間の標準化効果量: </span><span class="sc">{</span>effect_size_study<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-298"><a href="#cb16-298" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;奨学金の標準化効果量: </span><span class="sc">{</span>effect_size_scholarship<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-299"><a href="#cb16-299" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb16-300"><a href="#cb16-300" aria-hidden="true" tabindex="-1"></a>    <span class="co">## 予測精度の詳細評価</span></span>
<span id="cb16-301"><a href="#cb16-301" aria-hidden="true" tabindex="-1"></a>    r2 <span class="op">=</span> r2_score(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean)</span>
<span id="cb16-302"><a href="#cb16-302" aria-hidden="true" tabindex="-1"></a>    mae <span class="op">=</span> mean_absolute_error(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean)</span>
<span id="cb16-303"><a href="#cb16-303" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">=== 予測精度 ===&quot;</span>)</span>
<span id="cb16-304"><a href="#cb16-304" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;R²: </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-305"><a href="#cb16-305" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;MAE: </span><span class="sc">{</span>mae<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-306"><a href="#cb16-306" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;RMSE: </span><span class="sc">{</span>bayes_rmse<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb16-307"><a href="#cb16-307" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-308"><a href="#cb16-308" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 階層効果の可視化</span></span>
<span id="cb16-309"><a href="#cb16-309" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb16-310"><a href="#cb16-310" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-311"><a href="#cb16-311" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 学生別の切片分布</span></span>
<span id="cb16-312"><a href="#cb16-312" aria-hidden="true" tabindex="-1"></a>    alpha_means <span class="op">=</span> np.mean(posterior_samples.alpha.values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-313"><a href="#cb16-313" aria-hidden="true" tabindex="-1"></a>    alpha_stds <span class="op">=</span> np.std(posterior_samples.alpha.values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb16-314"><a href="#cb16-314" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-315"><a href="#cb16-315" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb16-316"><a href="#cb16-316" aria-hidden="true" tabindex="-1"></a>    plt.errorbar(<span class="bu">range</span>(<span class="bu">len</span>(alpha_means)), alpha_means, yerr<span class="op">=</span>alpha_stds, fmt<span class="op">=</span><span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb16-317"><a href="#cb16-317" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Student ID&quot;</span>)</span>
<span id="cb16-318"><a href="#cb16-318" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Random Intercept (α)&quot;</span>)</span>
<span id="cb16-319"><a href="#cb16-319" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Student-specific Random Intercepts&quot;</span>)</span>
<span id="cb16-320"><a href="#cb16-320" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb16-321"><a href="#cb16-321" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-322"><a href="#cb16-322" aria-hidden="true" tabindex="-1"></a>    <span class="co"># パラメータの事後分布</span></span>
<span id="cb16-323"><a href="#cb16-323" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb16-324"><a href="#cb16-324" aria-hidden="true" tabindex="-1"></a>    plt.hist(beta_st_vals, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Study Hours Effect'</span>)</span>
<span id="cb16-325"><a href="#cb16-325" aria-hidden="true" tabindex="-1"></a>    plt.hist(beta_ss_vals, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Scholarship Effect'</span>)</span>
<span id="cb16-326"><a href="#cb16-326" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Parameter Value&quot;</span>)</span>
<span id="cb16-327"><a href="#cb16-327" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Frequency&quot;</span>)</span>
<span id="cb16-328"><a href="#cb16-328" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Posterior Distributions of Effects&quot;</span>)</span>
<span id="cb16-329"><a href="#cb16-329" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb16-330"><a href="#cb16-330" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb16-331"><a href="#cb16-331" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-332"><a href="#cb16-332" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測精度の比較</span></span>
<span id="cb16-333"><a href="#cb16-333" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb16-334"><a href="#cb16-334" aria-hidden="true" tabindex="-1"></a>    plt.scatter(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Bayesian'</span>)</span>
<span id="cb16-335"><a href="#cb16-335" aria-hidden="true" tabindex="-1"></a>    plt.scatter(df[<span class="st">&quot;GPA&quot;</span>], lm_preds, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Linear Regression'</span>)</span>
<span id="cb16-336"><a href="#cb16-336" aria-hidden="true" tabindex="-1"></a>    plt.plot([df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">min</span>(), df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">max</span>()], </span>
<span id="cb16-337"><a href="#cb16-337" aria-hidden="true" tabindex="-1"></a>             [df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">min</span>(), df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">max</span>()], <span class="st">'r--'</span>, label<span class="op">=</span><span class="st">'Perfect'</span>)</span>
<span id="cb16-338"><a href="#cb16-338" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Actual GPA&quot;</span>)</span>
<span id="cb16-339"><a href="#cb16-339" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Predicted GPA&quot;</span>)</span>
<span id="cb16-340"><a href="#cb16-340" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Prediction Accuracy Comparison&quot;</span>)</span>
<span id="cb16-341"><a href="#cb16-341" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb16-342"><a href="#cb16-342" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb16-343"><a href="#cb16-343" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-344"><a href="#cb16-344" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 残差分析</span></span>
<span id="cb16-345"><a href="#cb16-345" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb16-346"><a href="#cb16-346" aria-hidden="true" tabindex="-1"></a>    residuals <span class="op">=</span> df[<span class="st">&quot;GPA&quot;</span>] <span class="op">-</span> posterior_mean</span>
<span id="cb16-347"><a href="#cb16-347" aria-hidden="true" tabindex="-1"></a>    plt.scatter(posterior_mean, residuals, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb16-348"><a href="#cb16-348" aria-hidden="true" tabindex="-1"></a>    plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb16-349"><a href="#cb16-349" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Predicted GPA&quot;</span>)</span>
<span id="cb16-350"><a href="#cb16-350" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb16-351"><a href="#cb16-351" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Residual Plot&quot;</span>)</span>
<span id="cb16-352"><a href="#cb16-352" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb16-353"><a href="#cb16-353" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-354"><a href="#cb16-354" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb16-355"><a href="#cb16-355" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">&quot;comprehensive_results.png&quot;</span>, dpi<span class="op">=</span><span class="dv">300</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb16-356"><a href="#cb16-356" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb16-357"><a href="#cb16-357" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-358"><a href="#cb16-358" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 結果の要約をCSVに保存</span></span>
<span id="cb16-359"><a href="#cb16-359" aria-hidden="true" tabindex="-1"></a>    results_summary <span class="op">=</span> {</span>
<span id="cb16-360"><a href="#cb16-360" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Metric'</span>: [<span class="st">'R²'</span>, <span class="st">'MAE'</span>, <span class="st">'RMSE'</span>, <span class="st">'Study_Effect_Mean'</span>, <span class="st">'Study_Effect_Std'</span>, </span>
<span id="cb16-361"><a href="#cb16-361" aria-hidden="true" tabindex="-1"></a>                  <span class="st">'Scholarship_Effect_Mean'</span>, <span class="st">'Scholarship_Effect_Std'</span>, <span class="st">'Student_Variability'</span>],</span>
<span id="cb16-362"><a href="#cb16-362" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Value'</span>: [r2, mae, bayes_rmse, np.mean(beta_st_vals), np.std(beta_st_vals),</span>
<span id="cb16-363"><a href="#cb16-363" aria-hidden="true" tabindex="-1"></a>                 np.mean(beta_ss_vals), np.std(beta_ss_vals), np.mean(sigma_alpha_vals)]</span>
<span id="cb16-364"><a href="#cb16-364" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb16-365"><a href="#cb16-365" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb16-366"><a href="#cb16-366" aria-hidden="true" tabindex="-1"></a>    results_df <span class="op">=</span> pd.DataFrame(results_summary)</span>
<span id="cb16-367"><a href="#cb16-367" aria-hidden="true" tabindex="-1"></a>    results_df.to_csv(<span class="st">&quot;model_results_summary.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>, encoding<span class="op">=</span><span class="st">'utf-8-sig'</span>)</span></code></pre></div>
<p>以下, 個別の部分に関して確認していきます.
特にモデルの設定部分に関して,モデルの定義との対応関係を確認するようにしましょう.</p>
<h3 data-number="5.1" id="事前準備"><span class="header-section-number">5.1</span> 事前準備</h3>
<p>まずは,ライブラリのインポート,データの読み込み,数量化,標準化などを実施します.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pymc <span class="im">as</span> pm</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> seaborn <span class="im">as</span> sns</span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> arviz <span class="im">as</span> az</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> graphviz</span>
<span id="cb17-8"><a href="#cb17-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pymc <span class="im">import</span> model_to_graphviz</span>
<span id="cb17-9"><a href="#cb17-9" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> statsmodels.api <span class="im">as</span> sm</span>
<span id="cb17-10"><a href="#cb17-10" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> mean_squared_error, r2_score, mean_absolute_error</span>
<span id="cb17-11"><a href="#cb17-11" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> scipy.stats <span class="im">import</span> norm, rankdata</span>
<span id="cb17-12"><a href="#cb17-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-13"><a href="#cb17-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-14"><a href="#cb17-14" aria-hidden="true" tabindex="-1"></a>save_dir <span class="op">=</span> <span class="st">'../../images/slds/ch12/'</span></span>
<span id="cb17-15"><a href="#cb17-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-16"><a href="#cb17-16" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> <span class="va">__name__</span> <span class="op">==</span> <span class="st">&quot;__main__&quot;</span>:</span>
<span id="cb17-17"><a href="#cb17-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-18"><a href="#cb17-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb17-19"><a href="#cb17-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 1. データ読み込み</span></span>
<span id="cb17-20"><a href="#cb17-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb17-21"><a href="#cb17-21" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.read_csv(<span class="st">'hierarchical_regression.csv'</span></span>
<span id="cb17-22"><a href="#cb17-22" aria-hidden="true" tabindex="-1"></a>                ,dtype<span class="op">=</span>{<span class="st">'GPA'</span>: <span class="bu">float</span></span>
<span id="cb17-23"><a href="#cb17-23" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Scholarship'</span>: <span class="bu">bool</span></span>
<span id="cb17-24"><a href="#cb17-24" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Study_Hours'</span>: <span class="bu">float</span></span>
<span id="cb17-25"><a href="#cb17-25" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Sports_hours'</span>: <span class="bu">float</span></span>
<span id="cb17-26"><a href="#cb17-26" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'Part_time_Work'</span>: <span class="bu">float</span></span>
<span id="cb17-27"><a href="#cb17-27" aria-hidden="true" tabindex="-1"></a>                       ,<span class="st">'StudentID'</span>:<span class="bu">int</span>})</span>
<span id="cb17-28"><a href="#cb17-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-29"><a href="#cb17-29" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 数量化</span></span>
<span id="cb17-30"><a href="#cb17-30" aria-hidden="true" tabindex="-1"></a>    df <span class="op">=</span> pd.get_dummies(df, columns<span class="op">=</span>[<span class="st">'Scholarship'</span>], dtype<span class="op">=</span><span class="st">'int'</span>)</span>
<span id="cb17-31"><a href="#cb17-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-32"><a href="#cb17-32" aria-hidden="true" tabindex="-1"></a>    <span class="co">#標準化</span></span>
<span id="cb17-33"><a href="#cb17-33" aria-hidden="true" tabindex="-1"></a>    <span class="im">from</span> sklearn.preprocessing <span class="im">import</span> StandardScaler</span>
<span id="cb17-34"><a href="#cb17-34" aria-hidden="true" tabindex="-1"></a>    scaler <span class="op">=</span> StandardScaler()</span>
<span id="cb17-35"><a href="#cb17-35" aria-hidden="true" tabindex="-1"></a>    df[[<span class="st">'Scholarship_True'</span></span>
<span id="cb17-36"><a href="#cb17-36" aria-hidden="true" tabindex="-1"></a>       ,<span class="st">'Study_Hours'</span></span>
<span id="cb17-37"><a href="#cb17-37" aria-hidden="true" tabindex="-1"></a>       ,<span class="st">'Part_time_Work'</span></span>
<span id="cb17-38"><a href="#cb17-38" aria-hidden="true" tabindex="-1"></a>       ,<span class="st">'Sports_hours'</span>]] <span class="op">=</span> scaler.fit_transform(df[[<span class="st">'Scholarship_True'</span></span>
<span id="cb17-39"><a href="#cb17-39" aria-hidden="true" tabindex="-1"></a>                                                   ,<span class="st">'Study_Hours'</span></span>
<span id="cb17-40"><a href="#cb17-40" aria-hidden="true" tabindex="-1"></a>                                                   ,<span class="st">'Part_time_Work'</span></span>
<span id="cb17-41"><a href="#cb17-41" aria-hidden="true" tabindex="-1"></a>                                                   ,<span class="st">'Sports_hours'</span>]])</span>
<span id="cb17-42"><a href="#cb17-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-43"><a href="#cb17-43" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb17-44"><a href="#cb17-44" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ２. 線形回帰</span></span>
<span id="cb17-45"><a href="#cb17-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># ---------------------------</span></span>
<span id="cb17-46"><a href="#cb17-46" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb17-47"><a href="#cb17-47" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 説明変数(X)と目的変数(y)に分割</span></span>
<span id="cb17-48"><a href="#cb17-48" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> df[[<span class="st">'Scholarship_True'</span>, <span class="st">'Study_Hours'</span>]]</span>
<span id="cb17-49"><a href="#cb17-49" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> df[<span class="st">'GPA'</span>]</span>
<span id="cb17-50"><a href="#cb17-50" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-51"><a href="#cb17-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-52"><a href="#cb17-52" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 切片(定数項)を追加</span></span>
<span id="cb17-53"><a href="#cb17-53" aria-hidden="true" tabindex="-1"></a>    X <span class="op">=</span> sm.add_constant(X)</span>
<span id="cb17-54"><a href="#cb17-54" aria-hidden="true" tabindex="-1"></a>    lm <span class="op">=</span> sm.OLS(y, X).fit()</span>
<span id="cb17-55"><a href="#cb17-55" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">[通常の線形回帰の結果]</span><span class="ch">\n</span><span class="st">&quot;</span>)</span>
<span id="cb17-56"><a href="#cb17-56" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(lm.summary())</span>
<span id="cb17-57"><a href="#cb17-57" aria-hidden="true" tabindex="-1"></a>    lm_preds <span class="op">=</span> lm.predict(X)</span></code></pre></div>
<div class="warn">
<ul>
<li><code>if __name__ == "__main__":</code> の必要性</li>
</ul>
<p>このコードブロックでは,メインの処理を<code>if __name__ == "__main__":</code>の下に記述しています. これは以下の理由から重要です:</p>
<ul>
<li><p><strong>モジュールとしてのインポートを防ぐ</strong>: スクリプトを他のファイルからインポートした場合に,メイン処理が自動的に実行されることを防ぎます.</p></li>
<li><p><strong>PyMCの並列処理との互換性</strong>: PyMCはMCMCサンプリング時に並列処理を行うため,<code>multiprocessing</code>モジュールを使用します. <code>multiprocessing</code>は各プロセスでスクリプトを再インポートするため,<code>if __name__ == "__main__":</code>がないと,無限再帰や予期しない動作が発生する可能性があります.</p></li>
<li><p><strong>Windowsでの実行保証</strong>: 特にWindows環境では,<code>multiprocessing</code>を使用する際に<code>if __name__ == "__main__":</code>が必須です. これがないとエラーが発生します.</p></li>
</ul>
<p>そのため,ベイズ統計モデリングを行う際は,必ずメイン処理を<code>if __name__ == "__main__":</code>の下に記述するようにしてください.</p>
<p>以降のコードは全て<code>if __name__ == "__main__":</code>の下でインデントされている前提となりますので注意して下さい.</p>
</div>
<h3 data-number="5.2" id="座標設定"><span class="header-section-number">5.2</span> 座標設定</h3>
<p>まず,<code>PyMC</code>でデータを処理するに当たって,<strong>座標(coordinates)</strong>を定義します.</p>
<div class="sourceCode" id="cb18"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>    coords <span class="op">=</span> {</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;student&quot;</span>: df[<span class="st">&quot;StudentID&quot;</span>].values,</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;obs_id&quot;</span>: np.arange(<span class="bu">len</span>(df))</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>    }</span></code></pre></div>
<div class="warn">
<ul>
<li>座標(coordinates)の意味</li>
</ul>
<p><code>coords</code>は,<code>PyMC</code>で多次元データや階層モデルを扱う際に使用する座標系の定義です. この辞書は以下のような役割を持ちます:</p>
<ul>
<li><p><strong><code>"student"</code></strong>: 学生のIDを表す座標軸です. <code>df["StudentID"].values</code>により,各データポイントがどの学生に対応するかを定義します. これにより,ランダム切片<span class="math inline">\alpha_i</span>を学生ごとに定義できるようになります.</p></li>
<li><p><strong><code>"obs_id"</code></strong>: 観測値のIDを表す座標軸です. <code>np.arange(len(df))</code>により,各観測値に連番のIDを割り当てます. これにより,データの順序を保持し,各観測値に対応するパラメータや変数を定義できます.</p></li>
</ul>
<p>この座標系により,<code>PyMC</code>は学生ごとのパラメータ(<span class="math inline">\alpha_i</span>など)を適切に管理し,階層モデルを構築することができます.</p>
</div>
<h3 data-number="5.3" id="モデル構築"><span class="header-section-number">5.3</span> モデル構築</h3>
<p>次にモデルを構築していきます. 以下,モデル全体像で定義した各要素とPyMCコードの対応関係を示しながら説明します.</p>
<div class="sourceCode" id="cb19"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> pm.Model(coords<span class="op">=</span>coords) <span class="im">as</span> model:</span>
<span id="cb19-2"><a href="#cb19-2" aria-hidden="true" tabindex="-1"></a>        st <span class="op">=</span> pm.Data(<span class="st">&quot;st&quot;</span>, df[<span class="st">&quot;Study_Hours&quot;</span>])</span>
<span id="cb19-3"><a href="#cb19-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">#ptw = pm.Data(&quot;ptw&quot;, df[&quot;Part_time_Work&quot;])</span></span>
<span id="cb19-4"><a href="#cb19-4" aria-hidden="true" tabindex="-1"></a>        ss <span class="op">=</span> pm.Data(<span class="st">&quot;ss&quot;</span>, df[<span class="st">&quot;Scholarship_True&quot;</span>])</span>
<span id="cb19-5"><a href="#cb19-5" aria-hidden="true" tabindex="-1"></a>        student_idx <span class="op">=</span> pm.Data(<span class="st">&quot;student_idx&quot;</span>, df[<span class="st">&quot;StudentID&quot;</span>])</span>
<span id="cb19-6"><a href="#cb19-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-7"><a href="#cb19-7" aria-hidden="true" tabindex="-1"></a>        <span class="co">#ランダム切片</span></span>
<span id="cb19-8"><a href="#cb19-8" aria-hidden="true" tabindex="-1"></a>        z_alpha <span class="op">=</span> pm.Normal(<span class="st">&quot;z_alpha&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>, dims<span class="op">=</span><span class="st">&quot;student&quot;</span>)</span>
<span id="cb19-9"><a href="#cb19-9" aria-hidden="true" tabindex="-1"></a>        mu_alpha <span class="op">=</span> pm.Normal(<span class="st">&quot;mu_alpha&quot;</span>, <span class="dv">0</span>, <span class="dv">1</span>)</span>
<span id="cb19-10"><a href="#cb19-10" aria-hidden="true" tabindex="-1"></a>        sigma_alpha <span class="op">=</span> pm.HalfNormal(<span class="st">&quot;sigma_alpha&quot;</span>, <span class="dv">1</span>)</span>
<span id="cb19-11"><a href="#cb19-11" aria-hidden="true" tabindex="-1"></a>        alpha <span class="op">=</span> pm.Deterministic(<span class="st">&quot;alpha&quot;</span>, mu_alpha <span class="op">+</span> sigma_alpha <span class="op">*</span> z_alpha, dims<span class="op">=</span><span class="st">&quot;student&quot;</span>)</span>
<span id="cb19-12"><a href="#cb19-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-13"><a href="#cb19-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-14"><a href="#cb19-14" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 勉強時間の効果</span></span>
<span id="cb19-15"><a href="#cb19-15" aria-hidden="true" tabindex="-1"></a>        beta_st <span class="op">=</span> pm.Normal(<span class="st">&quot;beta_st&quot;</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb19-16"><a href="#cb19-16" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb19-17"><a href="#cb19-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 奨学金の効果</span></span>
<span id="cb19-18"><a href="#cb19-18" aria-hidden="true" tabindex="-1"></a>        beta_ss <span class="op">=</span> pm.Normal(<span class="st">&quot;beta_ss&quot;</span>,<span class="dv">0</span>,<span class="dv">1</span>)</span>
<span id="cb19-19"><a href="#cb19-19" aria-hidden="true" tabindex="-1"></a> </span>
<span id="cb19-20"><a href="#cb19-20" aria-hidden="true" tabindex="-1"></a>        mu <span class="op">=</span> alpha[student_idx] <span class="op">+</span> beta_st <span class="op">*</span> st <span class="op">+</span>  beta_ss <span class="op">*</span> ss</span>
<span id="cb19-21"><a href="#cb19-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb19-22"><a href="#cb19-22" aria-hidden="true" tabindex="-1"></a>        <span class="co">#sigma = pm.HalfNormal(&quot;sigma&quot;, 1)</span></span>
<span id="cb19-23"><a href="#cb19-23" aria-hidden="true" tabindex="-1"></a>        sigma <span class="op">=</span> <span class="fl">0.3</span></span>
<span id="cb19-24"><a href="#cb19-24" aria-hidden="true" tabindex="-1"></a>        gpa_obs <span class="op">=</span> pm.Normal(<span class="st">&quot;GPA&quot;</span>,</span>
<span id="cb19-25"><a href="#cb19-25" aria-hidden="true" tabindex="-1"></a>                            mu<span class="op">=</span>mu,</span>
<span id="cb19-26"><a href="#cb19-26" aria-hidden="true" tabindex="-1"></a>                            sigma<span class="op">=</span>sigma,</span>
<span id="cb19-27"><a href="#cb19-27" aria-hidden="true" tabindex="-1"></a>                            observed<span class="op">=</span>df[<span class="st">&quot;GPA&quot;</span>],</span>
<span id="cb19-28"><a href="#cb19-28" aria-hidden="true" tabindex="-1"></a>                            dims<span class="op">=</span><span class="st">&quot;obs_id&quot;</span></span>
<span id="cb19-29"><a href="#cb19-29" aria-hidden="true" tabindex="-1"></a>                        )</span>
<span id="cb19-30"><a href="#cb19-30" aria-hidden="true" tabindex="-1"></a>        trace <span class="op">=</span> pm.sample(draws <span class="op">=</span><span class="dv">2000</span>, tune<span class="op">=</span><span class="dv">2000</span>, chains<span class="op">=</span><span class="dv">4</span>, target_accept<span class="op">=</span><span class="fl">0.95</span>, return_inferencedata<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb19-31"><a href="#cb19-31" aria-hidden="true" tabindex="-1"></a>        posterior_predictive <span class="op">=</span> pm.sample_posterior_predictive(trace)</span></code></pre></div>
<h4 data-number="5.3.1" id="モデル全体像との対応関係"><span class="header-section-number">5.3.1</span> モデル全体像との対応関係</h4>
<table>
<colgroup>
<col style="width: 41%" />
<col style="width: 37%" />
<col style="width: 20%" />
</colgroup>
<thead>
<tr class="header">
<th>モデル全体像</th>
<th>PyMCコード</th>
<th>説明</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">y_i \sim N(\mu_i, \sigma^2)</span></td>
<td><code>gpa_obs = pm.Normal("GPA", mu=mu, sigma=sigma, observed=df["GPA"])</code></td>
<td>目的変数の分布</td>
</tr>
<tr class="even">
<td><span class="math inline">\mu_i = \alpha_i + \beta_{st} \cdot S_t + \beta_{ss} \cdot S_s</span></td>
<td><code>mu = alpha[student_idx] + beta_st * st + beta_ss * ss</code></td>
<td>線形予測子</td>
</tr>
<tr class="odd">
<td><span class="math inline">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span></td>
<td><code>alpha = pm.Deterministic(..., mu_alpha + sigma_alpha * z_alpha, dims="student")</code></td>
<td>ランダム切片</td>
</tr>
<tr class="even">
<td><span class="math inline">\mu_{\alpha} \sim N(0, 1)</span>, <span class="math inline">\sigma_{\alpha} \sim HN(1)</span></td>
<td><code>mu_alpha = pm.Normal(0, 1)</code>, <code>sigma_alpha = pm.HalfNormal(1)</code></td>
<td>超事前分布(α)</td>
</tr>
<tr class="odd">
<td><span class="math inline">\beta_{st} \sim N(0, 1)</span></td>
<td><code>beta_st = pm.Normal("beta_st", 0, 1)</code></td>
<td>勉強時間の効果(固定効果)</td>
</tr>
<tr class="even">
<td><span class="math inline">\beta_{ss} \sim N(0, 1)</span></td>
<td><code>beta_ss = pm.Normal("beta_ss", 0, 1)</code></td>
<td>奨学金の効果(固定効果)</td>
</tr>
</tbody>
</table>
<div class="note">
<ul>
<li><p>コードの詳細説明</p></li>
<li><p><strong>非中心パラメータ化</strong>: <code>alpha = mu_alpha + sigma_alpha * z_alpha</code>という形式は,<strong>非中心パラメータ化(non-centered parameterization)</strong>と呼ばれます. これにより,<span class="math inline">z_{\alpha} \sim N(0, 1)</span>から<span class="math inline">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>を生成します. この方法は,MCMCサンプリングの効率を向上させることがあります.</p></li>
<li><p><strong><code>dims="student"</code></strong>: ランダム切片<span class="math inline">\alpha_i</span>は学生ごとに異なるため,<code>dims="student"</code>を指定して学生の座標軸に沿って定義します.</p></li>
<li><p><strong><code>alpha[student_idx]</code></strong>: 各観測値に対応する学生の切片を取得します. <code>student_idx</code>は各観測値がどの学生に対応するかを示すインデックスです.</p></li>
<li><p><strong><code>beta_st</code>と<code>beta_ss</code>は固定効果</strong>: 現在のモデルでは,<code>beta_st</code>と<code>beta_ss</code>は学生間で同じ値を持つ固定効果として扱われています. これらは階層構造を持たず,直接<span class="math inline">N(0, 1)</span>の事前分布から推定されます.</p></li>
<li><p><strong><code>observed=df["GPA"]</code></strong>: 観測データを指定することで,尤度関数が定義されます.</p></li>
</ul>
</div>
<h3 data-number="5.4" id="mcmc"><span class="header-section-number">5.4</span> MCMC</h3>
<p>続いて,作成したモデルをMCMCによって,推定します.</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a>        <span class="co"># MCMCサンプリングの実行</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>        trace <span class="op">=</span> pm.sample(draws <span class="op">=</span><span class="dv">2000</span>, tune<span class="op">=</span><span class="dv">2000</span>, chains<span class="op">=</span><span class="dv">4</span>, target_accept<span class="op">=</span><span class="fl">0.95</span>, return_inferencedata<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 事後予測分布のサンプリング</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        posterior_predictive <span class="op">=</span> pm.sample_posterior_predictive(trace)</span></code></pre></div>
<div class="note">
<ul>
<li>MCMCサンプリングの仕組み</li>
</ul>
<ol type="1">
<li><p><strong>初期値の設定</strong>: 各チェーンは異なる初期値から開始します.</p></li>
<li><p><strong>ウォームアップ期間</strong>: <code>tune</code>期間中,サンプラーはパラメータ空間を探索し,効率的なサンプリングのために調整されます.</p></li>
<li><p><strong>サンプリング期間</strong>: <code>draws</code>期間中,事後分布からサンプルを取得します. 各ステップで,現在のパラメータ値から新しい値を提案し,受容/棄却を決定します.</p></li>
<li><p><strong>収束の確認</strong>: 複数のチェーンが同じ分布に収束しているか確認します. これにより,サンプリングが適切に行われたかを診断できます.</p></li>
</ol>
<ul>
<li>MCMCサンプリングの基本概念</li>
</ul>
<p><strong><code>pm.sample()</code></strong>は,MCMCサンプリングを実行し,パラメータの事後分布を推定します. 各パラメータの意味は以下の通りです.</p>
<ul>
<li><p><strong><code>draws=2000</code></strong>: 各チェーンから取得するサンプル数です. この例では,各チェーンから2000個のサンプルを取得します. 合計では<code>chains × draws = 4 × 2000 = 8000</code>個のサンプルが得られます.</p></li>
<li><p><strong><code>tune=2000</code></strong>: <strong>バーンイン期間(burn-in period)</strong>または<strong>ウォームアップ期間</strong>です. この期間中に取得されたサンプルは破棄されます. MCMCは初期値から開始するため,初期のサンプルは事後分布に収束していない可能性があります. この期間でサンプラーが適切に動作するよう調整されます.</p></li>
<li><p><strong><code>chains=4</code></strong>: 並列に実行する<strong>マルコフ連鎖</strong>の数です. 複数のチェーンを実行することで,収束の診断が可能になります. 通常は4つのチェーンを使用し,それぞれ異なる初期値から開始します. すべてのチェーンが同じ分布に収束すれば,適切にサンプリングできていると判断できます.</p></li>
<li><p><strong><code>target_accept=0.95</code></strong>: サンプリングアルゴリズムの<strong>受容率(acceptance rate)</strong>の目標値です. NUTS(No-U-Turn Sampler)などの適応型MCMCアルゴリズムでは,この受容率を調整しながらサンプリングを行います. 0.95は高い受容率で,より効率的なサンプリングを目指します.</p></li>
<li><p><strong><code>return_inferencedata=True</code></strong>: 結果を<code>arviz</code>の<code>InferenceData</code>形式で返します. これにより,可視化や診断が容易になります.</p></li>
</ul>
<p><strong><code>pm.sample_posterior_predictive(trace)</code></strong>は,推定された事後分布から新しいデータを生成します. これにより,モデルの予測性能を評価したり,予測区間を計算したりできます.</p>
</div>
<p>上記のサンプリングを実行することで,標準出力に以下のような表示がなされます.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode sh"><code class="sourceCode bash"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="ex">Initializing</span> NUTS using jitter+adapt_diag...</span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="ex">Multiprocess</span> sampling <span class="er">(</span><span class="ex">4</span> chains in 2 jobs<span class="kw">)</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a><span class="ex">NUTS:</span> [z_alpha, mu_alpha, sigma_alpha, beta_st, beta_ss]</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>  <span class="ex">Progress</span>                                   Draws   Divergences   Step size   Grad evals   Sampling Speed   Elapsed   Remaining</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a> <span class="ex">────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>  <span class="ex">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4000    0             0.205       15           326.43 draws/s   0:00:12   0:00:00</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a>  <span class="ex">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4000    0             0.222       31           340.23 draws/s   0:00:11   0:00:00</span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>  <span class="ex">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4000    0             0.203       15           151.86 draws/s   0:00:26   0:00:00</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>  <span class="ex">━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━</span>   4000    0             0.182       15           158.16 draws/s   0:00:25   0:00:00</span></code></pre></div>
<h5 data-number="5.4.0.1" id="標準出力の解説"><span class="header-section-number">5.4.0.1</span> 標準出力の解説</h5>
<p>この出力は,MCMCサンプリングの実行状況を示しています. 各項目の意味は以下の通りです:</p>
<ul>
<li><p><strong><code>Initializing NUTS using jitter+adapt_diag...</code></strong>: NUTS(No-U-Turn Sampler)アルゴリズムを初期化しています. <code>jitter+adapt_diag</code>は初期値の設定方法と,ステップサイズの調整方法を表します.</p></li>
<li><p><strong><code>Multiprocess sampling (4 chains in 2 jobs)</code></strong>: 4つのチェーンを2つの並列ジョブで実行しています. これにより,計算時間を短縮できます.</p></li>
<li><p><strong><code>NUTS: [z_alpha, mu_alpha, ...]</code></strong>: NUTSアルゴリズムでサンプリングするパラメータのリストです. モデルで定義したすべての確率変数が表示されます.</p></li>
</ul>
<p><img src="../images/slds/ch12/mcmc.png" /></p>
<div class="note">
<ul>
<li>NUTSアルゴリズム</li>
</ul>
<p><strong>NUTS(No-U-Turn Sampler)</strong>は,ハミルトニアンモンテカルロ法(HMC)の一種で,PyMCでは,デフォルトでNUTSアルゴリズムが使用されるため,ユーザーはアルゴリズムの詳細を意識せずに,効率的なベイズ推定を行うことができます.</p>
<p>通常のMCMCアルゴリズム(メトロポリス法など)と比べて以下の特徴があります.</p>
<p><strong>通常のMCMC(メトロポリス法など):</strong></p>
<ul>
<li><strong>ランダムウォーク</strong>: 現在のパラメータ値からランダムに近くの値を提案します.</li>
<li><strong>ステップサイズの固定</strong>: ステップサイズを手動で設定する必要があります.</li>
<li><strong>効率の低さ</strong>: パラメータ空間をゆっくりと探索するため,多くのサンプルが必要です.</li>
<li><strong>高次元での非効率</strong>: パラメータ数が増えると,探索が非常に非効率になります.</li>
</ul>
<p><strong>NUTSアルゴリズム:</strong></p>
<ul>
<li><strong>勾配情報の利用</strong>: 尤度関数の勾配(微分)を計算して,パラメータ空間を効率的に探索します.</li>
<li><strong>適応的ステップサイズ</strong>: ステップサイズを自動的に調整します. これにより,手動での調整が不要になります.</li>
<li><strong>長距離の移動</strong>: 1ステップでパラメータ空間を大きく移動できるため,少ないサンプル数で効率的に探索できます.</li>
<li><strong>「Uターン」の回避</strong>: パラメータ空間を探索する際に,同じ方向に戻ることを避けます. これが「No-U-Turn」という名前の由来です.</li>
<li><strong>高次元での効率</strong>: 高次元のパラメータ空間でも効率的にサンプリングできます.</li>
</ul>
</div>
<p>各行は1つのチェーンの進捗状況を表します(この例では4つのチェーンが表示されています). 各列の意味は以下の通りです:</p>
<ul>
<li><p><strong><code>Progress</code></strong>: サンプリングの進捗状況を視覚的に表示します. <code>━</code>が表示されるほど,サンプリングが進んでいます.</p></li>
<li><p><strong><code>Draws</code></strong>: 現在までに取得したサンプル数です. <code>tune + draws = 2000 + 2000 = 4000</code>となるため,合計4000ステップが実行されます.</p></li>
<li><p><strong><code>Divergences</code></strong>: <strong>発散(divergence)</strong>の発生回数です. 発散は,サンプラーがパラメータ空間の困難な領域に遭遇した際に発生します. 発散が多い場合(通常は100を超える場合),モデルやサンプリング設定を見直す必要があります. この例では,すべてのチェーンで発散が0回となっており,良好なサンプリングが行われていることを示しています. これは,非中心パラメータ化により,パラメータ間の相関が低減され,効率的なサンプリングが実現された結果です.</p></li>
<li><p><strong><code>Step size</code></strong>: サンプリングの<strong>ステップサイズ</strong>です. これが大きいほど,1ステップで大きくパラメータ空間を移動します. NUTSアルゴリズムは,このステップサイズを自動的に調整します. この例では,0.182-0.222の範囲で調整されており,適切なステップサイズでサンプリングが行われています. 値はチェーンごとに異なる場合があります.</p></li>
<li><p><strong><code>Grad evals</code></strong>: 1ステップあたりの<strong>勾配評価回数</strong>です. NUTSアルゴリズムは,尤度関数の勾配を計算して効率的にサンプリングします. この値が大きいほど,より多くの探索を行っていることを意味します. この例では,15-31回となっており,効率的な探索が行われています.</p></li>
<li><p><strong><code>Sampling Speed</code></strong>: サンプリングの速度(1秒あたりのサンプル数)です. この値が大きいほど,高速にサンプリングできます. モデルの複雑さやパラメータ数によって大きく変わります. この例では,151.86-340.23 draws/sとなっており,比較的高速にサンプリングが行われています.</p></li>
</ul>
<div class="warn">
<ul>
<li><strong>発散について:</strong></li>
</ul>
<p>発散が多く発生している場合は,モデル等がデータに上手く適合していない状況を表しています.
今回は実施しませんが,そのような場合には以下の対処によって,サンプリング手法やモデルを修正する必要があります.</p>
<ul>
<li>モデルの再パラメータ化(非中心パラメータ化の使用)</li>
<li><code>target_accept</code>の調整(通常は0.8-0.9が推奨される)</li>
<li>事前分布の見直し</li>
<li>データの再検討</li>
</ul>
<p>ただし,発散が少ない場合でも,以下で行うモデル診断(トレースプロット)で収束を確認することが重要です.</p>
</div>
<h3 data-number="5.5" id="モデル診断と可視化"><span class="header-section-number">5.5</span> モデル診断と可視化</h3>
<p>まずは作成したモデルを確認します.</p>
<div class="sourceCode" id="cb22"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a>    graph <span class="op">=</span> model_to_graphviz(model)</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    graph.render(filename<span class="op">=</span> save_dir <span class="op">+</span> <span class="st">&quot;hierarchical_bayes_model&quot;</span>, <span class="bu">format</span><span class="op">=</span><span class="st">&quot;pdf&quot;</span>)</span></code></pre></div>
<p>上記のコードで,モデルの全体像がpdfファイルとして保存されます.</p>
<p><img src="../images/slds/ch12/hierarchical_bayes_model.png" /></p>
<p>論文等に掲載することもできるので,コードとの対応関係を確認しましょう.</p>
<p>続いて,モデルの診断結果を確認します. <code>az.plot_trace()</code>を用いることで,指定した変数のサンプリング過程を可視化することができます.</p>
<div class="sourceCode" id="cb23"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>    az.plot_trace(trace</span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a>                 ,var_names<span class="op">=</span>[<span class="st">&quot;sigma_alpha&quot;</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a>                            ,<span class="st">&quot;alpha&quot;</span></span>
<span id="cb23-4"><a href="#cb23-4" aria-hidden="true" tabindex="-1"></a>                            ,<span class="st">&quot;beta_st&quot;</span></span>
<span id="cb23-5"><a href="#cb23-5" aria-hidden="true" tabindex="-1"></a>                            ,<span class="st">&quot;beta_ss&quot;</span>])</span>
<span id="cb23-6"><a href="#cb23-6" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb23-7"><a href="#cb23-7" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'trace.png'</span>)</span>
<span id="cb23-8"><a href="#cb23-8" aria-hidden="true" tabindex="-1"></a>    plt.close()</span></code></pre></div>
<p><img src="../images/slds/ch12/trace.png" /></p>
<p>この図では,左側に4つのサンプルごとに推定された分布の計上,右側にトレースの過程が表示されています.</p>
<p><img src="../images/slds/ch12/sampling_image.png" /></p>
<p>大まかには,</p>
<ul>
<li>左側の分布が全てのサンプルで概ね同じ形状になっている</li>
<li>右側のトレースが一箇所に収束している</li>
</ul>
<p>ことから,概ね良好に推定できていることが分かります.</p>
<p>ただし,alphaに関しては学生別の分布が異なる色で表示されている点に注意して下さい.
学生それぞれの基礎学力の分布が異なっている様が表現されています.</p>
<p>グラフでは大まかな印象しかわからないので更に,<code>az.summary()</code>を利用してサンプリング結果を数値で確認してみます.</p>
<div class="sourceCode" id="cb24"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(az.summary(trace</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>                    ,var_names<span class="op">=</span>[<span class="st">&quot;sigma_alpha&quot;</span></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>                               ,<span class="st">&quot;alpha&quot;</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>                               ,<span class="st">&quot;beta_st&quot;</span></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>                               ,<span class="st">&quot;beta_ss&quot;</span>]))</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>    summary_df <span class="op">=</span> az.summary(trace</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>                           ,var_names<span class="op">=</span>[<span class="st">&quot;sigma_alpha&quot;</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>                                      ,<span class="st">&quot;alpha&quot;</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>                                      ,<span class="st">&quot;beta_st&quot;</span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>                                      ,<span class="st">&quot;beta_ss&quot;</span>])</span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a>    summary_df.to_csv(<span class="st">&quot;bayesian_summary.csv&quot;</span></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>                     ,encoding<span class="op">=</span><span class="st">&quot;utf-8-sig&quot;</span>)</span></code></pre></div>
<p>結果は学生別のalphaが出力されており非常に長いため,抜粋したものが以下になります.</p>
<table>
<colgroup>
<col style="width: 12%" />
<col style="width: 7%" />
<col style="width: 4%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 12%" />
<col style="width: 10%" />
<col style="width: 11%" />
<col style="width: 11%" />
<col style="width: 8%" />
</colgroup>
<thead>
<tr class="header">
<th>パラメータ</th>
<th>mean</th>
<th>sd</th>
<th>hdi_3%</th>
<th>hdi_97%</th>
<th>mcse_mean</th>
<th>mcse_sd</th>
<th>ess_bulk</th>
<th>ess_tail</th>
<th>r_hat</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sigma_alpha</td>
<td>0.586</td>
<td>0.039</td>
<td>0.514</td>
<td>0.659</td>
<td>0.001</td>
<td>0.001</td>
<td>2193</td>
<td>4123</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[0]</td>
<td>1.337</td>
<td>0.276</td>
<td>0.826</td>
<td>1.851</td>
<td>0.003</td>
<td>0.002</td>
<td>9277</td>
<td>5519</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[1]</td>
<td>2.23</td>
<td>0.272</td>
<td>1.725</td>
<td>2.759</td>
<td>0.003</td>
<td>0.002</td>
<td>9863</td>
<td>6118</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[2]</td>
<td>2.044</td>
<td>0.267</td>
<td>1.541</td>
<td>2.543</td>
<td>0.003</td>
<td>0.002</td>
<td>9771</td>
<td>5823</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[3]</td>
<td>2.177</td>
<td>0.278</td>
<td>1.644</td>
<td>2.683</td>
<td>0.003</td>
<td>0.002</td>
<td>6533</td>
<td>5528</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[4]</td>
<td>3.02</td>
<td>0.27</td>
<td>2.506</td>
<td>3.512</td>
<td>0.003</td>
<td>0.002</td>
<td>10929</td>
<td>6162</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[5]</td>
<td>1.543</td>
<td>0.276</td>
<td>1.036</td>
<td>2.054</td>
<td>0.003</td>
<td>0.002</td>
<td>9980</td>
<td>5880</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[6]</td>
<td>1.709</td>
<td>0.275</td>
<td>1.184</td>
<td>2.22</td>
<td>0.003</td>
<td>0.002</td>
<td>7096</td>
<td>4980</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[7]</td>
<td>2.535</td>
<td>0.273</td>
<td>1.985</td>
<td>3.023</td>
<td>0.003</td>
<td>0.002</td>
<td>8866</td>
<td>5628</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[8]</td>
<td>2.003</td>
<td>0.265</td>
<td>1.503</td>
<td>2.498</td>
<td>0.003</td>
<td>0.002</td>
<td>8334</td>
<td>6208</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[9]</td>
<td>2.783</td>
<td>0.27</td>
<td>2.247</td>
<td>3.267</td>
<td>0.003</td>
<td>0.002</td>
<td>8919</td>
<td>4931</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[10]</td>
<td>2.443</td>
<td>0.271</td>
<td>1.944</td>
<td>2.977</td>
<td>0.003</td>
<td>0.002</td>
<td>8207</td>
<td>5363</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[180]</td>
<td>1.205</td>
<td>0.283</td>
<td>0.666</td>
<td>1.724</td>
<td>0.003</td>
<td>0.002</td>
<td>8038</td>
<td>5827</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[181]</td>
<td>1.952</td>
<td>0.274</td>
<td>1.443</td>
<td>2.463</td>
<td>0.003</td>
<td>0.002</td>
<td>7932</td>
<td>6444</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[182]</td>
<td>2.039</td>
<td>0.272</td>
<td>1.529</td>
<td>2.552</td>
<td>0.003</td>
<td>0.002</td>
<td>9013</td>
<td>5407</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[183]</td>
<td>2.503</td>
<td>0.265</td>
<td>2.018</td>
<td>3.006</td>
<td>0.003</td>
<td>0.002</td>
<td>10853</td>
<td>6100</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[184]</td>
<td>2.017</td>
<td>0.277</td>
<td>1.481</td>
<td>2.519</td>
<td>0.003</td>
<td>0.002</td>
<td>6437</td>
<td>5893</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[185]</td>
<td>1.985</td>
<td>0.274</td>
<td>1.483</td>
<td>2.521</td>
<td>0.003</td>
<td>0.002</td>
<td>7264</td>
<td>6106</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[186]</td>
<td>1.729</td>
<td>0.279</td>
<td>1.198</td>
<td>2.251</td>
<td>0.003</td>
<td>0.002</td>
<td>8657</td>
<td>5455</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[187]</td>
<td>1.153</td>
<td>0.274</td>
<td>0.65</td>
<td>1.68</td>
<td>0.003</td>
<td>0.002</td>
<td>11005</td>
<td>5637</td>
<td>1</td>
</tr>
<tr class="odd">
<td>alpha[188]</td>
<td>2.412</td>
<td>0.272</td>
<td>1.898</td>
<td>2.918</td>
<td>0.003</td>
<td>0.002</td>
<td>7896</td>
<td>5719</td>
<td>1</td>
</tr>
<tr class="even">
<td>alpha[189]</td>
<td>2.093</td>
<td>0.269</td>
<td>1.588</td>
<td>2.6</td>
<td>0.003</td>
<td>0.002</td>
<td>7705</td>
<td>5726</td>
<td>1</td>
</tr>
<tr class="odd">
<td>beta_st</td>
<td>0.559</td>
<td>0.048</td>
<td>0.473</td>
<td>0.651</td>
<td>0.001</td>
<td>0.001</td>
<td>1889</td>
<td>3833</td>
<td>1</td>
</tr>
<tr class="even">
<td>beta_ss</td>
<td>0.139</td>
<td>0.048</td>
<td>0.044</td>
<td>0.225</td>
<td>0.001</td>
<td>0.001</td>
<td>1957</td>
<td>3460</td>
<td>1</td>
</tr>
</tbody>
</table>
<div class="note">
<ul>
<li>MCMCサンプリング統計量</li>
</ul>
<p>この表には,MCMCサンプリングの結果を要約する統計量が含まれています.</p>
<ul>
<li><p><strong><code>mean</code></strong>: 事後分布の<strong>平均値</strong>です. パラメータの点推定値として使用されます.</p></li>
<li><p><strong><code>sd</code></strong>: 事後分布の<strong>標準偏差</strong>です. パラメータの不確実性を表します.</p></li>
<li><p><strong><code>hdi_3%</code>, <code>hdi_97%</code></strong>: <strong>最高密度区間(Highest Density Interval, HDI)</strong>の下限と上限です. 事後分布の94%が含まれる区間を表します. これは信頼区間のような概念で,この区間内に真のパラメータ値が含まれる確率が高いことを示します.</p></li>
<li><p><strong><code>mcse_mean</code>, <code>mcse_sd</code></strong>: <strong>モンテカルロ標準誤差(Monte Carlo Standard Error)</strong>です. MCMCサンプリングによる推定の不確実性を表します. この値が小さいほど,推定が安定しています.</p></li>
<li><p><strong><code>ess_bulk</code>, <code>ess_tail</code></strong>: <strong>有効サンプルサイズ(Effective Sample Size, ESS)</strong>です. MCMCサンプルには自己相関があるため,実際に独立な情報を含むサンプル数を表します. <code>ess_bulk</code>は分布の中心部分,<code>ess_tail</code>は分布の尾部の有効サンプルサイズです. 通常,400以上であることが推奨されます.</p></li>
<li><p><strong><code>r_hat</code></strong>: <strong><span class="math inline">\hat{R}</span>統計量(Gelman-Rubin統計量)</strong>です. 複数のチェーンが同じ分布に収束しているかを示す指標です. 1.0に近いほど良好で,通常1.01以下が推奨されます. 1.01を超える場合は,サンプリングが不十分である可能性があります.</p></li>
</ul>
</div>
<p><strong>収束の診断:</strong>
- すべてのパラメータで<code>r_hat = 1.0</code>となっており,良好な収束を示しています. これは,4つのチェーンがすべて同じ分布に収束していることを意味します.</p>
<ul>
<li>有効サンプルサイズ(<code>ess_bulk</code>)は,すべてのパラメータで400を大きく超えており,サンプリングは適切に行われています. 特に,<code>alpha[i]</code>の多くは6000-11000の範囲で,非常に良好なサンプリングが行われています. <code>beta_st</code>と<code>beta_ss</code>は1889と1957となっており,やや低めですが,推奨値(400)を大きく上回っており,問題ありません.</li>
</ul>
<p><strong>パラメータの推定値:</strong></p>
<ul>
<li><p><strong><code>sigma_alpha = 0.586</code></strong>: 学生間の基礎学力のばらつきの標準偏差です. この値が大きいほど,学生間の学力差が大きいことを示します. HDI区間[0.514, 0.659]は比較的狭く,推定の不確実性は小さいです.</p></li>
<li><p><strong><code>alpha[i]</code></strong>: 各学生の基礎学力です. 例えば,<code>alpha[4] = 3.02</code>は,学生4の基礎学力が高いことを示しています. 一方,<code>alpha[187] = 1.153</code>は,学生187の基礎学力が低いことを示しています. 学生間で基礎学力に大きなばらつきがあることが分かります.</p></li>
<li><p><strong><code>beta_st = 0.559</code></strong>: 勉強時間の効果です. この値は正で,勉強時間が1単位増加すると,GPAが平均的に0.559増加することを示しています. HDI区間[0.473, 0.651]は0を含まないため,統計的に有意な正の効果があります. この区間は比較的狭く,推定の不確実性は小さいです.</p></li>
<li><p><strong><code>beta_ss = 0.139</code></strong>: 奨学金の効果です. この値も正で,奨学金を受給している学生のGPAが平均的に0.139高いことを示しています. HDI区間[0.044, 0.225]は0を含まないため,統計的に有意な正の効果があります. この区間も比較的狭く,推定の不確実性は小さいです.</p></li>
</ul>
<h3 data-number="5.6" id="結果の可視化"><span class="header-section-number">5.6</span> 結果の可視化</h3>
<p>続いて今回のモデルの説明能力について可視化していきます.</p>
<h4 data-number="5.6.1" id="事後予測チェックposterior-predictive-check-ppc"><span class="header-section-number">5.6.1</span> 事後予測チェック(Posterior Predictive Check, PPC)</h4>
<p><strong>事後予測チェック</strong>は,推定されたモデルが実際のデータをどれだけよく説明できるかを確認するための手法です. 具体的には,推定された事後分布から新しいデータを生成し,その分布が実際の観測データとどの程度一致しているかを可視化します.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>    idata <span class="op">=</span> trace.copy()</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    idata.extend(posterior_predictive)</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>    az.plot_ppc(idata, data_pairs<span class="op">=</span>{<span class="st">&quot;GPA&quot;</span>: <span class="st">&quot;GPA&quot;</span>})</span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'ppc.png'</span>)</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>    plt.close()</span></code></pre></div>
<ul>
<li><code>idata.extend(posterior_predictive)</code>: 事後予測分布のサンプルを<code>InferenceData</code>オブジェクトに追加します.</li>
<li><code>az.plot_ppc()</code>: 事後予測チェックの可視化を行います. 観測データと事後予測分布を比較します.</li>
</ul>
<p><img src="../images/slds/ch12/ppc.png" /></p>
<p>この図は,観測データ(黒い実線)と事後予測分布(青い領域)を比較しています.</p>
<ul>
<li><strong>黒い実線</strong>: 実際に観測されたGPAデータの分布です.</li>
<li><strong>青い領域</strong>: 推定されたモデルから生成された事後予測分布です. 複数のサンプルから生成された分布の範囲を示しています.</li>
<li><strong>オレンジの実践</strong>: 青い実線の平均値</li>
</ul>
<p>観測データの分布が事後予測分布の範囲内(青い線の範囲)に収まっている場合,モデルはデータを適切に説明していると判断できます. 一方,観測データの分布が事後予測分布から大きく外れている場合,モデルの仮定や構造を見直す必要があります.</p>
<p>この例では,観測データの分布が事後予測分布の範囲内に収まっており,モデルがデータを適切に説明していることが確認できます.</p>
<h4 data-number="5.6.2" id="予測値の確認"><span class="header-section-number">5.6.2</span> 予測値の確認</h4>
<p>つぎに,線形回帰と同様に予測値と実測値の関係とkdeプロットを見てみましょう. まずは予測値を取り出します.</p>
<div class="sourceCode" id="cb26"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 事後予測分布から予測値(平均)を取り出す</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    posterior_mean <span class="op">=</span> idata.posterior_predictive[<span class="st">&quot;GPA&quot;</span>].mean(dim<span class="op">=</span>(<span class="st">&quot;chain&quot;</span>, <span class="st">&quot;draw&quot;</span>)).values</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a>    <span class="co"># RMSEを計算</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    bayes_rmse <span class="op">=</span> mean_squared_error(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean)</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;[階層ベイズモデルのRMSE] </span><span class="sc">{</span>bayes_rmse<span class="sc">:.4f}</span><span class="ch">\n</span><span class="ss">&quot;</span>)</span></code></pre></div>
<ul>
<li><p><strong><code>idata.posterior_predictive["GPA"]</code></strong>: 事後予測分布から生成されたGPAのサンプルです. このデータは,複数のチェーン(<code>chain</code>)と複数のドロー(<code>draw</code>)から構成される多次元配列です.</p></li>
<li><p><strong><code>.mean(dim=("chain", "draw"))</code></strong>: すべてのチェーンとドローにわたって平均を計算します. これにより,各観測値について,事後予測分布の平均値(期待値)を取得できます. つまり,各学生のGPAについて,モデルが予測する平均的な値を計算しています.</p></li>
<li><p><strong><code>.values</code></strong>: <code>xarray</code>のデータ配列をNumPy配列に変換します. これにより,後続の計算や可視化で使用しやすくなります.</p></li>
<li><p><strong><code>mean_squared_error(df["GPA"], posterior_mean)</code></strong>: 実測値(<code>df["GPA"]</code>)と予測値(<code>posterior_mean</code>)の間の平均二乗誤差(RMSE)を計算します. この値が小さいほど,モデルの予測精度が高いことを示します.</p></li>
</ul>
<div class="sourceCode" id="cb27"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># プロット</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">8</span>, <span class="dv">6</span>))</span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a>    plt.scatter(df[<span class="st">'GPA'</span>], posterior_mean, alpha<span class="op">=</span><span class="fl">0.7</span>, edgecolors<span class="op">=</span><span class="st">&quot;k&quot;</span>)</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Predicted (Bayesian)&quot;</span>)</span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Actual GPA&quot;</span>)</span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Bayesian Actual vs. Predicted&quot;</span>)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    plt.plot([df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()],</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>             [df[<span class="st">'GPA'</span>].<span class="bu">min</span>(), df[<span class="st">'GPA'</span>].<span class="bu">max</span>()],</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>             color<span class="op">=</span><span class="st">&quot;red&quot;</span>, linestyle<span class="op">=</span><span class="st">&quot;--&quot;</span>, label<span class="op">=</span><span class="st">&quot;Perfect prediction&quot;</span>)</span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    plt.grid()</span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'bayes.png'</span>)</span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">16</span>,<span class="dv">8</span>))</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(posterior_mean, label <span class="op">=</span> <span class="st">'Predicted'</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    sns.kdeplot(df[<span class="st">'GPA'</span>], label <span class="op">=</span> <span class="st">'Actual'</span>)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">'Actual/Predicted'</span>)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">'GPA'</span>)</span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">'Density'</span>)</span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">'bayes_kde.png'</span>)</span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a>    plt.close()</span></code></pre></div>
<p><img src="../images/slds/ch12/bayes.png" />
<img src="../images/slds/ch12/bayes_kde.png" /></p>
<p>予測値,分布共に大幅に改善されていることが分かります.</p>
<h4 data-number="5.6.3" id="予測範囲の可視化"><span class="header-section-number">5.6.3</span> 予測範囲の可視化</h4>
<p>線形モデルとは異なり,分布を推定しているため,勉強時間と奨学金の有無によるGPAの予測値と,その不確実性(94%予測区間)を可視化することが可能です.
奨学金の有無で異なる予測線を描画し,モデルの説明能力を確認します.</p>
<div class="sourceCode" id="cb28"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># x軸用の study_hours の範囲（100点）</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>    study_grid <span class="op">=</span> np.linspace(df[<span class="st">&quot;Study_Hours&quot;</span>].<span class="bu">min</span>(), df[<span class="st">&quot;Study_Hours&quot;</span>].<span class="bu">max</span>(), <span class="dv">100</span>)</span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金なしの予測</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a>    predict_df_0 <span class="op">=</span> pd.DataFrame({</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Study_Hours&quot;</span>: study_grid,</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Scholarship_True&quot;</span>: <span class="dv">0</span>,</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;StudentID&quot;</span>: <span class="dv">0</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金ありの予測</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    predict_df_1 <span class="op">=</span> pd.DataFrame({</span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Study_Hours&quot;</span>: study_grid,</span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;Scholarship_True&quot;</span>: <span class="dv">1</span>,</span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a>        <span class="st">&quot;StudentID&quot;</span>: <span class="dv">0</span></span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    })</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    <span class="co"># より簡単な方法：事後分布から直接予測</span></span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 事後分布のサンプルを取得（新しいAPIを使用）</span></span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    posterior_samples <span class="op">=</span> az.extract(trace)</span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測計算</span></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a>    n_samples <span class="op">=</span> <span class="bu">len</span>(posterior_samples.draw)</span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a>    n_grid <span class="op">=</span> <span class="bu">len</span>(study_grid)</span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測値を格納する配列（奨学金なし・あり）</span></span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>    predictions_0 <span class="op">=</span> np.zeros((n_samples, n_grid))</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>    predictions_1 <span class="op">=</span> np.zeros((n_samples, n_grid))</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>    <span class="co"># パラメータを一度に取得</span></span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>    mu_alpha_vals <span class="op">=</span> posterior_samples.mu_alpha.values</span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>    sigma_alpha_vals <span class="op">=</span> posterior_samples.sigma_alpha.values</span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>    z_alpha_vals <span class="op">=</span> posterior_samples.z_alpha.values  <span class="co"># 全学生のz_alpha    </span></span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>    beta_st_vals <span class="op">=</span> posterior_samples.beta_st.values    </span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a>    beta_ss_vals <span class="op">=</span> posterior_samples.beta_ss.values</span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples):</span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 各サンプルでのパラメータ（学生0の切片を計算）</span></span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>        alpha_0 <span class="op">=</span> mu_alpha_vals[i] <span class="op">+</span> sigma_alpha_vals[i] <span class="op">*</span> z_alpha_vals[<span class="dv">0</span>, i]  <span class="co"># 学生0の切片</span></span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 奨学金なしの予測値の計算</span></span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>        mu_pred_0 <span class="op">=</span> alpha_0 <span class="op">+</span> beta_st_vals[i] <span class="op">*</span> study_grid <span class="op">+</span> beta_ss_vals[i] <span class="op">*</span> predict_df_0[<span class="st">&quot;Scholarship_True&quot;</span>]</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>        predictions_0[i, :] <span class="op">=</span> np.random.normal(mu_pred_0, <span class="fl">0.3</span>)</span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a>        <span class="co"># 奨学金ありの予測値の計算</span></span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a>        mu_pred_1 <span class="op">=</span> alpha_0 <span class="op">+</span> beta_st_vals[i] <span class="op">*</span> study_grid <span class="op">+</span> beta_ss_vals[i] <span class="op">*</span> predict_df_1[<span class="st">&quot;Scholarship_True&quot;</span>]</span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a>        predictions_1[i, :] <span class="op">=</span> np.random.normal(mu_pred_1, <span class="fl">0.3</span>)</span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 平均と予測区間（94%）</span></span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a>    mean_pred_0 <span class="op">=</span> np.mean(predictions_0, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a>    lower_pred_0 <span class="op">=</span> np.percentile(predictions_0, <span class="dv">3</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a>    upper_pred_0 <span class="op">=</span> np.percentile(predictions_0, <span class="dv">97</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a>    mean_pred_1 <span class="op">=</span> np.mean(predictions_1, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a>    lower_pred_1 <span class="op">=</span> np.percentile(predictions_1, <span class="dv">3</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a>    upper_pred_1 <span class="op">=</span> np.percentile(predictions_1, <span class="dv">97</span>, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 実データも合わせて表示（奨学金の有無で色分け）</span></span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金なしの予測線</span></span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>    plt.plot(study_grid, mean_pred_0, color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, label<span class="op">=</span><span class="st">&quot;Bayesian prediction (Scholarship=0)&quot;</span>)</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(study_grid, lower_pred_0, upper_pred_0, color<span class="op">=</span><span class="st">&quot;blue&quot;</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">&quot;94% CI (Scholarship=0)&quot;</span>)</span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金ありの予測線</span></span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>    plt.plot(study_grid, mean_pred_1, color<span class="op">=</span><span class="st">&quot;green&quot;</span>, label<span class="op">=</span><span class="st">&quot;Bayesian prediction (Scholarship=1)&quot;</span>)</span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a>    plt.fill_between(study_grid, lower_pred_1, upper_pred_1, color<span class="op">=</span><span class="st">&quot;green&quot;</span>, alpha<span class="op">=</span><span class="fl">0.2</span>, label<span class="op">=</span><span class="st">&quot;94% CI (Scholarship=1)&quot;</span>)</span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 奨学金の有無で実測値を色分け</span></span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>    scholarship_0 <span class="op">=</span> df[df[<span class="st">&quot;Scholarship_True&quot;</span>] <span class="op">==</span> <span class="dv">0</span>]</span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a>    scholarship_1 <span class="op">=</span> df[df[<span class="st">&quot;Scholarship_True&quot;</span>] <span class="op">==</span> <span class="dv">1</span>]</span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a>    plt.scatter(scholarship_0[<span class="st">&quot;Study_Hours&quot;</span>], scholarship_0[<span class="st">&quot;GPA&quot;</span>], </span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">&quot;red&quot;</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">&quot;Observed GPA (Scholarship=0)&quot;</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a>    plt.scatter(scholarship_1[<span class="st">&quot;Study_Hours&quot;</span>], scholarship_1[<span class="st">&quot;GPA&quot;</span>], </span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a>                color<span class="op">=</span><span class="st">&quot;orange&quot;</span>, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">&quot;Observed GPA (Scholarship=1)&quot;</span>, s<span class="op">=</span><span class="dv">30</span>)</span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Study Hours&quot;</span>)</span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;GPA&quot;</span>)</span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Bayesian Regression Lines with 94% Prediction Intervals&quot;</span>)</span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">&quot;bayes_prediction_band.png&quot;</span>)</span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a>    plt.close()</span></code></pre></div>
<p><img src="../images/slds/ch12/bayes_prediction_band.png" /></p>
<p>予測区間内に実測値が多く含まれており,モデルがデータを適切に説明していることが確認できます.
また,奨学金ありのグループの予測線が上に位置しており,奨学金の正の効果が可視化されています.</p>
<h4 data-number="5.6.4" id="その他の可視化手法"><span class="header-section-number">5.6.4</span> その他の可視化手法</h4>
<p>最後にその他によく利用される手法として,ランダム切片の可視化手法として学生の個別の能力値の分布,推定された分布の可視化手法として各パラメータの推定された分布,Regidual Plotなどをまとめて掲載します. 論文執筆などの際には,必要なものをピックアップしてグラフ化しましょう.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode py"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>    <span class="co"># パラメータの解釈可能性を向上</span></span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">&quot;</span><span class="ch">\n</span><span class="st">=== モデル結果の解釈 ===&quot;</span>)</span>
<span id="cb29-3"><a href="#cb29-3" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;勉強時間の効果 (beta_st): </span><span class="sc">{</span>np<span class="sc">.</span>mean(beta_st_vals)<span class="sc">:.3f}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(beta_st_vals)<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-4"><a href="#cb29-4" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;奨学金の効果 (beta_ss): </span><span class="sc">{</span>np<span class="sc">.</span>mean(beta_ss_vals)<span class="sc">:.3f}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(beta_ss_vals)<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-5"><a href="#cb29-5" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;学生間のばらつき (sigma_alpha): </span><span class="sc">{</span>np<span class="sc">.</span>mean(sigma_alpha_vals)<span class="sc">:.3f}</span><span class="ss"> ± </span><span class="sc">{</span>np<span class="sc">.</span>std(sigma_alpha_vals)<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-6"><a href="#cb29-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb29-7"><a href="#cb29-7" aria-hidden="true" tabindex="-1"></a>    <span class="co">## 効果量の計算</span></span>
<span id="cb29-8"><a href="#cb29-8" aria-hidden="true" tabindex="-1"></a>    effect_size_study <span class="op">=</span> np.mean(beta_st_vals) <span class="op">/</span> np.mean(sigma_alpha_vals)</span>
<span id="cb29-9"><a href="#cb29-9" aria-hidden="true" tabindex="-1"></a>    effect_size_scholarship <span class="op">=</span> np.mean(beta_ss_vals) <span class="op">/</span> np.mean(sigma_alpha_vals)</span>
<span id="cb29-10"><a href="#cb29-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;勉強時間の標準化効果量: </span><span class="sc">{</span>effect_size_study<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-11"><a href="#cb29-11" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;奨学金の標準化効果量: </span><span class="sc">{</span>effect_size_scholarship<span class="sc">:.3f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-12"><a href="#cb29-12" aria-hidden="true" tabindex="-1"></a>    <span class="co">#</span></span>
<span id="cb29-13"><a href="#cb29-13" aria-hidden="true" tabindex="-1"></a>    <span class="co">## 予測精度の詳細評価</span></span>
<span id="cb29-14"><a href="#cb29-14" aria-hidden="true" tabindex="-1"></a>    r2 <span class="op">=</span> r2_score(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean)</span>
<span id="cb29-15"><a href="#cb29-15" aria-hidden="true" tabindex="-1"></a>    mae <span class="op">=</span> mean_absolute_error(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean)</span>
<span id="cb29-16"><a href="#cb29-16" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;</span><span class="ch">\n</span><span class="ss">=== 予測精度 ===&quot;</span>)</span>
<span id="cb29-17"><a href="#cb29-17" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;R²: </span><span class="sc">{</span>r2<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-18"><a href="#cb29-18" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;MAE: </span><span class="sc">{</span>mae<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-19"><a href="#cb29-19" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f&quot;RMSE: </span><span class="sc">{</span>bayes_rmse<span class="sc">:.4f}</span><span class="ss">&quot;</span>)</span>
<span id="cb29-20"><a href="#cb29-20" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-21"><a href="#cb29-21" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 階層効果の可視化</span></span>
<span id="cb29-22"><a href="#cb29-22" aria-hidden="true" tabindex="-1"></a>    plt.figure(figsize<span class="op">=</span>(<span class="dv">12</span>, <span class="dv">8</span>))</span>
<span id="cb29-23"><a href="#cb29-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-24"><a href="#cb29-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 学生別の切片分布</span></span>
<span id="cb29-25"><a href="#cb29-25" aria-hidden="true" tabindex="-1"></a>    alpha_means <span class="op">=</span> np.mean(posterior_samples.alpha.values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb29-26"><a href="#cb29-26" aria-hidden="true" tabindex="-1"></a>    alpha_stds <span class="op">=</span> np.std(posterior_samples.alpha.values, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb29-27"><a href="#cb29-27" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-28"><a href="#cb29-28" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">1</span>)</span>
<span id="cb29-29"><a href="#cb29-29" aria-hidden="true" tabindex="-1"></a>    plt.errorbar(<span class="bu">range</span>(<span class="bu">len</span>(alpha_means)), alpha_means, yerr<span class="op">=</span>alpha_stds, fmt<span class="op">=</span><span class="st">'o'</span>, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb29-30"><a href="#cb29-30" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Student ID&quot;</span>)</span>
<span id="cb29-31"><a href="#cb29-31" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Random Intercept (α)&quot;</span>)</span>
<span id="cb29-32"><a href="#cb29-32" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Student-specific Random Intercepts&quot;</span>)</span>
<span id="cb29-33"><a href="#cb29-33" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb29-34"><a href="#cb29-34" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-35"><a href="#cb29-35" aria-hidden="true" tabindex="-1"></a>    <span class="co"># パラメータの事後分布</span></span>
<span id="cb29-36"><a href="#cb29-36" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">2</span>)</span>
<span id="cb29-37"><a href="#cb29-37" aria-hidden="true" tabindex="-1"></a>    plt.hist(beta_st_vals, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Study Hours Effect'</span>)</span>
<span id="cb29-38"><a href="#cb29-38" aria-hidden="true" tabindex="-1"></a>    plt.hist(beta_ss_vals, bins<span class="op">=</span><span class="dv">50</span>, alpha<span class="op">=</span><span class="fl">0.7</span>, label<span class="op">=</span><span class="st">'Scholarship Effect'</span>)</span>
<span id="cb29-39"><a href="#cb29-39" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Parameter Value&quot;</span>)</span>
<span id="cb29-40"><a href="#cb29-40" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Frequency&quot;</span>)</span>
<span id="cb29-41"><a href="#cb29-41" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Posterior Distributions of Effects&quot;</span>)</span>
<span id="cb29-42"><a href="#cb29-42" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb29-43"><a href="#cb29-43" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb29-44"><a href="#cb29-44" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-45"><a href="#cb29-45" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 予測精度の比較</span></span>
<span id="cb29-46"><a href="#cb29-46" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">3</span>)</span>
<span id="cb29-47"><a href="#cb29-47" aria-hidden="true" tabindex="-1"></a>    plt.scatter(df[<span class="st">&quot;GPA&quot;</span>], posterior_mean, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Bayesian'</span>)</span>
<span id="cb29-48"><a href="#cb29-48" aria-hidden="true" tabindex="-1"></a>    plt.scatter(df[<span class="st">&quot;GPA&quot;</span>], lm_preds, alpha<span class="op">=</span><span class="fl">0.6</span>, label<span class="op">=</span><span class="st">'Linear Regression'</span>)</span>
<span id="cb29-49"><a href="#cb29-49" aria-hidden="true" tabindex="-1"></a>    plt.plot([df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">min</span>(), df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">max</span>()], </span>
<span id="cb29-50"><a href="#cb29-50" aria-hidden="true" tabindex="-1"></a>             [df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">min</span>(), df[<span class="st">&quot;GPA&quot;</span>].<span class="bu">max</span>()], <span class="st">'r--'</span>, label<span class="op">=</span><span class="st">'Perfect'</span>)</span>
<span id="cb29-51"><a href="#cb29-51" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Actual GPA&quot;</span>)</span>
<span id="cb29-52"><a href="#cb29-52" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Predicted GPA&quot;</span>)</span>
<span id="cb29-53"><a href="#cb29-53" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Prediction Accuracy Comparison&quot;</span>)</span>
<span id="cb29-54"><a href="#cb29-54" aria-hidden="true" tabindex="-1"></a>    plt.legend()</span>
<span id="cb29-55"><a href="#cb29-55" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb29-56"><a href="#cb29-56" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-57"><a href="#cb29-57" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 残差分析</span></span>
<span id="cb29-58"><a href="#cb29-58" aria-hidden="true" tabindex="-1"></a>    plt.subplot(<span class="dv">2</span>, <span class="dv">2</span>, <span class="dv">4</span>)</span>
<span id="cb29-59"><a href="#cb29-59" aria-hidden="true" tabindex="-1"></a>    residuals <span class="op">=</span> df[<span class="st">&quot;GPA&quot;</span>] <span class="op">-</span> posterior_mean</span>
<span id="cb29-60"><a href="#cb29-60" aria-hidden="true" tabindex="-1"></a>    plt.scatter(posterior_mean, residuals, alpha<span class="op">=</span><span class="fl">0.6</span>)</span>
<span id="cb29-61"><a href="#cb29-61" aria-hidden="true" tabindex="-1"></a>    plt.axhline(y<span class="op">=</span><span class="dv">0</span>, color<span class="op">=</span><span class="st">'r'</span>, linestyle<span class="op">=</span><span class="st">'--'</span>)</span>
<span id="cb29-62"><a href="#cb29-62" aria-hidden="true" tabindex="-1"></a>    plt.xlabel(<span class="st">&quot;Predicted GPA&quot;</span>)</span>
<span id="cb29-63"><a href="#cb29-63" aria-hidden="true" tabindex="-1"></a>    plt.ylabel(<span class="st">&quot;Residuals&quot;</span>)</span>
<span id="cb29-64"><a href="#cb29-64" aria-hidden="true" tabindex="-1"></a>    plt.title(<span class="st">&quot;Residual Plot&quot;</span>)</span>
<span id="cb29-65"><a href="#cb29-65" aria-hidden="true" tabindex="-1"></a>    plt.grid(<span class="va">True</span>)</span>
<span id="cb29-66"><a href="#cb29-66" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-67"><a href="#cb29-67" aria-hidden="true" tabindex="-1"></a>    plt.tight_layout()</span>
<span id="cb29-68"><a href="#cb29-68" aria-hidden="true" tabindex="-1"></a>    plt.savefig(save_dir <span class="op">+</span> <span class="st">&quot;comprehensive_results.png&quot;</span>, dpi<span class="op">=</span><span class="dv">300</span>, bbox_inches<span class="op">=</span><span class="st">'tight'</span>)</span>
<span id="cb29-69"><a href="#cb29-69" aria-hidden="true" tabindex="-1"></a>    plt.close()</span>
<span id="cb29-70"><a href="#cb29-70" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-71"><a href="#cb29-71" aria-hidden="true" tabindex="-1"></a>    <span class="co"># 結果の要約をCSVに保存</span></span>
<span id="cb29-72"><a href="#cb29-72" aria-hidden="true" tabindex="-1"></a>    results_summary <span class="op">=</span> {</span>
<span id="cb29-73"><a href="#cb29-73" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Metric'</span>: [<span class="st">'R²'</span>, <span class="st">'MAE'</span>, <span class="st">'RMSE'</span>, <span class="st">'Study_Effect_Mean'</span>, <span class="st">'Study_Effect_Std'</span>, </span>
<span id="cb29-74"><a href="#cb29-74" aria-hidden="true" tabindex="-1"></a>                  <span class="st">'Scholarship_Effect_Mean'</span>, <span class="st">'Scholarship_Effect_Std'</span>, <span class="st">'Student_Variability'</span>],</span>
<span id="cb29-75"><a href="#cb29-75" aria-hidden="true" tabindex="-1"></a>        <span class="st">'Value'</span>: [r2, mae, bayes_rmse, np.mean(beta_st_vals), np.std(beta_st_vals),</span>
<span id="cb29-76"><a href="#cb29-76" aria-hidden="true" tabindex="-1"></a>                 np.mean(beta_ss_vals), np.std(beta_ss_vals), np.mean(sigma_alpha_vals)]</span>
<span id="cb29-77"><a href="#cb29-77" aria-hidden="true" tabindex="-1"></a>    }</span>
<span id="cb29-78"><a href="#cb29-78" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb29-79"><a href="#cb29-79" aria-hidden="true" tabindex="-1"></a>    results_df <span class="op">=</span> pd.DataFrame(results_summary)</span>
<span id="cb29-80"><a href="#cb29-80" aria-hidden="true" tabindex="-1"></a>    results_df.to_csv(<span class="st">&quot;model_results_summary.csv&quot;</span>, index<span class="op">=</span><span class="va">False</span>, encoding<span class="op">=</span><span class="st">'utf-8-sig'</span>)</span></code></pre></div>
<p><img src="../images/slds/ch12/comprehensive_results.png" /></p>
<h2 data-number="6" id="まとめ-線形回帰-vs-ベイズモデリング"><span class="header-section-number">6</span> まとめ (線形回帰 VS ベイズモデリング)</h2>
<p>この章では,線形回帰分析とベイズ統計モデリングの違い,および固定効果モデルとランダム効果モデル(階層ベイズ)の違いについて学びました. 以下に,これらの手法の比較と特徴をまとめます.</p>
<h3 data-number="6.1" id="線形回帰頻度主義と統計モデリングベイズ主義の比較"><span class="header-section-number">6.1</span> 線形回帰(頻度主義)と統計モデリング(ベイズ主義)の比較</h3>
<p>頻度主義的な線形回帰分析とベイズ主義的な統計モデリングには,以下のような根本的な違いがあります.</p>
<table>
<colgroup>
<col style="width: 17%" />
<col style="width: 35%" />
<col style="width: 47%" />
</colgroup>
<thead>
<tr class="header">
<th>比較項目</th>
<th>線形回帰(頻度主義)</th>
<th>統計モデリング(ベイズ主義)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>分布の使い方</strong></td>
<td>誤差のみが正規分布に従うと仮定<br><span class="math inline">\epsilon_i \sim N(0, \sigma^2)</span></td>
<td>出力もパラメータも分布として扱う<br><span class="math inline">y_i \sim N(\mu_i, \sigma^2)</span><br><span class="math inline">\theta \sim F</span> (パラメータの事前分布)</td>
</tr>
<tr class="even">
<td><strong>制約</strong></td>
<td>誤差のみに制約がある</td>
<td>パラメータにも制約がある</td>
</tr>
<tr class="odd">
<td><strong>推定方法</strong></td>
<td>最小二乗法</td>
<td>ベイズ更新(MCMCなど)</td>
</tr>
<tr class="even">
<td><strong>結果の解釈</strong></td>
<td>点推定と検定(区間推定)</td>
<td>事後分布に基づく不確実性の推定</td>
</tr>
</tbody>
</table>
<h4 data-number="6.1.1" id="詳細な説明"><span class="header-section-number">6.1.1</span> 詳細な説明</h4>
<p><strong>分布の使い方:</strong></p>
<p>頻度主義的な線形回帰では,誤差項<span class="math inline">\epsilon_i</span>のみが正規分布に従うと仮定します. パラメータ(<span class="math inline">\beta_0, \beta_1</span>など)は固定値として扱われ,分布を持ちません.</p>
<p>一方,ベイズ統計モデリングでは,出力変数<span class="math inline">y_i</span>だけでなく,パラメータ自体も確率分布として扱います. 事前分布を設定し,データを観測した後の事後分布を推定することで,パラメータの不確実性を表現できます.</p>
<p><strong>制約:</strong></p>
<p>線形回帰では,誤差項に対してのみ分布の仮定(正規分布,等分散性など)が課されます. パラメータには特別な制約はありません.</p>
<p>ベイズ統計モデリングでは,パラメータに対して事前分布を設定することで,パラメータの取りうる範囲や値に制約を課すことができます. これにより,データが少ない場合でも,事前の知識を活用した推定が可能になります.</p>
<p><strong>推定方法:</strong></p>
<p>線形回帰では,最小二乗法により誤差の二乗和を最小化するパラメータを求めます. この方法は解析的に解を求めることができ,計算が比較的簡単です.</p>
<p>ベイズ統計モデリングでは,ベイズの定理に基づいて事後分布を計算します. 複雑なモデルでは解析的に解を求めることが困難なため,MCMC(マルコフ連鎖モンテカルロ法)などの数値計算手法を用いて事後分布をサンプリングします.</p>
<p><strong>結果の解釈:</strong></p>
<p>線形回帰では,推定されたパラメータの点推定値と,その信頼区間や検定結果を解釈します. 結果は「パラメータの真の値はこの区間に含まれる確率が95%」という頻度主義的な解釈になります.</p>
<p>ベイズ統計モデリングでは,事後分布全体を解釈します. パラメータの不確実性を確率分布として表現できるため,「パラメータがこの値である確率が95%」という主観的な解釈が可能になります. また,予測区間の計算も容易で,予測の不確実性も表現できます.</p>
<h3 data-number="6.2" id="固定効果ダミー変数とランダム効果階層ベイズの比較"><span class="header-section-number">6.2</span> 固定効果(ダミー変数)とランダム効果(階層ベイズ)の比較</h3>
<p>個体差を考慮する際に,固定効果モデルとランダム効果モデルでは,推定方法や結果の解釈が大きく異なります.</p>
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 38%" />
<col style="width: 44%" />
</colgroup>
<thead>
<tr class="header">
<th>比較項目</th>
<th>ダミー変数(固定効果)</th>
<th>ランダム効果(階層ベイズ)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>各個体の推定値</strong></td>
<td>各個体は独立に自由に推定される(制約なし)</td>
<td>分布からのサンプルとして推定される(制約あり)</td>
</tr>
<tr class="even">
<td><strong>パラメータ数</strong></td>
<td>個体数分のパラメータが必要</td>
<td>分布の平均と分散のみ(+個体ごとの潜在変数)</td>
</tr>
<tr class="odd">
<td><strong>新しい個体の予測</strong></td>
<td>不可能(その個体のダミーがないため)</td>
<td>可能(分布からのサンプルとして扱えるため)</td>
</tr>
<tr class="even">
<td><strong>過学習のリスク</strong></td>
<td>高い(特にデータが少ない個体で顕著)</td>
<td>縮小推定により抑制される</td>
</tr>
<tr class="odd">
<td><strong>モデルの柔軟性</strong></td>
<td>高すぎる(過剰適合の可能性)</td>
<td>適度に制約された柔軟性</td>
</tr>
</tbody>
</table>
<h4 data-number="6.2.1" id="詳細な説明-1"><span class="header-section-number">6.2.1</span> 詳細な説明</h4>
<p><strong>各個体の推定値:</strong></p>
<p>固定効果モデルでは,各個体(例:学生)に対して独立したパラメータ(例:切片<span class="math inline">\alpha_i</span>)を推定します. 各パラメータには特別な制約がなく,データから直接推定されます.</p>
<p>ランダム効果モデルでは,各個体のパラメータが,より大きな分布(例:<span class="math inline">\alpha_i \sim N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>)からサンプリングされると仮定します. この仮定により,個体間で情報を共有し,より安定した推定が可能になります.</p>
<p><strong>パラメータ数:</strong></p>
<p>固定効果モデルでは,個体数<span class="math inline">N</span>に対して<span class="math inline">N</span>個のパラメータを推定する必要があります. 個体数が増えると,パラメータ数も線形に増加します.</p>
<p>ランダム効果モデルでは,分布のパラメータ(平均<span class="math inline">\mu_{\alpha}</span>と分散<span class="math inline">\sigma_{\alpha}^2</span>)のみを推定すればよく,個体数に関わらずパラメータ数は一定です. これにより,個体数が多くても効率的に推定できます.</p>
<p><strong>新しい個体の予測:</strong></p>
<p>固定効果モデルでは,モデルに含まれていない新しい個体に対して予測を行うことはできません. その個体のダミー変数が存在しないため,パラメータを推定できないからです.</p>
<p>ランダム効果モデルでは,新しい個体も既存の分布からサンプリングされると仮定することで,予測が可能です. 推定された分布<span class="math inline">N(\mu_{\alpha}, \sigma_{\alpha}^2)</span>に基づいて,新しい個体のパラメータを推定できます.</p>
<p><strong>過学習のリスク:</strong></p>
<p>固定効果モデルでは,特に各個体のデータが少ない場合,過学習が発生しやすくなります. 各パラメータが個別に推定されるため,データに過度に適合してしまいます.</p>
<p>ランダム効果モデルでは,「縮小推定(shrinkage estimation)」が働きます. 各個体の推定値が,全体の平均<span class="math inline">\mu_{\alpha}</span>に向かって「縮小」されるため,過学習が抑制されます. データが少ない個体でも,他の個体の情報を活用して安定した推定が可能になります.</p>
<p><strong>モデルの柔軟性:</strong></p>
<p>固定効果モデルは,各個体に対して完全に独立したパラメータを許容するため,非常に柔軟です. しかし,この柔軟性が過剰適合を引き起こす可能性があります.</p>
<p>ランダム効果モデルは,分布の仮定により適度に制約された柔軟性を持ちます. 個体間で情報を共有しながらも,個体差を適切に表現できます.</p>
<h3 data-number="6.3" id="まとめ"><span class="header-section-number">6.3</span> まとめ</h3>
<p>ベイズ統計モデリングは,パラメータの不確実性を表現し,事前知識を活用できる点で,頻度主義的な線形回帰を一般化した手法といえます. 特に,階層ベイズモデル(ランダム効果モデル)は,個体差を考慮しながらも,過学習を抑制し,新しいデータに対する予測も可能にする優れた手法です.</p>
<p>ただし,ベイズ統計モデリングは計算コストが高く,モデルの設定や事前分布の選択が結果に影響を与えるため,適切な知識と経験が必要です. 研究の目的やデータの特性に応じて,適切な手法を選択することが重要です.</p>


<!-- 前後の章へのナビゲーション -->
<div class="chapter-navigation">
    <nav>
        
            <a class="nav-link prev" href="slds11.html">← Previous Chapter</a>
        
        
            <a class="nav-link next" href="slds13.html">Next Chapter →</a>
        
    </nav>
</div>

    <div style="clear: both"></div>

    <div id="footer">
        Site proudly generated by
        <a href="http://jaspervdj.be/hakyll">Hakyll</a>.
    </div>
</div>


    <!-- GUID -->
    <div style="display: none">ce0f13b2-4a83-4c1c-b2b9-b6d18f4ee6d2</div>




    <!-- JavaScript TOC generator (only runs on lecture pages) -->
    <script>
    document.addEventListener("DOMContentLoaded", function() {
      var tocContainer = document.getElementById('lecture-toc');
      if (!tocContainer) return;

      // メインコンテンツから h2, h3, h4 を抽出
      var content = document.querySelector('article') || document.getElementById('content') || document.body;
      var headings = content.querySelectorAll('h2, h3, h4');
      if (headings.length === 0) return;

      // 目次用のUL要素を作成
      var tocList = document.createElement('ul');

      // 章番号カウンタ (h2, h3, h4に対応して配列を用意)
      var chapterNumbers = [0, 0, 0];

      headings.forEach(function(heading) {
        if (heading.closest('li')) return;

        var level;
        switch (heading.tagName.toLowerCase()) {
          case 'h2': level = 0; break;
          case 'h3': level = 1; break;
          case 'h4': level = 2; break;
          default: return;
        }

        chapterNumbers[level]++;
        for (var i = level + 1; i < chapterNumbers.length; i++) {
          chapterNumbers[i] = 0;
        }


        var li = document.createElement('li');
        li.classList.add('toc-level-' + (level + 1));

        var anchor = document.createElement('a');
        anchor.href = '#' + heading.id;
        anchor.textContent =  heading.textContent;

        li.appendChild(anchor);
        tocList.appendChild(li);
      });

      tocContainer.appendChild(tocList);
    });
    </script>
     <script>
  document.addEventListener("DOMContentLoaded", function() {
    // すべての <pre><code> 要素を走査
    const codeBlocks = document.querySelectorAll('pre code');
    codeBlocks.forEach(function(codeBlock) {
      // 親<pre>要素を取得
      const pre = codeBlock.parentNode;

      // <pre> を相対配置にし、子要素を絶対配置できるようにする
      pre.style.position = 'relative';

      // コピーボタンを作成
      const copyButton = document.createElement('button');
      copyButton.textContent = 'Copy';
      // ボタンのデザインはCSSで指定するのが望ましいが、簡易的にスタイルを直接指定する例:
      copyButton.style.position = 'absolute';
      copyButton.style.top = '8px';
      copyButton.style.right = '8px';
      copyButton.style.backgroundColor = '#add8e6'; // 水色
      copyButton.style.color = '#fff';             // 白文字
      copyButton.style.border = 'none';
      copyButton.style.padding = '6px 10px';
      copyButton.style.borderRadius = '4px';
      copyButton.style.cursor = 'pointer';

      // クリックされたらクリップボードにコピー
      copyButton.addEventListener('click', function() {
        const codeText = codeBlock.innerText;
        navigator.clipboard.writeText(codeText).then(function() {
          copyButton.textContent = 'Copied!';
          setTimeout(function() {
            copyButton.textContent = 'Copy';
          }, 2000);
        }, function(err) {
          console.error('Failed to copy: ', err);
        });
      });

      // ボタンを <pre> の子要素として挿入
      pre.appendChild(copyButton);
    });
  });
  </script>
  <script>
    function toggleMenu() {
      var nav = document.getElementById('navigation');
      nav.classList.toggle('open');
    }
  </script>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      var mathElements = document.querySelectorAll('.math');
      for (var i = 0; i < mathElements.length; i++) {
        var texText = mathElements[i].firstChild
        if (mathElements[i].tagName == "SPAN") {
            katex.render( texText.data
                        , mathElements[i]
                        , { displayMode: mathElements[i].classList.contains("display")
                          , throwOnError: true
                         }
                        );
          }
      }
    });
  </script>

  </body>
</html>